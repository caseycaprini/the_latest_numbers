---
title: "Gaps in the Analytics Debate"
description: |
  A few non-curmudgeonly issues with the "pro analytics" camp.
author:
  - name: Casey Caprini
date: 12-27-2021
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE, echo=FALSE}
library(tidyverse)
library(nflfastR)
library(nflplotR)
```

## Issues with the "pro analytics" side of the analytics debate

As many have noted, Week 15 of the NFL helped bring the sport's analytics debate to an unfortunate place.

Here are a few non-curmudgeonly issues I have with some in the "pro-analytics" camp.

-   Communicating model estimates without uncertainty

    -   The expected win probability advantage of decisions are communicated without any uncertainty. However, a 'Kick FG' vs. 'Go' 4th down analysis contains six estimated parameters.

-   Implying expected value optimization is the only valid decision making paradigm

    -   For environments where it is hard to anticipate every contingency, the expected value paradigm that dominates the NFL decision making discourse has legitimate alternatives. However, anyone who questions the "analytics experts" is frequently cast as being unsophisticated or unable to grasp "the math."

        -   Gary Klein's [Recognition-primed Decision model](https://en.wikipedia.org/wiki/Recognition_primed_decision) describes how experienced experts in many domains can reliably make high-quality, time-compressed decisions in a completely different manner.
        -   [Gerd Gigerenzer](https://en.wikipedia.org/wiki/Gerd_Gigerenzer) has offered a compelling defense of Gut Decisions in many domains, demonstrating where heuristics can be superior to optimization via complex algorithms/models.

## Model estimates without uncertainty

Let's look at an example. Just before the half in Week 15 against the KC Chiefs, Brandon Staley faced the following decision.

![Link: <https://twitter.com/ben_bot_baldwin/status/1471673042371858433>](lac_bot_end_half.png)

The "medium, go for it" recommendation is the result of an expected value calculation based on six estimated parameters.

-   Two "conversion rate" estimates

    -   For example, P(Score TD \| 'Go for it') = 60%.

-   Four win probability estimates for the possible resulting game states

    -   For example, P(Win \| Failed 'Kick FG' attempt) = 53%.

What happens if we propagate some uncertainty into the estimates that are inputs into the expected value calculation? (Like a less sophisticated cousin of this [awesome Shiny app](https://tuckerboynton.shinyapps.io/4thdowns/), only that also considers that the Win Probability model could also have small errors.)

-   Assume FG estimate is very accurate (i.e., error N(0, 0.25%)).
-   For other parameters, assume estimates is within 5% margin of error (i.e., errors N(0, 2.5%)).

Are errors of such magnitude sufficiently charitable to the model? For a toy exercise, I think so. That would seem like an excellent model to me! Sources of such errors could be attributable to injuries, weather, game plan, etc. that couldn't be captured by a model.

```{r sim_image_code, echo=FALSE}
p_fg <- 0.99
q_fg <- 1 - p_fg
wp_p_fg <- 63
wp_q_fg <- 53

p_go <- 0.60
q_go <- 1 - p_go
wp_p_go <- 73
wp_q_go <- 53

e_wp_fg <- (p_fg * wp_p_fg) + (q_fg * wp_q_fg)
e_wp_go <- (p_go * wp_p_go) + (q_go * wp_q_go)

n = 10000

# SE 0.02 since FG% far from 100%
p_fg_err <- rnorm(n, 0, 0.0025)
p_fg_dist <- p_fg + p_fg_err
q_fg_dist <- 1 - p_fg_dist
p_go_err <- rnorm(n, 0, 0.025)
p_go_dist <- p_go + p_go_err
q_go_dist <- 1 - p_go_dist

wp_p_fg_err <- rnorm(n, 0, 2.5)
wp_p_fg_dist <- wp_p_fg + wp_p_fg_err
wp_q_fg_err <- rnorm(n, 0, 2.5)
wp_q_fg_dist <- wp_q_fg + wp_q_fg_err
wp_p_go_err <- rnorm(n, 0, 2.5)
wp_p_go_dist <- wp_p_go + wp_p_go_err
wp_q_go_err <- rnorm(n, 0, 2.5)
wp_q_go_dist <- wp_q_go + wp_q_go_err

e_wp_go_sim <- (p_go_dist * wp_p_go_dist) + (q_go_dist * wp_q_go_dist)
e_wp_fg_sim <- (p_fg_dist * wp_p_fg_dist) + (q_fg_dist * wp_q_fg_dist)
e_wp_sim <- e_wp_go_sim - e_wp_fg_sim

df <- data.frame(e_wp_adv_go = e_wp_sim) %>%
  mutate(recommendation = case_when(e_wp_adv_go <= -1 ~ "Kick",
                                    e_wp_adv_go > -1 & e_wp_adv_go < 1 ~ "Toss up",
                                    e_wp_adv_go >= 1 ~ "Go"))

sim_mean <- df %>%
  pull(e_wp_adv_go) %>%
  mean() %>%
  round(digits = 1)
sim_sd <- df %>%
  pull(e_wp_adv_go) %>%
  sd() %>%
  round(digits = 1)

df_summ <- df %>%
  group_by(recommendation) %>%
  summarize(percent_sims = n()/nrow(df)) %>%
  mutate(percent_sims = round(percent_sims * 100, 1))

df %>%
  ggplot(aes(x = e_wp_adv_go)) +
  geom_vline(xintercept = 0,
             color = "black") +
  geom_vline(xintercept = e_wp_go - e_wp_fg,
             color = "black",
             size = 2) +
  geom_histogram(binwidth = 0.5,
                 color = "black",
                 aes(fill = recommendation)) +
  theme_light() +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = seq(from = -30, to = 30, by = 2),
                     minor_breaks = NULL) +
  scale_fill_manual(values = c("green", "red", "yellow")) +
  labs(x = "E[WP] Advantage for 'Go'",
       y = NULL,
       title = "Simulation with Modest Random Error in Estimated Parameters",
       subtitle = "Distribution of 10,000 sims.") +
  theme(legend.position = "none") +
  geom_label(aes(x = -4, y = 250,
                 label = paste0("Bot advantage: ",
                                round(e_wp_go - e_wp_fg, 1),
                                "\nSim mean: ",
                                sim_mean,
                                "\nSim sd: ",
                                sim_sd,
                                "\nSim Go Rate: ",
                                df_summ %>%
                                  filter(recommendation == "Go") %>%
                                  pull(percent_sims)))) -> p

sim_go_rate <- df_summ %>%
  filter(recommendation == "Go") %>%
  pull(percent_sims)

ggsave(filename = "lac_wk_15_sim_with_parameter_error_end_half.png",
       p,
       height = 6,
       width = 9,
       units = "in",
       dpi = "retina")
```

![Distribution of Expected Win Probability Results with Random Error in Underlying Parameters](lac_wk_15_sim_with_parameter_error_end_half.png)

#### What's the point?

[Some](https://twitter.com/benbbaldwin/status/1473015668777922568) use an expected win probability advantage of at least 1 WP as a threshold to judge when a decisions is "correct." Assume we accept the expected value paradigm (i.e., ignore my second complaint at present).

* Propagating some modest uncertainty in the estimated parameters underlying the expected value calculation suggests that categorizing coaching quality based on agreement with a specific model may not be justified for decisions with relatively narrow margins.
* In this simulation, `r 100 - sim_go_rate[1]` percent of the simulated expected value calculations fail to exceeded the 1 WP advantage threshold for the aggressive "Go for it" decision.

I'm not suggesting I've done this optimally (or perhaps even acceptably) for this example, but I think it is important to express uncertainty with estimates. In the future, I hope the "pro analytics" side of the debate finds a way to communicate uncertainty with their estimates.

## Expected value optimization as only valid paradigm

I highly recommend Gary Klein's [<u>Sources of Power</u>](https://www.amazon.com/Sources-Power-People-Decisions-Press/dp/0262534290/). Klein studied how individuals in complex domains (e.g., emergency responders, combatants) actually make decisions. The result was the (descriptive) Recognition-primed Decision (RPD) model.

Here's my rough summary of the findings relevant to NFL coaching decision discussion.

* Contrary to prevailing wisdom, it is <i>novice decision makers</i> in many domains that make decisions by comparing the projected consequences of multiple alternative courses of action.
* <i>Expert decision makers</i>, in contrast, draw upon cues in the environment and recognize a high quality candidate solution using their prior experience.
  * This candidate solution is interrogated for potential problems via a (comparatively) simple mental simulation.
  * Field studies in these domains demonstrate this single-path analysis mode is an <i>improvement</i> over comparative analysis (similar to the expected win probability analysis).

Here's a recent example of "expected value optimization is the only valid decision making paradigm" in the analytics debate.

![Link: https://twitter.com/benbbaldwin/status/1473012593547751424](belichick_gut_criticism.png)

I see a lot of Klein's descriptive model of expert "naturalistic decision making" from Coach Belichick, including critical reflection on previous decisions to learn like an expert. If I were a Pats fan, I would not be discouraged.

#### What's the point?

Coaches are trying to find the most probable single trajectory to victory; the expected value of a strategy will not be realized for a single decision. Additionally, it is very plausible to me that an expert coach could identify valid reasons -- unique to the specific game situation -- why an expected win probability model may not apply perfectly for the specific decision being made.

There is a lot of serious academic and other work that support the view that there are alternatives that can be superior to expected value optimization in some settings. Their discussion appears entirely absent from the slice of the analytics debate to which I am exposed.

If I were to anticipate criticisms, it would be something like the following: win probability models are now of sufficient quality that NFL games are now "smallish" enough words that no biased human expert can systematically be superior to the model. I'm skeptical.

A few other book recommendations that explore alternatives to "rational" optimization in the applicable contexts:

* [<u>Radical Uncertainty</u>](https://www.amazon.com/Radical-Uncertainty-Decision-Making-Beyond-Numbers/dp/1324004770/), by John Kay and Mervyn King
* [<u>Gut Feelings: The Intelligence of the Unconscious</u>](https://www.amazon.com/Gut-Feelings-Intelligence-Gerd-Gigerenzer/dp/0143113763/), by Gerd Gigerenzer
