[
  {
    "path": "posts/2023-05-09-2022-nfl-regular-season-win-luck/",
    "title": "2022-nfl-regular-season-win-luck",
    "description": "Estimates of NFL Regular Season Record Luck",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-05-09",
    "categories": [],
    "contents": "\r\nPackages Used\r\n\r\n\r\nShow code\r\n\r\nknitr::opts_chunk$set(echo = FALSE)\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\nlibrary(nflplotR)\r\nlibrary(nfltools)\r\n\r\n\r\nMeasures of Win ‘Luck’\r\nPythagorean Win Luck\r\nTime Average Win Luck\r\nThis post takes a look at two estimates of win luck, or roughly actual record compared with expected record.\r\nOne, Pythagorean Win Expectation. I’ll use the Daryl Morey formulation, which built form Bill James’s work for baseball. The basic logic of Pythagorean Win Expectation is that points scored and allowed for an entire season are more reliable gauges of team’s true ability than within a single game. Thus, Pythagorean Win Expectation is an across game estimate of expected regular season record.\r\nTwo, I’ll use my time average lead concept. This approach estimates the expected win percentage for each individual game, and estimates the season win percentage by taking the average of the individual game estimates. uses the average lead or deficit in a game to estimate the average win percentage for each game and computes. Since the real estimation is done at the individual game level, I call this a within game estimate.\r\nTime Average Model Win Expectancy Model\r\nThe code below fits a logistic model to the 2015 to 2021 regular seasons. It also assembles the information for and calculates the Pythagorean Win Expectation.\r\n\r\n\r\nShow code\r\n\r\ntrain <- map_dfr(\r\n  2015:2021,\r\n  nfl_mvt_season) %>%\r\n  filter(home_away == \"home\")\r\n\r\ntrain_results <- load_pbp(2015:2021) %>%\r\n  filter(season_type == \"REG\") %>%\r\n  mutate(home_result = case_when(\r\n    result > 0 ~ \"Win\",\r\n    result == 0 ~ \"Tie\",\r\n    result < 0 ~ \"Loss\"\r\n  )) %>%\r\n  group_by(game_id) %>%\r\n  summarize(\r\n    season = season,\r\n    week = week,\r\n    team = home_team,\r\n    opponent = away_team,\r\n    result = home_result,\r\n    .groups = \"drop\") %>%\r\n  distinct() %>%\r\n  mutate(game_id = NULL)\r\n\r\ntrain_df <- left_join(\r\n  train,\r\n  train_results) %>%\r\n  filter(result != \"Tie\") %>%\r\n  filter(home_away == \"home\") %>%\r\n  mutate(\r\n    result = ifelse(\r\n      result == \"Win\",\r\n      1L, 0L),\r\n    home_away = as.factor(home_away)\r\n  )\r\n\r\nmodel <- rstanarm::stan_glm(\r\n  result ~ -1 + time_avg_lead,\r\n  data = train_df,\r\n  family = \"binomial\"\r\n)\r\n\r\n\r\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 1).\r\nChain 1: \r\nChain 1: Gradient evaluation took 0 seconds\r\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 1: Adjust your expectations accordingly!\r\nChain 1: \r\nChain 1: \r\nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 1: \r\nChain 1:  Elapsed Time: 0.317 seconds (Warm-up)\r\nChain 1:                0.438 seconds (Sampling)\r\nChain 1:                0.755 seconds (Total)\r\nChain 1: \r\n\r\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 2).\r\nChain 2: \r\nChain 2: Gradient evaluation took 0 seconds\r\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 2: Adjust your expectations accordingly!\r\nChain 2: \r\nChain 2: \r\nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 2: \r\nChain 2:  Elapsed Time: 0.319 seconds (Warm-up)\r\nChain 2:                0.428 seconds (Sampling)\r\nChain 2:                0.747 seconds (Total)\r\nChain 2: \r\n\r\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 3).\r\nChain 3: \r\nChain 3: Gradient evaluation took 0 seconds\r\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 3: Adjust your expectations accordingly!\r\nChain 3: \r\nChain 3: \r\nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 3: \r\nChain 3:  Elapsed Time: 0.324 seconds (Warm-up)\r\nChain 3:                0.443 seconds (Sampling)\r\nChain 3:                0.767 seconds (Total)\r\nChain 3: \r\n\r\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 4).\r\nChain 4: \r\nChain 4: Gradient evaluation took 0 seconds\r\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\r\nChain 4: Adjust your expectations accordingly!\r\nChain 4: \r\nChain 4: \r\nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\r\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\r\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\r\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\r\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\r\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\r\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\r\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\r\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\r\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\r\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\r\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\r\nChain 4: \r\nChain 4:  Elapsed Time: 0.324 seconds (Warm-up)\r\nChain 4:                0.447 seconds (Sampling)\r\nChain 4:                0.771 seconds (Total)\r\nChain 4: \r\n\r\nShow code\r\n\r\ntest <- nfl_mvt_season(2022)\r\n\r\nbeta <- coef(model)\r\n\r\ntest_df <- test %>%\r\n  mutate(exp_wp = 1/(1 + exp(-beta * time_avg_lead)))\r\n\r\ndf_2022 <- test_df %>%\r\n  group_by(team) %>%\r\n  summarize(exp_wp = mean(exp_wp, na.rm = T),\r\n            .groups = \"drop\")\r\n\r\nhome_2022 <- load_pbp(2022) %>%\r\n  filter(season_type == \"REG\") %>%\r\n  group_by(game_id) %>%\r\n  slice_head(n = 1) %>%\r\n  ungroup() %>%\r\n  mutate(home_win = ifelse(result > 0,\r\n                           1L, 0L)) %>%\r\n  group_by(home_team) %>%\r\n  summarize(games_played = n(),\r\n            points_scored = sum(home_score),\r\n            points_allowed = sum(away_score),\r\n            wins = sum(home_win),\r\n            .groups = \"drop\") %>%\r\n  rename(team = home_team)\r\n\r\naway_2022 <- load_pbp(2022) %>%\r\n  filter(season_type == \"REG\") %>%\r\n  group_by(game_id) %>%\r\n  slice_head(n = 1) %>%\r\n  ungroup() %>%\r\n  mutate(away_win = ifelse(result < 0,\r\n                           1L, 0L)) %>%\r\n  group_by(away_team) %>%\r\n  summarize(games_played = n(),\r\n            points_scored = sum(away_score),\r\n            points_allowed = sum(home_score),\r\n            wins = sum(away_win),\r\n            .groups = \"drop\") %>%\r\n  rename(team = away_team)\r\n\r\nteam_2022 <- rbind(\r\n  home_2022,\r\n  away_2022) %>%\r\n  group_by(team) %>%\r\n  summarize(\r\n    games_played = sum(games_played),\r\n    points_scored = sum(points_scored),\r\n    points_allowed = sum(points_allowed),\r\n    wins = sum(wins),\r\n    .groups = \"drop\") %>%\r\n  mutate(actual_wp = wins/games_played,\r\n         pythag_wp = (points_scored ^ 2.37)/((points_scored ^ 2.37) + (points_allowed) ^ 2.37)) %>%\r\n  left_join(df_2022) %>%\r\n  mutate(time_avg_win_luck = (actual_wp - exp_wp) * 17,\r\n         pythag_win_luck = (actual_wp - pythag_wp) * 17)\r\n\r\n\r\nThe summary for the time average win expectation model:\r\n\r\n\r\nShow code\r\n\r\nsummary(model)\r\n\r\n\r\nModel Info:\r\n function:     stan_glm\r\n family:       binomial [logit]\r\n formula:      result ~ -1 + time_avg_lead\r\n algorithm:    sampling\r\n sample:       4000 (posterior sample size)\r\n priors:       see help('prior_summary')\r\n observations: 1801\r\n predictors:   1\r\n\r\nEstimates:\r\n                mean   sd   10%   50%   90%\r\ntime_avg_lead 0.3    0.0  0.3   0.3   0.3  \r\n\r\nFit Diagnostics:\r\n           mean   sd   10%   50%   90%\r\nmean_PPD 0.5    0.0  0.5   0.5   0.6  \r\n\r\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\r\n\r\nMCMC diagnostics\r\n              mcse Rhat n_eff\r\ntime_avg_lead 0.0  1.0  1350 \r\nmean_PPD      0.0  1.0  3946 \r\nlog-posterior 0.0  1.0  1220 \r\n\r\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\r\n\r\nPlot\r\n\r\n\r\nShow code\r\n\r\np <- team_2022 %>%\r\n  ggplot(aes(x = time_avg_win_luck,\r\n             y = pythag_win_luck)) +\r\n  geom_hline(yintercept = 0) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_nfl_logos(aes(team_abbr = team),\r\n                 width = 0.05) +\r\n  geom_label(aes(x = -2,\r\n                y = 3,\r\n                label = paste0(\"Correlation: \",\r\n                               cor(team_2022$time_avg_win_luck,\r\n                                   team_2022$pythag_win_luck) %>%\r\n                                 round(2)))) +\r\n  scale_x_continuous(breaks = seq(-6, 6, by = 1),\r\n                     minor_breaks = NULL) +\r\n  scale_y_continuous(breaks = seq(-6, 6, by = 1),\r\n                     minor_breaks = NULL) +\r\n  labs(x = \"Time Average Win Luck\",\r\n       y = \"Pythagorean Win Luck\",\r\n       caption = \"Data via nflfastR. Plot via nflplotR.\",\r\n       title = \"NFL 2022 Regular Season Win 'Luck'\",\r\n       subtitle = \"Within Game (Time Average) and Across Game (Pythagorean)\") +\r\n  theme_light()\r\n\r\nggsave(plot = p,\r\n       filename = \"nfl_2022_win_luck.png\",\r\n       height = 5.25,\r\n       width = 5,\r\n       units = \"in\",\r\n       dpi = \"retina\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-05-10T00:50:25-04:00",
    "input_file": "2022-nfl-regular-season-win-luck.knit.md"
  },
  {
    "path": "posts/2022-10-15-nflfastr-win-prob-excessmovement/",
    "title": "Analysis of Excess Movement in nflfastR's Win Probability Prediction Streams",
    "description": "Applying the Augenblick and Rabin (2020) Tests for Rational Bayesian Updating.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-10-15",
    "categories": [],
    "contents": "\r\nPacakges Used\r\nI will use the tidyverse, packages from the nflverse, and the\r\nbaseballR package for this post.\r\nBayesian Updating\r\nof Win Probability Streams\r\nWin probability (WP) models are becoming an increasingly component of\r\nsports analysis, including mainstream media and commentary. In the NFL,\r\nwin probability models are at the heart of the 4th down and 2-point\r\nconversion decision analyses that seem to overwhelm discussion of\r\ncoaching quality and fuel a significant amount of Monday morning\r\nquarterbacking.\r\nLike election forecasts, win probabilities are essentially the\r\nexpected value of a zero(loss)-one(win) process. However, a proper\r\nBayesian updating scheme is a martingale: “knowledge of the past will be\r\nof no use in predicting the future” and “the direction of anticipated\r\nfuture swings… should be already baked into the current prediction.”1\r\nThe martingale property implies that a stream of forecasted\r\nprobabilities should not change too much prior to resolution (e.g., end\r\nof game, election result decided). When the outcome is highly uncertain,\r\nthe forecast should be near 50%. Some what counter-intuitively, high\r\nuncertainty about the outcome should also imply low volatility in the\r\nforecast estimate.2\r\nIn 2020, Augenblick and Rabin3 proposed tests that\r\ncompare the movements in a belief stream (e.g., the squared difference\r\nbetween WP forecasts from play to play within an NFL game) to the change\r\nin uncertainty implied by the change in WP forecasts from play to play\r\n(e.g., uncertainty = WP * (1 - WP)).\r\n\r\n\r\nShow code\r\n\r\nx <- seq(0, 1, by = 0.001)\r\nplot_df <- data.frame(x)\r\nggplot(plot_df,\r\n       aes(x = x)) +\r\n  stat_function(fun=function(x) (x * (1-x))) +\r\n  labs(x = \"Win Probability (WP)\",\r\n       y = \"Uncertainty (= WP * (1 - WP))\") +\r\n  theme_light()\r\n\r\n\r\n\r\nThe Augenblick and Rabin tests are agnostic about what the correct\r\nbelief about a process should be. Rather, the tests evaluate the\r\ninformation processing implied by movements in the belief stream: on\r\naverage, the (squared) movement of probability forecasts should be equal\r\nin magnitude to the change in uncertainty implied by the probability\r\nforecasts.\r\nUsing its proposed statistics, the Augenblick and Rabin paper\r\nevaluated the “Bayesianess” of (1) estimates from human forecasters, (2)\r\nFangraphs in-game WP estimates, and (3) “market beliefs” implied by\r\nmarket odds from the prediction market Betfair.\r\nTo summarize their Table 3 results, all three sources exhibited some\r\namount of excess movement (i.e., statistically significant difference\r\nbetween movement and uncertainty reduction). Whereas the probabilities\r\nproduced by individual forecasters were somewhat over-reactive\r\n(normalized excess movement > 1), the algorithmic Fangraphs WP\r\npredictions were somewhat under-reactive (normalized excess movement\r\n< 1).\r\nStatistic\r\nForecasters\r\nFanGraphs WP\r\nBetfair\r\nExcess Movement Z Score\r\n4.22\r\n9.40\r\n19.55\r\nNormalized Excess Movement\r\n1.20\r\n0.931\r\n1.046\r\nEvaluation of nflfastR WP\r\nStreams\r\nHere, I apply some Augenblick and Rabin’s techniques to the\r\nnflfastR’s WP model (specifically, vegas_home_wp). The 2021 season is\r\nreviewed, excluding games that went to overtime (i.e., belief streams\r\nthat are not resolved in regulation). The results do not identify\r\n“excess or insufficient movement of beliefs relative to uncertainty\r\nreduction”, on average, in the vegas_home_wp streams considered.\r\nExcess Movement Statistics\r\n\r\n\r\nShow code\r\n\r\n# Function that calculates WP movement stats for a data.frame containing a single game\r\n# Function expects data.frame with home WP (home_p) and its complement (home_q) each observation\r\nwp_movement_nfl <- function(df){\r\n  for(i in 1:nrow(df)){\r\n    # No movement in WP for first row, so stays NA\r\n    if(i == 1){\r\n      df$unc_reduction[i] = NA_real_\r\n      df$movement[i] = NA_real_\r\n    }\r\n    else {\r\n      df$unc_reduction[i] = (df$home_p[i - 1]*df$home_q[i - 1]) - (df$home_p[i]*df$home_q[i])\r\n      df$movement[i] = (df$home_p[i] - df$home_p[i-1])^2\r\n    }\r\n  }\r\n  \r\n  return(df)\r\n}\r\n\r\nraw_nfl_df <- load_pbp(2021)\r\n\r\n# Find games with plays in OT\r\not_games <- raw_nfl_df %>%\r\n  group_by(game_id, game_half) %>%\r\n  summarize(n = n()) %>%\r\n  filter(game_half == \"Overtime\") %>%\r\n  ungroup() %>%\r\n  pull(game_id)\r\n\r\n\r\nnfl_df <- raw_nfl_df %>%\r\n  # Remove games that went to OT\r\n  filter((game_id %in% ot_games) == F) %>%\r\n  # Create p and q variables that are more intuitive to me\r\n  rename(home_p = vegas_home_wp) %>%\r\n  mutate(home_q = 1 - home_p) %>%\r\n  select(game_id, home_p, home_q) %>%\r\n  # Add variable to calculate movement and uncertainty reduction\r\n  mutate(movement = NA_real_,\r\n         unc_reduction = NA_real_)\r\n\r\n# Nest season by game\r\n# Apply WP movement function to each game\r\n# Unnest back to long data.frame\r\nnfl_season_df <- nfl_df %>%\r\n  group_nest(game_id) %>%\r\n  mutate(movement_df = map(data, wp_movement_nfl)) %>%\r\n  select(game_id, movement_df) %>%\r\n  unnest(cols = movement_df)\r\n\r\n# For convenience, remove rows with NA values in movement statistics\r\nnfl_test_df <- nfl_season_df %>%\r\n  filter(is.na(movement) == F) %>%\r\n  filter(is.na(unc_reduction) == F)\r\n\r\n\r\nFirst, let’s look at a density plot of the excess movement, which\r\nshows the mode near zero.\r\n\r\n\r\nShow code\r\n\r\nnfl_test_df %>%\r\n  ggplot(aes(x = movement - unc_reduction)) +\r\n  geom_density(fill = \"gray\") +\r\n  labs(x = \"Excess Movement\",\r\n       y = \"Density\") +\r\n  theme_light()\r\n\r\n\r\n\r\nThen, we compute summary statistics for average excess movement and\r\nnormalized excess movement. The normalized excess movement suggests that\r\nvegas_home_wp is very slightly under-reactive (i.e., 0.960 slightly less\r\nthan 1).\r\n\r\n\r\nShow code\r\n\r\nnfl_test_df %>%\r\n  summarize(average_excess_movement = mean(movement - unc_reduction),\r\n            normalized_excess_movement = mean(movement) / mean(unc_reduction)) %>%\r\n  print()\r\n\r\n# A tibble: 1 x 2\r\n  average_excess_movement normalized_excess_movement\r\n                    <dbl>                      <dbl>\r\n1              -0.0000463                      0.960\r\n\r\nNext, we test to see if the average excess movement is statistically\r\ndifferent from zero. Unlike the streams tested by Auckenblick and Rabin\r\nin their paper, the average difference between movement and uncertainty\r\nreduction is not statistically different from zero (t = -0.70, p-value =\r\n0.486) for vegas_home_wp.\r\n\r\n\r\nShow code\r\n\r\nt.test(nfl_test_df$movement,\r\n       nfl_test_df$unc_reduction,\r\n       paired = T)\r\n\r\n\r\n    Paired t-test\r\n\r\ndata:  nfl_test_df$movement and nfl_test_df$unc_reduction\r\nt = -0.69684, df = 45779, p-value = 0.4859\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -1.763756e-04  8.385582e-05\r\nsample estimates:\r\nmean of the differences \r\n           -4.62599e-05 \r\n\r\nTotal Movement and\r\nInitial Uncertainty\r\nFinally, we consider the relationship between initial uncertainty (in\r\nthis case, uncertainty determined by vegas line used in the model) and\r\ntotal movement, which in expectation should be equal (Corollary 1 in\r\nAugenblick and Rabin). Following the previous pattern for excess\r\nmovement, we consider a density then perform a t test.\r\n\r\n\r\nShow code\r\n\r\ninitial_uncertainty <- nfl_season_df %>%\r\n  group_by(game_id) %>%\r\n  slice_head(n = 1) %>%\r\n  mutate(init_uncertainty = home_p * home_q) %>%\r\n  select(game_id, init_uncertainty)\r\n\r\ngame_movement <- nfl_season_df %>%\r\n  filter(is.na(movement) == F) %>%\r\n  group_by(game_id) %>%\r\n  summarize(total_movement = sum(movement))\r\n\r\ngame_summary <- left_join(initial_uncertainty,\r\n                          game_movement,\r\n                          by = \"game_id\")\r\n\r\ngame_summary %>%\r\n  ggplot(aes(x = total_movement - init_uncertainty)) +\r\n  geom_density(fill = \"gray\") +\r\n  labs(x = \"Total Movement - Initial Uncertainty\",\r\n       y = \"Density\") +\r\n  theme_light()\r\n\r\n\r\n\r\nWhile the difference between total movement and initial uncertainty\r\nis right skewed, their mean difference is not significantly different\r\nthan 0 (t = -0.82, p-value = 0.412).\r\n\r\n\r\nShow code\r\n\r\nt.test(game_summary$total_movement,\r\n       game_summary$init_uncertainty,\r\n       paired = T)\r\n\r\n\r\n    Paired t-test\r\n\r\ndata:  game_summary$total_movement and game_summary$init_uncertainty\r\nt = -0.82257, df = 261, p-value = 0.4115\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -0.02743286  0.01126661\r\nsample estimates:\r\nmean of the differences \r\n           -0.008083123 \r\n\r\nEvaluation of MLB.com WP\r\nStreams\r\nFor comparison, I perform the same process for the MLB.com\r\nWP model via the baseballr package. For time efficiency in a random\r\nsample of 500 games in the 2021 MLB season.\r\n\r\n\r\nShow code\r\n\r\n# Roberto Clemente: \"Any time you have an opportunity to make a difference\r\n# in this world and you don't, then you are wasting your time on Earth.\"\r\nset.seed(21)\r\n\r\n# This is probably not the best workflow to get this data.\r\n# I'm really not well versed with the baseballr package.\r\nmlb_game_pks <- baseballr::mlb_schedule(2021) %>%\r\n  filter(game_type == \"R\") %>%\r\n  pull(game_pk) %>%\r\n  sample(size = 500)\r\n\r\nraw_mlb_df <- map_df(mlb_game_pks, mlb_game_wp)\r\n\r\nmlb_df <- raw_mlb_df %>%\r\n  rename(home_p = home_team_win_probability,\r\n         home_q = away_team_win_probability) %>%\r\n  mutate(home_p = home_p / 100,\r\n         home_q = home_q / 100) %>%\r\n  select(at_bat_index, home_p, home_q) %>%\r\n  mutate(movement = NA_real_,\r\n         reduction = NA_real_)\r\n\r\nwp_movement_mlb <- function(df){\r\n  for(i in 1:nrow(df)){\r\n    if(df$at_bat_index[i] == 0){\r\n      df$reduction[i] = NA_real_\r\n      df$movement[i] = NA_real_\r\n    }\r\n    else {\r\n      df$reduction[i] = (df$home_p[i - 1]*df$home_q[i - 1]) - (df$home_p[i]*df$home_q[i])\r\n      df$movement[i] = (df$home_p[i] - df$home_p[i-1])^2\r\n    }\r\n  }\r\n  \r\n  return(df)\r\n}\r\n\r\nmlb_excess <- mlb_df %>%\r\n  wp_movement_mlb()\r\n\r\n\r\nExcess Movement Statistics\r\nWe look at the density plot for excess movement, which looks similar\r\nto the plot for nflfastR’s WP model.\r\n\r\n\r\nShow code\r\n\r\nmlb_excess %>%\r\n  filter(is.na(movement) == F) %>%\r\n  ggplot(aes(x = movement - reduction)) +\r\n  geom_density(fill = \"gray\") +\r\n  theme_light() +\r\n  labs(x = \"Excess Movement\",\r\n       y = \"Density\")\r\n\r\n\r\nShow code\r\n\r\nmlb_excess %>%\r\n  summarize(average_excess_movement = mean(movement - reduction, na.rm = TRUE),\r\n            normalized_excess_movement = mean(movement, na.rm = TRUE) / mean(reduction, na.rm = TRUE)) %>%\r\n  print()\r\n\r\n# A tibble: 1 x 2\r\n  average_excess_movement normalized_excess_movement\r\n                    <dbl>                      <dbl>\r\n1              -0.0000213                      0.994\r\n\r\nThe normalized excess movement for the MLB.com WP model is even\r\ncloser to 1. Similar to nflfastR’s WP model, the average excess movement\r\nis not significantly different from 0 (t = -0.164, p-value = 0.87).\r\n\r\n\r\nShow code\r\n\r\nmlb_test_df <- mlb_excess %>%\r\n  filter(is.na(movement) == F) %>%\r\n  filter(is.na(reduction) == F)\r\n\r\nt.test(mlb_test_df$movement,\r\n       mlb_test_df$reduction,\r\n       paired = T)\r\n\r\n\r\n    Paired t-test\r\n\r\ndata:  mlb_test_df$movement and mlb_test_df$reduction\r\nt = -0.16401, df = 36915, p-value = 0.8697\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -0.0002761741  0.0002335249\r\nsample estimates:\r\nmean of the differences \r\n          -2.132457e-05 \r\n\r\nTotal Movement and\r\nInitial Uncertainty\r\nFinally, we consider the relationship between initial uncertainty and\r\ntotal movement for the MLB.com WP model, which in expectation should be\r\nequal.\r\n\r\n\r\nShow code\r\n\r\ngame_index = 0\r\n\r\nmlb_excess <- mlb_excess %>%\r\n  mutate(game_index = NA_integer_)\r\n\r\nfor(i in 1:nrow(mlb_excess)){\r\n  if(mlb_excess$at_bat_index[i] == 0){\r\n    game_index = game_index + 1\r\n  }\r\n  mlb_excess$game_index[i] = game_index\r\n}\r\n\r\nmlb_init_uncertainty <- mlb_excess %>%\r\n  group_by(game_index) %>%\r\n  slice_head(n = 1) %>%\r\n  mutate(init_uncertainty = home_p * home_q) %>%\r\n  select(game_index, init_uncertainty)\r\n\r\nmlb_total_movement <- mlb_excess %>%\r\n  group_by(game_index) %>%\r\n  summarize(total_movement = sum(movement, na.rm = T))\r\n\r\nmlb_total_compare <- left_join(\r\n  mlb_init_uncertainty,\r\n  mlb_total_movement,\r\n  by = \"game_index\"\r\n)\r\n\r\nmlb_total_compare %>%\r\n  ggplot(aes(x = total_movement - init_uncertainty)) +\r\n  geom_density(fill = \"gray\") +\r\n  theme_light() +\r\n  labs(x = \"Total Movement - Initial Uncertainty\",\r\n       y = \"Density\")\r\n\r\n\r\n\r\nAs with the nflfastR model, the difference between total movement and\r\ninitial uncertainty is right skewed, but the mean difference is not\r\nsignificantly different than 0 (t = -0.174, p-value = 0.862).\r\n\r\n\r\nShow code\r\n\r\nt.test(mlb_total_compare$total_movement,\r\n       mlb_total_compare$init_uncertainty,\r\n       paired = T)\r\n\r\n\r\n    Paired t-test\r\n\r\ndata:  mlb_total_compare$total_movement and mlb_total_compare$init_uncertainty\r\nt = -0.17433, df = 499, p-value = 0.8617\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -0.01931849  0.01616961\r\nsample estimates:\r\nmean of the differences \r\n           -0.001574436 \r\n\r\nConclusion\r\nThe papers in the footnotes are really interesting to me because they\r\nillustrate that a WP conveys more than the expected relative frequency\r\nof winning given the current game state. The point estimate also conveys\r\nthe uncertainty of the outcome of the process.\r\nThis blog post applied some of Augenblick and Rabin’s information\r\nprocessing diagnostics for popular WP models. According to these\r\ndiagnostics, both nflfastR’s vegas_home_wp model and MLB.com’s WP model\r\ndemonstrate coherent Bayesian updating, on average. Additionally, both\r\nmodels appear to outperform the model’s evaluated by Augenblick and\r\nRabin in this respect.\r\n\r\nGelman, Hullman, Wlezien, and Morris;\r\n2020. Information,\r\nincentives, and goals in election forecasts.↩︎\r\nTaleb; 2017. Election predictions as\r\nmartingales: An arbitrage approach.↩︎\r\nAugenblick and Rabin; 2020. Belief\r\nMovement, Uncertainty Reduction, & Rational Updating.↩︎\r\n",
    "preview": "posts/2022-10-15-nflfastr-win-prob-excessmovement/nflfastr-win-prob-excessmovement_files/figure-html5/wp_uncertianty_plot-1.png",
    "last_modified": "2022-10-15T22:06:03-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-12-nfl-basic-multilevel-models/",
    "title": "Empirical Bayes Estimates of NFL Offense and Defense Quality",
    "description": "Replacing averages with random intercepts from a basic multilevel model.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-10-12",
    "categories": [],
    "contents": "\r\nPackages Used\r\nI will use the tidyverse, packages from the nflverse, and lme4 for\r\nthe multilevel model.\r\nEmpirical Bayes\r\nEstimates of Yards/Play\r\nYards per play is a basic measure of snap-to-snap team quality. In\r\nPittsburgh, analyst Matt\r\nWilliamson frequently cites yards/play in his, noting that former\r\nSteelers GM Kevin Colbert found great value in the metric. This makes\r\nsense because there are many plays in an NFL game: a true per snap\r\nadvantage is likely to build to a larger advantage over the course of a\r\ngame. The yard/play idea was also likely inspired by The\r\nSuccess Equation: Untangling Skill and Luck in Business, Sports, and\r\nInvesting by Michael J.\r\nMauboussin (Chapter 4), which I read earlier in 2022.\r\nBasic yards/play averages do not fully capture a true advantage. Some\r\nteams play a much weaker slate of opponents. There is sampling error or\r\nluck, good and bad, in different measures. This post hopes to improve on\r\nthe simple arithmetic mean via a very basic multilevel model.\r\nWhile penalty assessment is a bit arbitrary in the NFL, penalty\r\nyardage is earned by poor technique and related to the tactics employed.\r\nTherefore, net penalty yardage is considered just the same as yards\r\ngained or lost conventionally, even if the down is replayed. This is a\r\nbit like preferring plate appearances to at bats in baseball (since at\r\nbats exclude walks, scorer assessed sacrifices, etc.).\r\n\r\n\r\nShow code\r\n\r\npenalty_yards_gained <- function(df){\r\n  df %>%\r\n    mutate(yards_gained = ifelse(play_type_nfl == \"PENALTY\",\r\n                                 ifelse(penalty_team == posteam,\r\n                                        -1 * penalty_yards,\r\n                                        penalty_yards),\r\n                                 yards_gained)) %>%\r\n    return()\r\n}\r\n\r\n\r\nSince they are on the field for most of the snaps in a game, analysis\r\nis confined to the offensive and defensive units. Plays spikes and\r\nkneels, where the purpose of the down is not gain yardage, are\r\ndiscarded.\r\n\r\n\r\nShow code\r\n\r\npbp_df <- nflfastR::load_pbp(2021) %>%\r\n  filter(season_type == \"REG\") %>%\r\n  filter(play_type_nfl %in% c(\"GAME_START\",\r\n                              \"KICK_OFF\",\r\n                              \"PUNT\",\r\n                              \"TIMEOUT\",\r\n                              \"FIELD_GOAL\",\r\n                              \"XP_KICK\",\r\n                              \"END_QUARTER\",\r\n                              \"END_GAME\",\r\n                              \"PAT2\",\r\n                              \"FREE_KICK\",\r\n                              \"COMMENT\") == F) %>%\r\n  filter(is.na(play_type_nfl) == F) %>%\r\n  filter(play_type %in% c(\"qb_kneel\",\r\n                          \"qb_spike\") == F) %>%\r\n  filter(grepl(\"(Punt formation)\", desc) == F) %>%\r\n  filter(grepl(\", offsetting.\", desc) == F) %>%\r\n  penalty_yards_gained()\r\n\r\n\r\nThe Advantages of a\r\nMultilevel Approach\r\n\r\n“… multilevel regression deserves to be the default form of\r\nregression. Papers that do not use multilevel models should have to\r\njustify not using a multilevel approach.” - McElreath, Statistical\r\nRethinking, pg. 15\r\n\r\nMultilevel models have a number of advantages over simpler regression\r\nmodels. Random intercepts models are the simplest multilevel models and\r\nare generally used as a baseline for which to compare more complicated\r\nmultilevel models against. Here, a random intercepts model is proposed\r\nas an improvement on averaging.\r\nRealistic Assumptions\r\nAbout Independence\r\nImportantly, multilevel models make more realistic assumptions about\r\nthe independence of observations in a data set. Context rightly shapes\r\nthe meaning we derive from statistics. For example, gaining 5 yards/play\r\nagainst the NFL’s best defense reflects more positively on an offense\r\nthan gaining 5 yards/play against the league’s worst defense. For some\r\ngames, weather conditions severely depress offense (think the\r\nBills/Patriots game in the 2021 regular season). Averages and simpler\r\nregression models, however, do not recognize any such context; all\r\nobservations are assumed to be independent.\r\nClustering is expected in yards/play data, along a number of\r\ndifferent dimensions. Consider the following examples.\r\nTwo offenses playing in inclement weather are likely to be more\r\nsimilar to one another for that game than the other NFL offenses playing\r\nin more typical weather conditions that week.\r\nOffenses in 2022 are likely to be more similar to the one another\r\nthan they are to offenses in 1995.\r\nMultilevel models acknowledge and take advantage of these\r\ndependencies in the structure of the data.\r\nParitial Pooling and\r\nRegularization\r\nThe observations in one cluster of our data are also informative for\r\nother clusters. This may be less intuitive, but consider the\r\nfollowing.\r\nIf I asked you how many yards/play the Dallas Cowboys offense\r\nproduced in 2021, your guess would probably improve if I told you the\r\nanswer for the Carolina Panthers.\r\nIf I asked you how many yards/play the Green Bay Packers defense\r\nsurrendered in 2021, your guess would probably improve if I told you how\r\nmany yards/play in all NFL games played that season.\r\nSharing of information between the results of all plays across the\r\nleague, regardless of cluster, and the individual clusters in the data\r\nprovides regularized estimates. The regularization (or shrinkage)\r\nperformed by multilevel models tempers the conclusions drawn in each\r\ncluster of the data by considering all clusters, thus minimizing the\r\nadverse affects of overfitting (roughly, taking the noise in the sample\r\ntoo seriously). Regularization can thus improve out-of-sample\r\nprediction.\r\n\r\nComplete pooling -> predict grand mean for all teams -> maximum\r\nunderfitting  No pooling -> predict group mean for each team\r\n-> maximum overfitting  Partial pooling -> predict random\r\nintercept for each team -> a principled balance.\r\n\r\nReaders of the Football Outsiders may be familiar with the Plexiglass\r\nPrinciple, which roughly states that a big improvement (or decline) in\r\nperformance will on average be followed by a relapse (or bounce back).\r\nThe Plexiglass Principle was coined by the legendary baseball\r\nsabermetrician Bill James, who later renamed it the Whirlpool Principle:\r\n“All teams are drawn forcefully toward the center. Most of the teams\r\nwhich had winning records in 1982 will decline in 1983; most of the\r\nteams which had losing records in 1982 will improve in 1983.” 1\r\nTraditional statisticians call this the Paradox of Regression to the\r\nMean. Exceptionally good performance is likely the result of a\r\ncombination of high ability and good luck; subsequent performance is\r\nlikely to be better than average but not as good as the previous\r\nexceptional outcome. 2\r\nPartial Pooling: An\r\nIntuitive Approach\r\nWe will use Empirical Bayes estimates, or shrinkage estimates, to\r\nregularize NFL offensive and defensive yards/play averages.3\r\nHow much shrinkage is appropriate? The multilevel model will\r\ndetermine the strength of the whirlpool.\r\nWe begin by building a league-wide normal distribution centered on\r\nthe mean yards gained for all observations in the data set. The standard\r\ndeviation of this distribution is the standard error of the league-wide\r\nmean, assuming n = 1125 (a nominal number of plays for a team’s\r\noffensive or defensive unit that is representative for the 2021 season)\r\nrather than the total number of plays in the regular season. We’ll plot\r\nthis curve as a league-wide nominal distribution, since its parameters\r\nare estimated considering all plays in the 2021 NFL regular season.\r\nNext, we plot the average yards/play calculated for each offense and\r\ndefense under the league-wide nominal distribution. This allows us to\r\ncompare the distribution of offensive unit averages and the distribution\r\nof the defensive unit averages against all plays.\r\n\r\n\r\nShow code\r\n\r\n# Estimate NFL Distribution using mean for all plays...\r\navg_yards_per_play <- pbp_df %>%\r\n  summarize(mean = mean(yards_gained)) %>%\r\n  pull(mean)\r\n\r\n# and standard error of the mean, which assumes\r\n# 1125 plays in a season for a unit.\r\nsd_yards_per_play <- pbp_df %>%\r\n  summarize(sd = sd(yards_gained)) %>%\r\n  pull(sd)\r\nse_unit <- sd_yards_per_play/sqrt(1125)\r\n\r\n# Build data frame to plot estimated distribution\r\nypp <- seq(0, 10, by = 0.01)\r\nd_ypp <- dnorm(ypp, mean = avg_yards_per_play, sd = se_unit)\r\ndf_ypp <- data.frame(ypp, d_ypp)\r\nrm(ypp, d_ypp)\r\n\r\n# Calculate (Raw, Unadjusted) Unit Averages\r\noff_avg_df <- pbp_df %>%\r\n  group_by(posteam) %>%\r\n  summarize(off_yards_per_play = mean(yards_gained),\r\n            off_n = n()) %>%\r\n  rename(team = posteam)\r\ndef_avg_df <- pbp_df %>%\r\n  group_by(defteam) %>%\r\n  summarize(def_yards_per_play = mean(yards_gained),\r\n            def_n = n()) %>%\r\n  rename(team = defteam)\r\n\r\n\r\n# Plot unit averages against league-wide estimated distribution\r\n# Stack of plots will show much greater spread among offenses\r\n# compared to defenses.\r\np_top <- df_ypp %>%\r\n  ggplot(aes(x = ypp,\r\n             y = d_ypp)) +\r\n  geom_vline(xintercept = c(avg_yards_per_play,\r\n                            avg_yards_per_play - 2*se_unit,\r\n                            avg_yards_per_play + 2*se_unit),\r\n             color = \"dark gray\",\r\n             linetype = \"dashed\") +\r\n  geom_line() +\r\n  geom_point(data = off_avg_df,\r\n             aes(x = off_yards_per_play,\r\n                 y = 0,\r\n                 color = team)) +\r\n  scale_color_nfl() +\r\n  theme_light() +\r\n  scale_x_continuous(minor_breaks = NULL) +\r\n  scale_y_continuous(minor_breaks = NULL) +\r\n  coord_cartesian(xlim = c(4.0, 6.25)) +\r\n  labs(x = \"Offensive Yards Per Play\",\r\n       y = \"Density\",\r\n       title = \"Team Unit Averages Compared to NFL Distribution\",\r\n       subtitle = \"NFL Standard Error Assumes 1125 Plays for Unit\")\r\np_bottom <- df_ypp %>%\r\n  ggplot(aes(x = ypp,\r\n             y = d_ypp)) +\r\n  geom_vline(xintercept = c(avg_yards_per_play,\r\n                            avg_yards_per_play - 2*se_unit,\r\n                            avg_yards_per_play + 2*se_unit),\r\n             color = \"dark gray\",\r\n             linetype = \"dashed\") +\r\n  geom_line() +\r\n  geom_point(data = def_avg_df,\r\n             aes(x = def_yards_per_play,\r\n                 y = 0,\r\n                 color = team)) +\r\n  scale_color_nfl() +\r\n  theme_light() +\r\n  scale_x_continuous(minor_breaks = NULL) +\r\n  scale_y_continuous(minor_breaks = NULL) +\r\n  coord_cartesian(xlim = c(4.0, 6.25)) +\r\n  labs(x = \"Defensive Yards Per Play\",\r\n       y = \"Density\")\r\nlibrary(patchwork)\r\np_units <- p_top / p_bottom\r\nggsave(\"nfl_2021_unit_dist.png\",\r\n       plot = p_units,\r\n       height = 5.25,\r\n       width = 5,\r\n       units = \"in\",\r\n       dpi = \"retina\")\r\n\r\n\r\n\r\nA few observations:\r\nThe defensive average are much more closely packed around the league\r\nwide mean than the offensive averages. This indicates that the offense\r\nidentity more reliably accounts for variation in the data than defense\r\nidentity. Therefore, we expect our model shrink offensive deviations\r\nfrom average less than defensive deviations.\r\nLooking at the offenses, about 9 or 10 offenses (of 32) are greater\r\nthan 2 standard errors from the league wide mean (i.e., outside of gray\r\ndashed lines). Since the standard errors should be similar, about 2\r\nwould be expected (0.05 * 32 = about 2). This suggests that the offense\r\naverages are too greatly dispersed, so some modest shrinkage is\r\nexpected.\r\nSince each unit will have run a similar number of plays, the\r\nstandard error of the estimates of the offensive/defensive unit averages\r\nshould be similar. Therefore, shrinkage should mostly be dominated by\r\ndeviations from league average rather than differences in sample size\r\n(e.g., above average performance by some metric of a backup QB would be\r\nshrunk more than similar performance for a full time QB because the\r\nbackup much smaller sample size).\r\nLet’s look at the actual shrinkage in the Empirical Bayes\r\nestimate.\r\n\r\n\r\nShow code\r\n\r\nmod_rand_int <- lmer(yards_gained ~ 1 + (1|posteam) + (1|defteam) + (1|game_id),\r\n                     data = pbp_df)\r\n\r\n\r\nTo visualize the “whirlpool” effect of the multilevel model, we build\r\na data frame with the unadjusted unit averages and their Empirical Bayes\r\n(random intercepts) estimates.\r\n\r\n\r\nShow code\r\n\r\noff_rand_int <- coef(mod_rand_int)$posteam %>%\r\n  rownames_to_column(var = \"team\") %>%\r\n  rename(off_estimate = `(Intercept)`)\r\n\r\noff_avg_df <- off_avg_df %>%\r\n  left_join(off_rand_int,\r\n            by = \"team\") %>%\r\n  mutate(off_estimate_adjust = off_estimate - off_yards_per_play)\r\n\r\n\r\ndef_rand_int <- coef(mod_rand_int)$defteam %>%\r\n  rownames_to_column(var = \"team\") %>%\r\n  rename(def_estimate = `(Intercept)`)\r\n\r\ndef_avg_df <- def_avg_df %>%\r\n  left_join(def_rand_int,\r\n            by = \"team\") %>%\r\n  mutate(def_estimate_adjust = def_estimate - def_yards_per_play)\r\n\r\nest_df <- left_join(off_avg_df,\r\n                    def_avg_df,\r\n                    by = \"team\") %>%\r\n  mutate(combined_estimate = off_estimate - def_estimate) %>%\r\n  arrange(desc(combined_estimate)) %>%\r\n  mutate(estimate_rank = row_number(),\r\n         percentile_estimate = scale(combined_estimate)) %>%\r\n  mutate(percentile_estimate = pnorm(percentile_estimate))\r\n\r\nrm(off_avg_df,\r\n   def_avg_df)\r\n\r\n\r\nThen, we plot the unadjusted unit averages and their regularized\r\nestimate to show the shrinkage.\r\n\r\n\r\nShow code\r\n\r\np_pooling <- est_df %>%\r\n  ggplot(aes(x = off_yards_per_play,\r\n             y = def_yards_per_play)) +\r\n  geom_hline(yintercept = avg_yards_per_play) +\r\n  geom_vline(xintercept = avg_yards_per_play) +\r\n  geom_point(aes(color = team)) +\r\n  geom_segment(aes(xend = off_estimate,\r\n                   yend = def_estimate,\r\n                   color = team)) +\r\n  geom_nfl_logos(aes(x = off_estimate,\r\n                     y = def_estimate,\r\n                     team_abbr = team),\r\n                 width = 0.05) +\r\n  scale_y_reverse(breaks = seq(4.0, 6.0, by = 0.2),\r\n                  minor_breaks = NULL) +\r\n  scale_x_continuous(breaks = seq(4.0, 6.0, by = 0.2),\r\n                     minor_breaks = NULL) +\r\n  scale_color_nfl() +\r\n  labs(x = \"Offensive Yards Per Play\",\r\n       y = \"Defensive Yards Per Play\",\r\n       title = \"Adjusted Yards Per Play\",\r\n       subtitle = \"The Plexiglass Principle Adaptively Applied Via Partial Pooling\",\r\n       caption = \"Data: nflfastR | Plot: nflplotR | Model: lme4\") +\r\n  theme_light() +\r\n  coord_fixed()\r\n\r\nggsave(\"nfl_2021_yards_per_play_pooling.png\",\r\n       plot = p_pooling,\r\n       units = \"in\",\r\n       height = 5.25,\r\n       width = 5,\r\n       dpi = \"retina\")\r\n\r\n\r\n\r\nTo better quantify the average shrinkage applied to deviations from\r\naverage for offenses, we can fit an ordinary least squares regression to\r\na centered data frame (i.e., the league average is subtracted from both\r\nthe regularized estimates and the unadjusted averages).\r\n\r\n\r\nShow code\r\n\r\ncentered_df <- est_df %>%\r\n  mutate(off_estimate = off_estimate - avg_yards_per_play,\r\n         off_yards_per_play = off_yards_per_play - avg_yards_per_play,\r\n         def_estimate = def_estimate - avg_yards_per_play,\r\n         def_yards_per_play = def_yards_per_play - avg_yards_per_play)\r\n\r\nlm(off_estimate ~ off_yards_per_play,\r\n   data = centered_df) %>%\r\n  coefficients()\r\n\r\n       (Intercept) off_yards_per_play \r\n      -0.001170907        0.666408276 \r\n\r\nShow code\r\n\r\nest_df %>%\r\n  ggplot(aes(x = off_yards_per_play - avg_yards_per_play,\r\n             y = off_estimate - avg_yards_per_play)) +\r\n  geom_smooth(method = \"lm\",\r\n              se = FALSE) +\r\n  geom_text(aes(color = team,\r\n                label = team)) +\r\n  scale_color_nfl() +\r\n  scale_x_continuous(breaks = seq(-10, 10, by = 0.2),\r\n                     minor_breaks = NULL) +\r\n  scale_y_continuous(breaks = seq(-10, 10, by = 0.2),\r\n                     minor_breaks = NULL) +\r\n  theme_light() +\r\n  coord_fixed(xlim = c(-0.9, 0.9),\r\n              ylim = c(-0.8, 0.8))\r\n\r\n\r\n\r\nWe repeat for the defensive units. As anticipated, more shrinkage is\r\napplied to the defensive units (i.e., the slope of the defensive line is\r\ncloser to 0 than the offensive line).\r\n\r\n\r\nShow code\r\n\r\nlm(def_estimate ~ def_yards_per_play,\r\n   data = centered_df) %>%\r\n  coefficients()\r\n\r\n       (Intercept) def_yards_per_play \r\n      -0.001126752        0.309755311 \r\n\r\nShow code\r\n\r\nest_df %>%\r\n  ggplot(aes(x = def_yards_per_play - avg_yards_per_play,\r\n             y = def_estimate - avg_yards_per_play)) +\r\n  geom_smooth(method = \"lm\",\r\n              se = FALSE) +\r\n  geom_text(aes(color = team,\r\n                label = team)) +\r\n  scale_color_nfl() +\r\n  scale_x_continuous(breaks = seq(-10, 10, by = 0.2),\r\n                     minor_breaks = NULL) +\r\n  scale_y_continuous(breaks = seq(-10, 10, by = 0.2),\r\n                     minor_breaks = NULL) +\r\n  theme_light() +\r\n  coord_fixed(xlim = c(-0.9, 0.9),\r\n              ylim = c(-0.8, 0.8))\r\n\r\n\r\n\r\nA Point Estimate of Team\r\nQuality\r\nAs our point estimate of team quality, we consider adjusted yard/play\r\ndifferential (i.e., offense yards/play regularized estimate - defense\r\nyards/play regularized estimate). To more easily visualize the relative\r\nspread in team quality by this metric, we plot the results in terms of\r\npercentiles.\r\n\r\n\r\nShow code\r\n\r\np_2021 <- est_df %>%\r\n  ggplot(aes(x = percentile_estimate,\r\n             y = estimate_rank)) +\r\n  geom_hline(yintercept = c(0.5, 8.5, 16.5, 24.5, 32.5),\r\n             color = \"dark gray\",\r\n             linetype = \"dashed\") +\r\n  geom_vline(xintercept = c(0, 0.25, 0.5, 0.75, 1),\r\n             color = \"dark gray\") +\r\n  geom_nfl_logos(aes(team_abbr = team),\r\n                 width = 0.05) +\r\n  scale_x_continuous(minor_breaks = NULL,\r\n                     labels = scales::percent) +\r\n  scale_y_reverse(breaks = NULL,\r\n                  minor_breaks = NULL) +\r\n  scale_color_nfl() +\r\n  theme_light() +\r\n  labs(title = \"2021 NFL Regular Season Per Play Rankings\",\r\n       subtitle = \"Excludes Special Teams/Kneels/Spikes; Includes Penalty Yardage\",\r\n       x = \"Adjusted Yards Per Play Differential: Percentile\",\r\n       y = NULL,\r\n       caption = \"Data: nflfastR | Plot: nflplotR | Model: lme4\")\r\n\r\nggsave(\"nfl_2021_reg_season_per_play.png\",\r\n       plot = p_2021,\r\n       units = \"in\",\r\n       height = 5.25,\r\n       width = 5,\r\n       dpi = \"retina\")\r\n\r\n\r\n\r\nQuick\r\nBack Test: Adjusted Yard/Play Differential as a Predictor of Playoff\r\nSuccess\r\nAs a quick back test, we compare how predictive regular season\r\nadjusted yard/play differential was for the 2021 NFL playoffs. The Vegas\r\nspreads below4 show the winning team was favored in\r\n8 of 13 games. In this small sample, the adjusted yard/play differential\r\nmodel preferred the winning team in 12 of 13 games. This is not a\r\nrigorous test by any means, but it demonstrates the model likely carries\r\nsome genuine insight.\r\nAway\r\nHome\r\nResult\r\nVegas Spread\r\nModel\r\nCIN\r\nLV\r\nLV 19, CIN 26\r\nCIN -5.5 (Hit)\r\nCIN (Hit)\r\nNE\r\nBUF\r\nNE 17, BUF 47\r\nBUF -4 (Hit)\r\nBUF (Hit)\r\nPHI\r\nTB\r\nPHI 15, TB 31\r\nTB -8.5 (Hit)\r\nTB (Hit)\r\nSF\r\nDAL\r\nSF 23, DAL 17\r\nDAL -3 (Miss)\r\nSF (Hit)\r\nPIT\r\nKC\r\nPIT 21, KC 42\r\nKC -12.5 (Hit)\r\nKC (Hit)\r\nARI\r\nLAR\r\nARI 11, LAR 34\r\nLAR -3.5 (Hit)\r\nLAR (Hit)\r\nCIN\r\nTEN\r\nCIN 19, TEN 16\r\nTEN -4 (Miss)\r\nCIN (Hit)\r\nSF\r\nGB\r\nSF 13, GB 10\r\nGB -5.5 (Miss)\r\nSF (Hit)\r\nLAR\r\nTB\r\nLAR 30, TB 27\r\nTB -3 (Miss)\r\nLAR (Hit)\r\nBUF\r\nKC\r\nBUF 36, KC 42\r\nKC -2 (Hit)\r\nBUF (Miss)\r\nCIN\r\nKC\r\nCIN 27, KC 24\r\nKC -7 (Miss)\r\nCIN (Hit)\r\nSF\r\nLAR\r\nSF 17, LAR 20\r\nLAR -3.5 (Hit)\r\nLAR (Hit)\r\nLAR\r\nCIN\r\nLAR 23, CIN 20\r\nLAR -4.5 (Hit)\r\nLAR (Hit)\r\n\r\nQuotation and background repeated\r\nfrom this excellent\r\nblog post.↩︎\r\nSee Section 6.5 in Regression and Other\r\nStories by Gelman, Hill, and Vehtari, from which this summary of\r\nregression to the mean is adapted.↩︎\r\nSee Section 8.7.1 of Beyond\r\nMultiple Linear Regression by Roback and Legler or this NBER seminar\r\non Empirical Bayes\r\nmethods.↩︎\r\nRetrieved from Sports Odds\r\nHistory.com.↩︎\r\n",
    "preview": "posts/2022-09-12-nfl-basic-multilevel-models/nfl_2021_yards_per_play_pooling.png",
    "last_modified": "2022-10-12T00:37:29-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-17-time-average-qb-effects/",
    "title": "QB Quality and Time Average Evaluations",
    "description": "Incoprporating Mike Sando's QB Tiers with Time Averages to Predict Win/Loss Outcomes.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-04-21",
    "categories": [],
    "contents": "\r\nIntro\r\nPast posts have explored uses of NFL time average leads as a single-point summary of overall game performance.\r\nUsing a simple logistic regression model, I looked at 2021 regular season win luck. Notably, the “luckiest teams” had veteran/high quality QBs.\r\n\r\nMy intuition: quality QBs perform well in comeback and high leverage situations. The residuals from the single feature model reflect that the direct effect of QBs is significant.\r\nHere, I expand the model to include QB quality, using an approach inspired by Mike Sando’s QB Tiers.\r\nMike Sando’s QB Tiers\r\nMike Sando is probably my favorite national NFL writer. He’s one of a few writers that make me a subscriber to the Athletic. His recent tribute to the late John Clayton was tremendous.\r\nEach summer since 2014, Sando has released his QB Tier article, where several NFL team personnel evaluate the league’s QBs using a simple rubric. From the 2021 edition:\r\n\r\nTIER 1\r\nA Tier 1 quarterback can carry his team each week. The team wins because of him. He expertly handles pure passing situations. He has no real holes in his game…\r\nTIER 2\r\nA Tier 2 quarterback can carry his team sometimes but not as consistently. He can handle pure passing situations in doses and/or possesses other dimensions that are special enough to elevate him above Tier 3. He has a hole or two in his game…\r\nTIER 3\r\nA Tier 3 quarterback is a legitimate starter but needs a heavier running game and/or defensive component to win. A low-volume dropback passing offense suits him best…\r\nTIER 4\r\nA Tier 4 quarterback could be an unproven player with some upside or a veteran who is ultimately best suited as a backup…\r\n— Mike Sando\r\n\r\nI really like the tier approach: it aggregates subject matter expert opinion in a straightforward, meaningful way without requiring a false level of precision.\r\nTime Average Lead + QB Quality Model\r\nA summary of the model, fit on regular season games that occured between 2013 and 2020.\r\nIn addition to the game’s time average lead, a team’s Sando QB Tier advantage is included as a feature in a logistic regression.\r\nI’ve assumed that the following year’s QB Tier is a better indicator of QB quality for a season than their preseason QB Tier.\r\nThis has the tradeoff of reducing the size of the data set.\r\nIf one of the QBs is not evaluated the next season (e.g., retires), that game is not included in the data set.\r\n\r\n\r\nFor each game, the QB tier rating for QB with the most snap counts played for each team is used.\r\nFor an unranked QB that was the planned opening day starter, I assumed a QB rank of 4.0.\r\nFor an unranked QB that was not a planned opening day starter, I assumed a QB rank of 4.5.\r\nModel Summary\r\nA summary of the coefficients and standard error.\r\nTime Average Lead: 0.306\r\nStandard Error: 0.018\r\n\r\nQB Tier Advantage: 0.337\r\nStandard Error: 0.07\r\n\r\n\r\n\r\n\r\nEnd of Season 2021 Tier Rankings\r\nUsing (1) the logistic model above and (2) some estimates I made of the Sando QB Tier grade of the QBs that finished the season (i.e., not L. Jackson for BAL, Big Ben rated Tier 3.5 vice 2.64 from 2021 grades), I made a plot of an end of season power ranking based on a team’s season time average lead and QB quality.\r\nThe contour lines represent the model’s expected win probability (“exp_wp” in table) against an average team with an average QB. From the plot, think end of season Cleveland Browns led by an injury-plagued Baker Mayfield as roughly representing the baseline.\r\n\r\n\r\nrank\r\n\r\n\r\nteam\r\n\r\n\r\ntime_avg_lead\r\n\r\n\r\nqb_adv\r\n\r\n\r\nexp_wp\r\n\r\n\r\n1\r\n\r\n\r\nBUF\r\n\r\n\r\n5.1\r\n\r\n\r\n1.7\r\n\r\n\r\n89.5\r\n\r\n\r\n2\r\n\r\n\r\nKC\r\n\r\n\r\n4.4\r\n\r\n\r\n2.0\r\n\r\n\r\n88.2\r\n\r\n\r\n3\r\n\r\n\r\nTB\r\n\r\n\r\n4.2\r\n\r\n\r\n1.7\r\n\r\n\r\n86.6\r\n\r\n\r\n4\r\n\r\n\r\nDAL\r\n\r\n\r\n4.8\r\n\r\n\r\n0.7\r\n\r\n\r\n84.7\r\n\r\n\r\n5\r\n\r\n\r\nIND\r\n\r\n\r\n4.6\r\n\r\n\r\n-0.3\r\n\r\n\r\n78.9\r\n\r\n\r\n6\r\n\r\n\r\nGB\r\n\r\n\r\n2.0\r\n\r\n\r\n2.0\r\n\r\n\r\n78.3\r\n\r\n\r\n7\r\n\r\n\r\nLA\r\n\r\n\r\n2.8\r\n\r\n\r\n1.0\r\n\r\n\r\n76.6\r\n\r\n\r\n8\r\n\r\n\r\nARI\r\n\r\n\r\n3.3\r\n\r\n\r\n0.5\r\n\r\n\r\n76.4\r\n\r\n\r\n9\r\n\r\n\r\nSEA\r\n\r\n\r\n1.7\r\n\r\n\r\n1.2\r\n\r\n\r\n71.8\r\n\r\n\r\n10\r\n\r\n\r\nTEN\r\n\r\n\r\n2.4\r\n\r\n\r\n0.5\r\n\r\n\r\n71.0\r\n\r\n\r\n11\r\n\r\n\r\nNE\r\n\r\n\r\n3.1\r\n\r\n\r\n-0.8\r\n\r\n\r\n66.6\r\n\r\n\r\n12\r\n\r\n\r\nLAC\r\n\r\n\r\n1.1\r\n\r\n\r\n1.0\r\n\r\n\r\n66.1\r\n\r\n\r\n13\r\n\r\n\r\nMIN\r\n\r\n\r\n1.7\r\n\r\n\r\n0.2\r\n\r\n\r\n64.5\r\n\r\n\r\n14\r\n\r\n\r\nSF\r\n\r\n\r\n1.7\r\n\r\n\r\n0.0\r\n\r\n\r\n62.6\r\n\r\n\r\n15\r\n\r\n\r\nCIN\r\n\r\n\r\n-0.1\r\n\r\n\r\n0.7\r\n\r\n\r\n55.4\r\n\r\n\r\n16\r\n\r\n\r\nCLE\r\n\r\n\r\n0.8\r\n\r\n\r\n-0.5\r\n\r\n\r\n51.8\r\n\r\n\r\n17\r\n\r\n\r\nLV\r\n\r\n\r\n-1.4\r\n\r\n\r\n0.5\r\n\r\n\r\n43.4\r\n\r\n\r\n18\r\n\r\n\r\nPHI\r\n\r\n\r\n-0.7\r\n\r\n\r\n-0.5\r\n\r\n\r\n40.4\r\n\r\n\r\n19\r\n\r\n\r\nBAL\r\n\r\n\r\n-0.4\r\n\r\n\r\n-1.0\r\n\r\n\r\n38.6\r\n\r\n\r\n20\r\n\r\n\r\nNO\r\n\r\n\r\n-0.4\r\n\r\n\r\n-1.3\r\n\r\n\r\n36.6\r\n\r\n\r\n21\r\n\r\n\r\nMIA\r\n\r\n\r\n-0.8\r\n\r\n\r\n-1.0\r\n\r\n\r\n35.7\r\n\r\n\r\n22\r\n\r\n\r\nCAR\r\n\r\n\r\n-0.8\r\n\r\n\r\n-1.3\r\n\r\n\r\n33.8\r\n\r\n\r\n23\r\n\r\n\r\nDEN\r\n\r\n\r\n-2.0\r\n\r\n\r\n-0.8\r\n\r\n\r\n29.5\r\n\r\n\r\n24\r\n\r\n\r\nCHI\r\n\r\n\r\n-2.2\r\n\r\n\r\n-0.8\r\n\r\n\r\n28.3\r\n\r\n\r\n25\r\n\r\n\r\nATL\r\n\r\n\r\n-3.5\r\n\r\n\r\n0.2\r\n\r\n\r\n27.1\r\n\r\n\r\n26\r\n\r\n\r\nPIT\r\n\r\n\r\n-3.1\r\n\r\n\r\n-0.5\r\n\r\n\r\n24.6\r\n\r\n\r\n27\r\n\r\n\r\nWAS\r\n\r\n\r\n-2.9\r\n\r\n\r\n-0.8\r\n\r\n\r\n24.1\r\n\r\n\r\n28\r\n\r\n\r\nHOU\r\n\r\n\r\n-2.8\r\n\r\n\r\n-1.0\r\n\r\n\r\n23.2\r\n\r\n\r\n29\r\n\r\n\r\nDET\r\n\r\n\r\n-4.9\r\n\r\n\r\n-0.3\r\n\r\n\r\n17.0\r\n\r\n\r\n30\r\n\r\n\r\nNYG\r\n\r\n\r\n-5.0\r\n\r\n\r\n-1.5\r\n\r\n\r\n11.5\r\n\r\n\r\n31\r\n\r\n\r\nNYJ\r\n\r\n\r\n-6.2\r\n\r\n\r\n-1.0\r\n\r\n\r\n9.6\r\n\r\n\r\n32\r\n\r\n\r\nJAX\r\n\r\n\r\n-6.6\r\n\r\n\r\n-0.8\r\n\r\n\r\n9.3\r\n\r\n\r\nQuick Backtest on the 2021 Season\r\nModel accuracy for 2021 regular season games, using\r\n2020 end of season time average lead data\r\n2021 preseason Sando QB Tiers.\r\n\r\n\r\nAccuracy\r\n\r\n\r\n62.1\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-17-time-average-qb-effects/time-average-qb-effects_files/figure-html5/model_plots-1.png",
    "last_modified": "2022-04-30T23:32:18-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-10-2015-to-2021-nfl-regular-season-time-average-win-totals/",
    "title": "NFL Time Average Win Total Estimates: 2015 to 2021",
    "description": "Comparing season win totals to projections based on observed individual game time average lead.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-03-10",
    "categories": [],
    "contents": "\r\nMethod\r\nIn previous posts, I have looked explored the concept of the time average lead for NFL football games.\r\nHere, I make simple simulation-based plots that estimate where actual win totals\r\nFor a given team in a given season,\r\nCalculate the time average lead for each regular season game.\r\nGiven a team’s time average lead, estimate expected win percentage for each game (based on previous logistic regression).\r\nSimulate 10k seasons (each game a Bernoulli trial with p = expected win percentage from Item 2 above; no ties).\r\nPlot actual win total against 5th, 25th, 50th, 75th, and 95th percentiles of win total from the 10k simulated seasons (ties counted as 0.5 a win for actual win totals)\r\nExample: 2021 Pittsburgh Steelers\r\n\r\n\r\n\r\nTo start, I use a function in my hacky package nfltools to calculate the time average lead for all 2021 Steelers games.\r\nBased on the time average lead, an expected win probability is calculated for each week using the following equation.\r\n\\[E[Win Prob|Time Avg Lead] = \\frac{e^{0.31(Time Avg Lead)}}{1 + e^{0.31(Time Avg Lead)}}\\]\r\nThe weekly results are presented in the following table.\r\n\r\n\r\nWeek\r\n\r\n\r\nTeam\r\n\r\n\r\nOpponent\r\n\r\n\r\nTime Avg Lead\r\n\r\n\r\nE[Win Prob|Time Avg Lead]\r\n\r\n\r\n1\r\n\r\n\r\nPIT\r\n\r\n\r\n@ BUF\r\n\r\n\r\n-2.1\r\n\r\n\r\n34.8\r\n\r\n\r\n2\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. LV\r\n\r\n\r\n-3.6\r\n\r\n\r\n24.9\r\n\r\n\r\n3\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. CIN\r\n\r\n\r\n-8.8\r\n\r\n\r\n6.3\r\n\r\n\r\n4\r\n\r\n\r\nPIT\r\n\r\n\r\n@ GB\r\n\r\n\r\n-6.2\r\n\r\n\r\n13.1\r\n\r\n\r\n5\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. DEN\r\n\r\n\r\n8.8\r\n\r\n\r\n90.5\r\n\r\n\r\n6\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. SEA\r\n\r\n\r\n3.7\r\n\r\n\r\n75.0\r\n\r\n\r\n8\r\n\r\n\r\nPIT\r\n\r\n\r\n@ CLE\r\n\r\n\r\n-0.6\r\n\r\n\r\n45.6\r\n\r\n\r\n9\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. CHI\r\n\r\n\r\n8.6\r\n\r\n\r\n90.2\r\n\r\n\r\n10\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. DET\r\n\r\n\r\n0.6\r\n\r\n\r\n54.7\r\n\r\n\r\n11\r\n\r\n\r\nPIT\r\n\r\n\r\n@ LAC\r\n\r\n\r\n-6.7\r\n\r\n\r\n11.5\r\n\r\n\r\n12\r\n\r\n\r\nPIT\r\n\r\n\r\n@ CIN\r\n\r\n\r\n-21.2\r\n\r\n\r\n0.2\r\n\r\n\r\n13\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. BAL\r\n\r\n\r\n-2.7\r\n\r\n\r\n30.6\r\n\r\n\r\n14\r\n\r\n\r\nPIT\r\n\r\n\r\n@ MIN\r\n\r\n\r\n-13.6\r\n\r\n\r\n1.5\r\n\r\n\r\n15\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. TEN\r\n\r\n\r\n-4.6\r\n\r\n\r\n20.0\r\n\r\n\r\n16\r\n\r\n\r\nPIT\r\n\r\n\r\n@ KC\r\n\r\n\r\n-18.9\r\n\r\n\r\n0.3\r\n\r\n\r\n17\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. CLE\r\n\r\n\r\n6.5\r\n\r\n\r\n85.8\r\n\r\n\r\n18\r\n\r\n\r\nPIT\r\n\r\n\r\n@ BAL\r\n\r\n\r\n-0.8\r\n\r\n\r\n44.3\r\n\r\n\r\nGiven these weekly expected win probabilities, 10k seasons are simulated. Here’s the distribution of season win totals for the 10k simulations.\r\nThe red lines represent the quantiles used to summarize the distribution (i.e., 5th, 25th, 50th, 75th, and 95th quantiles).\r\n\r\nThis information gets collapsed into a single row of the league-wide plots.\r\nThe red line represent 0.500 winning percentage (i.e., 8.5 wins for 2021).\r\nThe interval extends from the 5th quantile to the 95th quantile.\r\nBars (i.e., “|”) are provided to mark the 25th and 75th quantiles.\r\nThe team logo is placed at the 50th quantile (i.e., median of simulations).\r\nAn “X” marks the actual win totals (where ties are counted as a 0.5 wins to reflect impact on standings).\r\n\r\nThe “X” at “9.5” wins here shows the Steelers 9 wins matched the 95th percentile outcome predicted by the model. It is unlikely to expect similar play, as measured by the time average lead, over the course of a season to result in 9 wins (let alone 9 wins and 1 tie).\r\nSeason Plots\r\n2021\r\n\r\n2020\r\n\r\n2019\r\n\r\n2018\r\n\r\n2017\r\n\r\n2016\r\n\r\n2015\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-10-2015-to-2021-nfl-regular-season-time-average-win-totals/2021_win_plot.png",
    "last_modified": "2022-03-14T23:49:19-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-28-2021-nfl-regular-season-win-luck/",
    "title": "2021 NFL Regular Season Win Luck",
    "description": "An estimate of record outperformance of underlying play based on game time average leads.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-01-28",
    "categories": [],
    "contents": "\r\nIntro\r\nThe following is an assessment of the agreement between 2021 regular season play and 2021 season win totals. The basic unit of analysis the time average lead for each game played in 2021.\r\nUsing 2011-2020 data, I made a simple model estimating the expected win percentage for a given time average lead (and also account for whether game is home or away game). As noted in a previous post, the time average lead for a game is a metric that summarizes its overall competitiveness.\r\nTen Least Likely Wins\r\n\r\n\r\nWk\r\n\r\n\r\nTeam\r\n\r\n\r\nOpponent\r\n\r\n\r\nTime Avg Lead (Points)\r\n\r\n\r\nAvg Win Percentage (%)\r\n\r\n\r\n5\r\n\r\n\r\nBAL\r\n\r\n\r\nvs IND\r\n\r\n\r\n-9.0\r\n\r\n\r\n6.5\r\n\r\n\r\n16\r\n\r\n\r\nCHI\r\n\r\n\r\n@ SEA\r\n\r\n\r\n-6.3\r\n\r\n\r\n12.2\r\n\r\n\r\n17\r\n\r\n\r\nTB\r\n\r\n\r\n@ NYJ\r\n\r\n\r\n-5.6\r\n\r\n\r\n14.5\r\n\r\n\r\n2\r\n\r\n\r\nTEN\r\n\r\n\r\n@ SEA\r\n\r\n\r\n-5.6\r\n\r\n\r\n14.6\r\n\r\n\r\n9\r\n\r\n\r\nBAL\r\n\r\n\r\nvs MIN\r\n\r\n\r\n-5.9\r\n\r\n\r\n15.2\r\n\r\n\r\n5\r\n\r\n\r\nPHI\r\n\r\n\r\n@ CAR\r\n\r\n\r\n-5.4\r\n\r\n\r\n15.6\r\n\r\n\r\n18\r\n\r\n\r\nSF\r\n\r\n\r\n@ LA\r\n\r\n\r\n-5.4\r\n\r\n\r\n15.6\r\n\r\n\r\n2\r\n\r\n\r\nBAL\r\n\r\n\r\nvs KC\r\n\r\n\r\n-5.7\r\n\r\n\r\n16.2\r\n\r\n\r\n5\r\n\r\n\r\nNE\r\n\r\n\r\n@ HOU\r\n\r\n\r\n-5.1\r\n\r\n\r\n16.5\r\n\r\n\r\n1\r\n\r\n\r\nKC\r\n\r\n\r\nvs CLE\r\n\r\n\r\n-5.3\r\n\r\n\r\n18.0\r\n\r\n\r\nGraphical Summary\r\n\r\n\r\nTable\r\n\r\n\r\nTm\r\n\r\n\r\nWins\r\n\r\n\r\nPythag Wins\r\n\r\n\r\nTime Avg Wins\r\n\r\n\r\nPythag Win Luck\r\n\r\n\r\nTime Avg Win Luck\r\n\r\n\r\nARI\r\n\r\n\r\n11\r\n\r\n\r\n10.5\r\n\r\n\r\n10.8\r\n\r\n\r\n0.2\r\n\r\n\r\n0.5\r\n\r\n\r\nATL\r\n\r\n\r\n7\r\n\r\n\r\n4.9\r\n\r\n\r\n7.1\r\n\r\n\r\n-0.1\r\n\r\n\r\n2.1\r\n\r\n\r\nBAL\r\n\r\n\r\n8\r\n\r\n\r\n8.4\r\n\r\n\r\n8.1\r\n\r\n\r\n-0.1\r\n\r\n\r\n-0.4\r\n\r\n\r\nBUF\r\n\r\n\r\n11\r\n\r\n\r\n13.1\r\n\r\n\r\n12.3\r\n\r\n\r\n-1.3\r\n\r\n\r\n-2.1\r\n\r\n\r\nCAR\r\n\r\n\r\n5\r\n\r\n\r\n5.7\r\n\r\n\r\n7.3\r\n\r\n\r\n-2.3\r\n\r\n\r\n-0.7\r\n\r\n\r\nCHI\r\n\r\n\r\n6\r\n\r\n\r\n5.9\r\n\r\n\r\n6.9\r\n\r\n\r\n-0.9\r\n\r\n\r\n0.1\r\n\r\n\r\nCIN\r\n\r\n\r\n10\r\n\r\n\r\n10.5\r\n\r\n\r\n8.7\r\n\r\n\r\n1.3\r\n\r\n\r\n-0.5\r\n\r\n\r\nCLE\r\n\r\n\r\n8\r\n\r\n\r\n7.9\r\n\r\n\r\n9.5\r\n\r\n\r\n-1.5\r\n\r\n\r\n0.1\r\n\r\n\r\nDAL\r\n\r\n\r\n12\r\n\r\n\r\n12.2\r\n\r\n\r\n10.3\r\n\r\n\r\n1.7\r\n\r\n\r\n-0.2\r\n\r\n\r\nDEN\r\n\r\n\r\n7\r\n\r\n\r\n8.9\r\n\r\n\r\n7.9\r\n\r\n\r\n-0.9\r\n\r\n\r\n-1.9\r\n\r\n\r\nDET\r\n\r\n\r\n3\r\n\r\n\r\n5.1\r\n\r\n\r\n5.3\r\n\r\n\r\n-2.3\r\n\r\n\r\n-2.1\r\n\r\n\r\nGB\r\n\r\n\r\n13\r\n\r\n\r\n10.4\r\n\r\n\r\n10.6\r\n\r\n\r\n2.4\r\n\r\n\r\n2.6\r\n\r\n\r\nHOU\r\n\r\n\r\n4\r\n\r\n\r\n4.1\r\n\r\n\r\n6.3\r\n\r\n\r\n-2.3\r\n\r\n\r\n-0.1\r\n\r\n\r\nIND\r\n\r\n\r\n9\r\n\r\n\r\n10.6\r\n\r\n\r\n11.1\r\n\r\n\r\n-2.1\r\n\r\n\r\n-1.6\r\n\r\n\r\nJAX\r\n\r\n\r\n3\r\n\r\n\r\n3.4\r\n\r\n\r\n4.0\r\n\r\n\r\n-1.0\r\n\r\n\r\n-0.4\r\n\r\n\r\nKC\r\n\r\n\r\n12\r\n\r\n\r\n11.2\r\n\r\n\r\n11.2\r\n\r\n\r\n0.8\r\n\r\n\r\n0.8\r\n\r\n\r\nLA\r\n\r\n\r\n12\r\n\r\n\r\n10.6\r\n\r\n\r\n10.3\r\n\r\n\r\n1.7\r\n\r\n\r\n1.4\r\n\r\n\r\nLAC\r\n\r\n\r\n9\r\n\r\n\r\n8.8\r\n\r\n\r\n9.3\r\n\r\n\r\n-0.3\r\n\r\n\r\n0.2\r\n\r\n\r\nLV\r\n\r\n\r\n10\r\n\r\n\r\n6.9\r\n\r\n\r\n8.2\r\n\r\n\r\n1.8\r\n\r\n\r\n3.1\r\n\r\n\r\nMIA\r\n\r\n\r\n9\r\n\r\n\r\n7.6\r\n\r\n\r\n9.0\r\n\r\n\r\n0.0\r\n\r\n\r\n1.4\r\n\r\n\r\nMIN\r\n\r\n\r\n8\r\n\r\n\r\n8.5\r\n\r\n\r\n9.8\r\n\r\n\r\n-1.8\r\n\r\n\r\n-0.5\r\n\r\n\r\nNE\r\n\r\n\r\n10\r\n\r\n\r\n12.4\r\n\r\n\r\n9.7\r\n\r\n\r\n0.3\r\n\r\n\r\n-2.4\r\n\r\n\r\nNO\r\n\r\n\r\n9\r\n\r\n\r\n9.3\r\n\r\n\r\n8.1\r\n\r\n\r\n0.9\r\n\r\n\r\n-0.3\r\n\r\n\r\nNYG\r\n\r\n\r\n4\r\n\r\n\r\n4.1\r\n\r\n\r\n5.2\r\n\r\n\r\n-1.2\r\n\r\n\r\n-0.1\r\n\r\n\r\nNYJ\r\n\r\n\r\n4\r\n\r\n\r\n4.1\r\n\r\n\r\n4.5\r\n\r\n\r\n-0.5\r\n\r\n\r\n-0.1\r\n\r\n\r\nPHI\r\n\r\n\r\n9\r\n\r\n\r\n9.9\r\n\r\n\r\n7.9\r\n\r\n\r\n1.1\r\n\r\n\r\n-0.9\r\n\r\n\r\nPIT\r\n\r\n\r\n9\r\n\r\n\r\n7.0\r\n\r\n\r\n6.4\r\n\r\n\r\n2.6\r\n\r\n\r\n2.0\r\n\r\n\r\nSEA\r\n\r\n\r\n7\r\n\r\n\r\n9.3\r\n\r\n\r\n9.8\r\n\r\n\r\n-2.8\r\n\r\n\r\n-2.3\r\n\r\n\r\nSF\r\n\r\n\r\n10\r\n\r\n\r\n10.1\r\n\r\n\r\n9.7\r\n\r\n\r\n0.3\r\n\r\n\r\n-0.1\r\n\r\n\r\nTB\r\n\r\n\r\n13\r\n\r\n\r\n12.0\r\n\r\n\r\n10.8\r\n\r\n\r\n2.2\r\n\r\n\r\n1.0\r\n\r\n\r\nTEN\r\n\r\n\r\n12\r\n\r\n\r\n10.2\r\n\r\n\r\n9.9\r\n\r\n\r\n2.1\r\n\r\n\r\n1.8\r\n\r\n\r\nWAS\r\n\r\n\r\n7\r\n\r\n\r\n6.0\r\n\r\n\r\n6.8\r\n\r\n\r\n0.2\r\n\r\n\r\n1.0\r\n\r\n\r\nMethod\r\nI put together a data set for the 2011 through 2020 regular seasons. Doing some EDA, the following histogram stood out.\r\n\r\nI fit an extremely simple logistic regression to this data set: for all home teams: win probability as a function of time average lead. The model summary looked acceptable.\r\nI binned the time average leads into 0.5 point bins, and I plotted both the average winning percentage for each bin. It looked visually like logistic regression would be appropriate, so I fit an extremely simple logistic regression to this data set: for all home teams: win probability as a function of time average lead. For the away perspective, I fit a separate model to the same data set for away games.\r\nThe “bins + model fit” plot for the away model is provided below.\r\n\r\nThe home model summary is provided below.\r\n\r\n\r\nCall:\r\nglm(formula = win ~ mean_point_diff, family = \"binomial\", data = results_df %>% \r\n    filter(home_away == \"home\"))\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-2.8364  -0.5750   0.1410   0.5969   2.6841  \r\n\r\nCoefficients:\r\n                Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)      0.08979    0.05768   1.557     0.12    \r\nmean_point_diff  0.30574    0.01248  24.495   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 3286.4  on 2399  degrees of freedom\r\nResidual deviance: 1875.6  on 2398  degrees of freedom\r\nAIC: 1879.6\r\n\r\nNumber of Fisher Scoring iterations: 6\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-28-2021-nfl-regular-season-win-luck/nfl_2021_win_luck.png",
    "last_modified": "2022-01-29T00:04:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-19-2021-nfl-regular-season-pass-defense-penalties/",
    "title": "2021 NFL Regular Season Defensive Pass Game Penalties",
    "description": "Defensive pass game penalties drawn and committed by team.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-01-19",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nIntro\r\nThe following is a collection of defensive pass game penalty statistics for the 2021 NFL regular season, derived from nflfastR play-by-play data.\r\nHere, “defensive pass” penalties include (1) defensive pass interference (DPI), (2) defensive holding, and (3) illegal contact penalties. These are all high leverage penalties that result in an automatic first down. DPIs often provide the offense with significant yardage.\r\n2021 Defensive Pass Penalty Stats\r\n\r\n\r\n\r\nSummary Graphics\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-19-2021-nfl-regular-season-pass-defense-penalties/nfl_2021_net_def_pass_penalties.png",
    "last_modified": "2022-01-20T17:02:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-15-2021-nfl-regular-season-offensive-holding/",
    "title": "2021 NFL Regular Season Offensive Holding",
    "description": "Offensive holding calls drawn and committed, by team and play type.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-01-15",
    "categories": [],
    "contents": "\r\nIntro\r\nThe following is a collection of offensive holding penalty statistics for the 2021 NFL regular season, derived from nflfastR play-by-play data.\r\nThe totals below include all called holding penalties, even if declined or offset by another penalty. Plays identified as QB scrambles are counted as “pass” play calls. Spikes and kneel downs are not counted. Two point conversion attempts are also not counted.\r\nFor the net offensive holds, positive numbers correspond to positive balance for the season: the team’s defense drew more offensive holds than they committed.\r\n\r\n\r\n\r\nLeague Wide Play Call Breakdown\r\nThe majority of play calls in the 2021 season were passes. For all offensive snaps across the NFL,\r\n58.5 percent of offensive snaps were pass play calls.\r\n41.5 percent were runs.\r\nOverall, more holds occurred on pass play calls.\r\n368 holds were committed on pass play calls\r\n315 holds were committed on run play calls.\r\nOn a per snap basis, however, run plays were more likely to result in an offensive holding. For all offensive snaps across the NFL,\r\n1.9 percent of pass play calls resulted in a hold.\r\n2.2 percent of run play calls resulted in a hold.\r\n2021 Offensive Holding Stats\r\n\r\n\r\n\r\nSummary Graphics\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-15-2021-nfl-regular-season-offensive-holding/nfl_2021_net_off_holds.png",
    "last_modified": "2022-01-20T16:36:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-14-2021-nfl-regular-season-time-average-metrics/",
    "title": "2021 NFL Regular Season Time Average Metrics",
    "description": "End of year time average metrics for the 2021 NFL regular season",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-01-14",
    "categories": [],
    "contents": "\r\nWin plot added and table updated 03/09/2022.\r\nIntro\r\nThe following is a collection of team summary statistics for the 2021 season.\r\nFor all of these statistics, the basic unit is the time average lead (regulation time only, in either points or possessions) for the 17 regular season games played in 2021.\r\nFor an introduction to time averages, check this out.\r\nGraphical Summaries\r\nPoint Differential Summaries\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nInteractive Table\r\n\r\n\r\n\r\nGlossary\r\nAbbreviations used in table:\r\nTALE: Time Average Lead Evaluation\r\nSeason time average lead with adjustment for opponent quality\r\nTALE = TAL + SoS\r\n\r\nUnits: Points\r\n\r\nTAL: Time Average Lead\r\nUnadjusted time average lead for regulation time of all regular season games.\r\nAt any give point in regulation of 2021, team __ led by an average of __ points.\r\n\r\nUnits: Points\r\n\r\nSoS: Strength of Schedule\r\nOpponent quality adjustment based on opponents’ average TAL of opponents time average lead\r\nDoes not consider games against team of interest\r\nFor example, SoS calculation for HOU does not include the games HOU’s opponents played against HOU\r\n\r\nUnits: Points\r\n\r\nMETAL: MEdian Time Average Lead\r\nThe median of individual game time average leads for season.\r\nUnits: Points\r\n\r\nMoV: Margin of Victory\r\nAggregate point differential for team, on per game basis\r\nMoV = (Season Points Scored - Season Points Allowed)/Games Played\r\n\r\nUnits: Points\r\n\r\n",
    "preview": "posts/2022-01-14-2021-nfl-regular-season-time-average-metrics/2021_tier_plot.png",
    "last_modified": "2022-03-17T22:17:44-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-02-net-offensive-holding/",
    "title": "NFL Net Offensive Holding Calls",
    "description": "A look at offensive holding calls by team and play type through Week 16.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-01-02",
    "categories": [],
    "contents": "\r\nMotivating questions\r\nThrough the black-and-gold colored glasses of a Steelers fan, it seemed like offensive holding calls basically were not called in 2020 (I can’t watch as much of other games as I’d like).\r\nI had been wondering how much a lack of holding calls may have negatively impacted the 2020 Steelers defense.\r\nHow crippling is a holding call to a drive?\r\nWhich teams have taken the most offensive holding penalties?\r\nWhich teams have drawn the most offensive holding calls?\r\nWhich teams draw the most holds while committing the fewest (i.e., drawn - taken = net holds drawn)?\r\nRecently, Matt Williamson wondered aloud whether more holds occur on run or pass plays on an episode of SNR: The Drive.\r\nMethod\r\nUsing play-by-play data via nflfastR:\r\nThrow out punt and kickoffs\r\nCount all holds called, even if declined or offset by another penalty\r\nIf not directly identified as a pass or run in the data set, infer whether the play call was a pass or run using keywords in the play description.\r\nSacks were counted as “pass” play calls.\r\nScrambles were also counted as play calls.\r\n\r\nThis is just amateur stuff for fun. I’m probably classifying some play calls incorrectly. I did a quick cross-check of total holding calls for 2021 against this website, and the results were tracking (it looks like the website includes special teams holds, whereas I threw them out).\r\nResults for 2021, Through Week 16\r\nAffect on series conversion rates\r\nBen Baldwin periodically post team series conversion rates, where a series is a success if it generates a new set of downs or scores a TD. Here’s a recent example.\r\nTypical series conversion rates for an offense are in the neighborhood of 70 percent.\r\nWhen an offense commits a holding penalty, the series conversion rate drops to 42.4 percent in 2021.\r\nNet offensive hold ranks\r\n\r\nOffensive holds committed ranks\r\n\r\nDefense offensive holding calls drawn ranks\r\n\r\nOffensive holds by play call\r\nThrough Week 16, 46.6 percent of all Offensive Holds have occurred on run plays.\r\nConclusion\r\nThis was a quick, interesting exercise.\r\nAn offensive holding call really does hurt a drive as much as it seems.\r\nIn the future, it would be interesting to look at the topic in more detail. For example, more holds were called on run plays (~54 percent) than pass plays in 2020, the opposite of 2021. Is there a trend in officiating, change in play call mix, or just random variation?\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-01-02T17:24:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-26-analytics-debate-gaps/",
    "title": "Gaps in the Analytics Debate",
    "description": "A few non-curmudgeonly issues with the \"pro analytics\" camp.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2021-12-27",
    "categories": [],
    "contents": "\r\nIssues with the “pro analytics” side of the analytics debate\r\nAs many have noted, Week 15 of the NFL helped bring the sport’s analytics debate to an unfortunate place.\r\nHere are a few non-curmudgeonly issues I have with some in the “pro-analytics” camp.\r\nCommunicating model estimates without uncertainty\r\nThe expected win probability advantage of decisions are communicated without any uncertainty. However, a ‘Kick FG’ vs. ‘Go’ 4th down analysis contains six estimated parameters.\r\n\r\nImplying expected value optimization is the only valid decision making paradigm\r\nFor environments where it is hard to anticipate every contingency, the expected value paradigm that dominates the NFL decision making discourse has legitimate alternatives. However, anyone who questions the “analytics experts” is frequently cast as being unsophisticated or unable to grasp “the math.”\r\nGary Klein’s Recognition-primed Decision model describes how experienced experts in many domains can reliably make high-quality, time-compressed decisions in a completely different manner.\r\nGerd Gigerenzer has offered a compelling defense of Gut Decisions in many domains, demonstrating where heuristics can be superior to optimization via complex algorithms/models.\r\n\r\n\r\nModel estimates without uncertainty\r\nLet’s look at an example. Just before the half in Week 15 against the KC Chiefs, Brandon Staley faced the following decision.\r\nLink: https://twitter.com/ben_bot_baldwin/status/1471673042371858433The “medium, go for it” recommendation is the result of an expected value calculation based on six estimated parameters.\r\nTwo “conversion rate” estimates\r\nFor example, P(Score TD | ‘Go for it’) = 60%.\r\n\r\nFour win probability estimates for the possible resulting game states\r\nFor example, P(Win | Failed ‘Kick FG’ attempt) = 53%.\r\n\r\nWhat happens if we propagate some uncertainty into the estimates that are inputs into the expected value calculation? (Like a less sophisticated cousin of this awesome Shiny app, only that also considers that the Win Probability model could also have small errors.)\r\nAssume FG estimate is very accurate (i.e., error N(0, 0.25%)).\r\nFor other parameters, assume estimates is within 5% margin of error (i.e., errors N(0, 2.5%)).\r\nAre errors of such magnitude sufficiently charitable to the model? For a toy exercise, I think so. That would seem like an excellent model to me! Sources of such errors could be attributable to injuries, weather, game plan, etc. that couldn’t be captured by a model.\r\n\r\n\r\n\r\nDistribution of Expected Win Probability Results with Random Error in Underlying ParametersWhat’s the point?\r\nSome use an expected win probability advantage of at least 1 WP as a threshold to judge when a decisions is “correct.” Assume we accept the expected value paradigm (i.e., ignore my second complaint at present).\r\nPropagating some modest uncertainty in the estimated parameters underlying the expected value calculation suggests that categorizing coaching quality based on agreement with a specific model may not be justified for decisions with relatively narrow margins.\r\nIn this simulation, 35.9 percent of the simulated expected value calculations fail to exceeded the 1 WP advantage threshold for the aggressive “Go for it” decision.\r\nI’m not suggesting I’ve done this optimally (or perhaps even acceptably) for this example, but I think it is important to express uncertainty with estimates. In the future, I hope the “pro analytics” side of the debate finds a way to communicate uncertainty with their estimates.\r\nExpected value optimization as only valid paradigm\r\nI highly recommend Gary Klein’s Sources of Power. Klein studied how individuals in complex domains (e.g., emergency responders, combatants) actually make decisions. The result was the (descriptive) Recognition-primed Decision (RPD) model.\r\nHere’s my rough summary of the findings relevant to NFL coaching decision discussion.\r\nContrary to prevailing wisdom, it is novice decision makers in many domains that make decisions by comparing the projected consequences of multiple alternative courses of action.\r\nExpert decision makers, in contrast, draw upon cues in the environment and recognize a high quality candidate solution using their prior experience.\r\nThis candidate solution is interrogated for potential problems via a (comparatively) simple mental simulation.\r\nField studies in these domains demonstrate this single-path analysis mode is an improvement over comparative analysis (similar to the expected win probability analysis).\r\n\r\nHere’s a recent example of “expected value optimization is the only valid decision making paradigm” in the analytics debate.\r\nLink: https://twitter.com/benbbaldwin/status/1473012593547751424I see a lot of Klein’s descriptive model of expert “naturalistic decision making” from Coach Belichick, including critical reflection on previous decisions to learn like an expert. If I were a Pats fan, I would not be discouraged.\r\nWhat’s the point?\r\nCoaches are trying to find the most probable single trajectory to victory; the expected value of a strategy will not be realized for a single decision. Additionally, it is very plausible to me that an expert coach could identify valid reasons – unique to the specific game situation – why an expected win probability model may not apply perfectly for the specific decision being made.\r\nThere is a lot of serious academic and other work that support the view that there are alternatives that can be superior to expected value optimization in some settings. Their discussion appears entirely absent from the slice of the analytics debate to which I am exposed.\r\nIf I were to anticipate criticisms, it would be something like the following: win probability models are now of sufficient quality that NFL games are now “smallish” enough words that no biased human expert can systematically be superior to the model. I’m skeptical.\r\nA few other book recommendations that explore alternatives to “rational” optimization in the applicable contexts:\r\nRadical Uncertainty, by John Kay and Mervyn King\r\nGut Feelings: The Intelligence of the Unconscious, by Gerd Gigerenzer\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-26-analytics-debate-gaps/lac_wk_15_sim_with_parameter_error_end_half.png",
    "last_modified": "2022-01-19T16:16:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-28-time-average-nfl-metrics/",
    "title": "Time Average Metrics for NFL Team Quality",
    "description": "An alternative to margin of victory that better reflects the path to the final result.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2021-11-28",
    "categories": [],
    "contents": "\r\nRevised 12/22/2021 and 01/10/2022.\r\nWhat is a “time average lead”?\r\nThe time average point lead for a game is a coarse-grained metric that summarizes its overall competitiveness.\r\nLet’s consider a simple example. In Week 15 of the 2021 season, the New Orleans Saints went to Tampa Bay and upset the Bucs, winning 9-0.\r\nThis example is simple because there were only three scoring plays. Also, since the Bucs were shut out, the New Orleans score is equal to the New Orleans lead throughout the game. (If New Orleans had trailed at some point in the game, their “lead” would have been a negative number for that part of the game.)\r\nLet’s look at a plot of the Saints shutout.\r\n\r\n\r\n\r\n\r\nThe “time average lead” is just a weighted average of the team’s leads throughout the game, weighted by the fraction of the game they held each lead. Geometrically, it is the sum of the area under the “lead” curve, divided by 60 minutes played. The plot is annotated to show the areas of the three lead intervals and the time average lead for the game.\r\nWhy “time average” point differentials?\r\nA few notable features of time average lead as a point-estimate game summary:\r\nThe units of the metric is points, which is intuitive to football fans.\r\nFor better or worse, the value of the metric relies on the assumption that a time average is informative. Time averages are often informative for processes that are at least partially path-dependent. A time average is just an average and therefore has the advantage of not relying on other modeling assumptions.\r\nIf you’re curious, the 2021 NFL season time average results (and other derived metrics) can be found here.\r\nA few other concrete examples\r\nConsider two more complex games from Week 1 of 2021.\r\nPIT 23, BUF 16\r\nSF 41, DET 33\r\n\r\n\r\n\r\nThe margin of victory for the Steelers and the 49ers was very similar. However, the time average point differential is significantly different.\r\nTeam\r\nMargin of Victory\r\nTime Average Lead\r\nPIT\r\n7\r\n-2.1\r\nSF\r\n8\r\n13.3\r\nLet’s compare the Win Probability graph for the two games.\r\n\r\n\r\nDespite the similar margin of victory for both teams, the Win Probability graphs reflect very different paths to the result.\r\nThe Steelers surged to take control in the 4th quarter, fueled by a Bills turnover on downs near midfield and a Pittsburgh TD on a blocked punt.\r\nThe Lions never threatened the 49ers until it was too late. Detroit scored two TDs and converted two 2-point conversions after the 2-minute warning in the 4th quarter. On top of defensive lapses, the 49ers allowed a successful onside kick and fumbled while trying to kill clock.\r\nThe Steelers made big plays late to win a game they kept close, while the 49ers controlled the game until hilarity ensued inside of 2-minutes. The time average point differential better reflects the path these games took to the final outcome than the final margin of victory.\r\nRecent NFL seasons\r\nThe following shows the distribution of time average game results (from the home team’s perspective) for all regular season games in 2011 to 20202. \r\nAs expected,\r\nMost positive time average differentials result in wins, while most negative time average point differentials result in losses.\r\nThere is evidence of a home team advantage: on average, home teams lead by ~1.4 points at any given point in regulation time for the decade considered of regular season games considered.\r\nResources and sources of inspiration\r\nInspired by an Ole Peters talk on Ergodicity Economics, I decided to investigate time averages in NFL games. Like life, my prior is that a football game is path dependent. Game script matters. Play calling goals vary with game situation.\r\nAt the time, I was reading 2021 NFL season previews and retrospectives on the 2020 season. The hypothesis was this: because of path dependence, time averages might enable better retrospective assessment of regular season team quality than season point differential and other aggregate metrics (e.g., Pythagorean Win Expectation).\r\nI the most amateur of R users. This post contains adapted code and inspiration from:\r\nThe tidyverse.\r\nThe nflfastR of the nflverse.\r\nTom Mock’s excellent posts on plotting images and tables\r\nShannon Pileggi’s excellent blog post on creating your own R package.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-28-time-average-nfl-metrics/no_tb_tal.png",
    "last_modified": "2022-01-19T16:18:35-05:00",
    "input_file": {}
  }
]
