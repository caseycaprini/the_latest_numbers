[
  {
    "path": "posts/2024-01-10-partivwinprobabilitystochasticprocess/",
    "title": "Part IV: Win Probability as a Stochastic Process",
    "description": "Some Top-down Analysis of nflfastR vegas_wp Model.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2024-01-10",
    "categories": [],
    "contents": "\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\n\r\n\r\n\r\nget_midpoint <- function(cut_label) {\r\n  mean(as.numeric(unlist(strsplit(gsub(\"\\\\(|\\\\)|\\\\[|\\\\]\", \"\",\r\n                                       as.character(cut_label)), \",\"))))\r\n}\r\n\r\n\r\nraw_df <-\r\n  load_pbp(2017:2023)\r\n\r\nhome_favorite <-\r\n  raw_df %>%\r\n  group_by(game_id) %>%\r\n  slice_head(n = 1) %>%\r\n  filter(vegas_home_wp >= 0.5) %>%\r\n  pull(game_id)\r\n\r\ndf <-\r\n  raw_df %>%\r\n  filter(qtr %in% c(1, 2, 3, 4)) %>%\r\n  select(game_id, desc,\r\n         vegas_home_wp, vegas_home_wpa,\r\n         game_seconds_remaining) %>%\r\n  mutate(favorite_wp = ifelse(\r\n    game_id %in% home_favorite,\r\n    vegas_home_wp,\r\n    1 - vegas_home_wp\r\n  )) %>%\r\n  mutate(favorite_wpa = ifelse(\r\n    game_id %in% home_favorite,\r\n    vegas_home_wpa,\r\n    -1 * vegas_home_wpa\r\n  ))\r\n\r\ninit_favorite_wp <-\r\n  df %>%\r\n  group_by(game_id) %>%\r\n  slice_head(n = 1) %>%\r\n  mutate(init_favorite_wp_binned = cut_width(\r\n    favorite_wp, width = 0.05, center = 0.525\r\n  )) %>%\r\n  mutate(init_favorite_wp_binned = map_dbl(\r\n    init_favorite_wp_binned, \r\n    get_midpoint\r\n    )\r\n  ) %>%\r\n  select(game_id, favorite_wp, init_favorite_wp_binned)\r\n\r\ndf_init_wp <-\r\n  df %>%\r\n  filter(desc != \"GAME\") %>%\r\n  mutate(game_seconds_played = 3600 - game_seconds_remaining) %>%\r\n  mutate(game_minutes_played_binned = cut_width(\r\n    game_seconds_played/60, width = 0.5, center = 0.0\r\n  )) %>%\r\n  group_by(game_id, game_minutes_played_binned) %>%\r\n  summarize(favorite_wp = mean(favorite_wp),\r\n            .groups = \"drop\") %>%\r\n  left_join(\r\n    init_favorite_wp %>%\r\n      select(game_id, init_favorite_wp_binned),\r\n    by = c(\"game_id\" = \"game_id\")\r\n  ) %>%\r\n  group_by(game_minutes_played_binned, init_favorite_wp_binned) %>%\r\n  summarize(min = min(favorite_wp),\r\n            q_25 = quantile(favorite_wp, 0.25),\r\n            q_50 = quantile(favorite_wp, 0.5),\r\n            q_75 = quantile(favorite_wp, 0.75),\r\n            max = max(favorite_wp),\r\n            .groups = \"drop\") %>%\r\n  mutate(game_minutes_played_binned =\r\n           map_dbl(game_minutes_played_binned,\r\n                   get_midpoint)\r\n         ) %>%\r\n  pivot_longer(min:max,\r\n               names_to = \"metric\",\r\n               values_to = \"value\") %>%\r\n  mutate(metric = factor(metric,\r\n                         levels = c(\"min\",\r\n                                    \"q_25\",\r\n                                    \"q_50\",\r\n                                    \"q_75\",\r\n                                    \"max\")\r\n                         )\r\n         )\r\n\r\ndf_init_wp %>%\r\n  ggplot(\r\n    aes(\r\n      x = game_minutes_played_binned,\r\n      y = value\r\n    )\r\n  ) +\r\n  geom_point(aes(color = metric)) +\r\n  theme_light() +\r\n  facet_wrap(~ init_favorite_wp_binned) +\r\n  scale_x_continuous(\r\n    breaks = seq(0, 60, by = 15),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_continuous(labels = scales::percent) +\r\n  labs(x = \"Minutes Played\",\r\n       y = \"WP\")\r\n\r\n\r\nShow code\r\n\r\ndf_init_wp %>%\r\n  filter(game_minutes_played_binned == 58) %>%\r\n  filter(metric == \"q_50\") %>%\r\n  ggplot(aes(x = init_favorite_wp_binned,\r\n             y = value)) +\r\n  geom_vline(xintercept = c(0.5, 1.0),\r\n             linetype = \"dashed\") +\r\n  geom_hline(yintercept = c(0.5, 1.0),\r\n             linetype = \"dashed\") +\r\n  geom_abline(intercept = 0.0, slope = 1,\r\n              linetype = \"dashed\",\r\n              color = \"dark gray\") +\r\n  geom_path(color = \"blue\") +\r\n  geom_point() +\r\n  theme_light() +\r\n  scale_x_continuous(\r\n    labels = scales::percent,\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_continuous(\r\n    labels = scales::percent,\r\n    minor_breaks = NULL\r\n  ) +\r\n  labs(x = \"Opening Kickoff WP\",\r\n       y = \"Median WP Near Final 2-minute Warning\")\r\n\r\n\r\nShow code\r\n\r\nhalf_favorite_df <-\r\n  df %>%\r\n  filter(game_seconds_remaining <= 1800) %>%\r\n  group_by(game_id) %>%\r\n  slice_head(n = 1) %>%\r\n  mutate(half_favorite_wp = ifelse(vegas_home_wp > 0.5,\r\n                                   vegas_home_wp,\r\n                                   1 - vegas_home_wp),\r\n         half_favorite = ifelse(vegas_home_wp > 0.5,\r\n                                \"home\",\r\n                                \"away\")\r\n         ) %>%\r\n  mutate(half_favorite_wp_binned = cut_width(\r\n    half_favorite_wp, width = 0.05, center = 0.525\r\n  )) %>%\r\n  mutate(half_favorite_wp_binned =\r\n           map_dbl(half_favorite_wp_binned, \r\n                   get_midpoint)\r\n         ) %>%\r\n  select(game_id, half_favorite_wp_binned, half_favorite)\r\n\r\ndf_2nd_half <-\r\n  df %>%\r\n  filter(game_seconds_remaining <= 1800) %>%\r\n  mutate(game_seconds_played = 3600 - game_seconds_remaining) %>%\r\n  mutate(game_minutes_played_binned = cut_width(\r\n    game_seconds_played/60, width = 0.5, center = 0.0\r\n  )) %>%\r\n  left_join(\r\n    half_favorite_df %>%\r\n      select(game_id, half_favorite_wp_binned, half_favorite),\r\n    by = c(\"game_id\" = \"game_id\")\r\n  ) %>%\r\n  mutate(half_favorite_wp = ifelse(half_favorite == \"home\",\r\n                                   vegas_home_wp,\r\n                                   1 - vegas_home_wp)\r\n  ) %>%\r\n  group_by(game_id, game_minutes_played_binned) %>%\r\n  summarize(\r\n    half_favorite_wp_binned = half_favorite_wp_binned,\r\n    half_favorite_wp = mean(half_favorite_wp),\r\n    .groups = \"drop\") %>%\r\n  group_by(game_minutes_played_binned, half_favorite_wp_binned) %>%\r\n  summarize(min = min(half_favorite_wp),\r\n            q_25 = quantile(half_favorite_wp, 0.25),\r\n            q_50 = quantile(half_favorite_wp, 0.5),\r\n            q_75 = quantile(half_favorite_wp, 0.75),\r\n            max = max(half_favorite_wp),\r\n            .groups = \"drop\") %>%\r\n  mutate(game_minutes_played_binned =\r\n           map_dbl(game_minutes_played_binned,\r\n                   get_midpoint)\r\n  ) %>%\r\n  pivot_longer(min:max,\r\n               names_to = \"metric\",\r\n               values_to = \"value\") %>%\r\n  mutate(metric = factor(metric,\r\n                         levels = c(\"min\",\r\n                                    \"q_25\",\r\n                                    \"q_50\",\r\n                                    \"q_75\",\r\n                                    \"max\")\r\n  )\r\n  )\r\n\r\ndf_2nd_half %>%\r\n  ggplot(\r\n    aes(\r\n      x = game_minutes_played_binned,\r\n      y = value\r\n    )\r\n  ) +\r\n  geom_point(aes(color = metric)) +\r\n  theme_light() +\r\n  facet_wrap(~ half_favorite_wp_binned) +\r\n  scale_x_continuous(\r\n    breaks = seq(0, 60, by = 15),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_continuous(labels = scales::percent) +\r\n  coord_cartesian(xlim = c(0, 60)) +\r\n  labs(x = \"Minutes Played\",\r\n       y = \"WP\")\r\n\r\n\r\nShow code\r\n\r\ndf_2nd_half %>%\r\n  filter(game_minutes_played_binned == 58) %>%\r\n  filter(metric == \"q_50\") %>%\r\n  ggplot(aes(x = half_favorite_wp_binned,\r\n             y = value)) +\r\n  geom_vline(xintercept = c(0.5, 1.0),\r\n             linetype = \"dashed\") +\r\n  geom_hline(yintercept = c(0.5, 1.0),\r\n             linetype = \"dashed\") +\r\n  geom_abline(intercept = 0.0, slope = 1,\r\n              linetype = \"dashed\",\r\n              color = \"dark gray\") +\r\n  geom_path(color = \"blue\") +\r\n  geom_point() +\r\n  theme_light() +\r\n  scale_x_continuous(\r\n    labels = scales::percent,\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_continuous(\r\n    labels = scales::percent,\r\n    minor_breaks = NULL\r\n  ) +\r\n  labs(x = \"WP to Open 2nd Half\",\r\n       y = \"Median WP Near Final 2-minute Warning\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-01-10-partivwinprobabilitystochasticprocess/partivwinprobabilitystochasticprocess_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2024-01-10T01:03:11-05:00",
    "input_file": "partivwinprobabilitystochasticprocess.knit.md"
  },
  {
    "path": "posts/2024-01-02-post-week-17-tails-update/",
    "title": "Post Week 17 TAILS Update",
    "description": "Time Average Inpredictable Line Smidge",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2024-01-02",
    "categories": [],
    "contents": "\r\nLoad Libraries\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\nlibrary(nfltools)\r\nlibrary(nflplotR)\r\n\r\n\r\nGet Time Average Lead Data\r\n\r\n\r\nShow code\r\n\r\nraw_df <-\r\n  nfltools::nfl_mvt_season(2023)\r\n\r\n\r\nCalculate TAILS Adjustment\r\n\r\n\r\nShow code\r\n\r\ntails <-\r\n  raw_df %>%\r\n  filter(!(is.na(opponent))) %>%\r\n  mutate(\r\n    adj_time_avg_lead = time_avg_lead + (0.5 * team_spread)\r\n  ) %>%\r\n  group_by(team) %>%\r\n  slice_tail(n = 5) %>%\r\n  summarize(\r\n    tails = mean(adj_time_avg_lead),\r\n    .groups = \"drop\"\r\n  ) %>%\r\n  mutate(\r\n    tails = 0.2 * tails\r\n  )\r\n\r\ninpred <-\r\n  # This is a function I wrote as my first bit of\r\n  # webscraping code. It retrieves info from the\r\n  # inpredictable power ratings at the time it is run.\r\n  get_current_inpredictable()\r\n\r\ninpred <-\r\n  left_join(\r\n    inpred,\r\n    tails\r\n  ) %>%\r\n  mutate(\r\n    adj_gpf = gpf + tails\r\n  )\r\n\r\n\r\nPlot\r\n\r\n\r\nShow code\r\n\r\np_adj_gpf <-\r\n  inpred %>%\r\n  ggplot(\r\n    aes(\r\n      x = gpf,\r\n      y = reorder(factor(team),\r\n                  adj_gpf)\r\n    )\r\n  ) +\r\n  geom_vline(\r\n    xintercept = 0,\r\n    linetype = \"dashed\"\r\n  ) +\r\n  geom_segment(\r\n    aes(\r\n      x = gpf,\r\n      xend = adj_gpf,\r\n      yend = team\r\n    ),\r\n    arrow = arrow(\r\n      type = \"closed\",\r\n      length = unit(\r\n        0.1, \"cm\"\r\n      )\r\n    )\r\n  ) +\r\n  geom_point() +\r\n  geom_nfl_logos(\r\n    aes(\r\n      team_abbr = team,\r\n      x = adj_gpf),\r\n    width = 0.04,\r\n    alpha = 0.4\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = seq(-18, 18, by = 3),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_discrete(\r\n    labels = NULL,\r\n    breaks = NULL\r\n  ) +\r\n  labs(\r\n    x = \"TAILS Adjusted Generic Points Favored\",\r\n    y = NULL,\r\n    title = \"Post Week 17 NFL Power Rankings\",\r\n    subtitle = \"As of 1/2/2024.\",\r\n    caption = \"Baseline: inpredictable.com Betting Market Ratings\\nMy silly adjustment calculated using nflfastR data.\"\r\n  ) +\r\n  theme_light()\r\n\r\nggsave(\r\n  \"post_week_17_tails_adj_gpf.png\",\r\n  plot = p_adj_gpf,\r\n  width = 6,\r\n  height = 4.5,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nTable\r\n\r\n\r\nShow code\r\n\r\ninpred %>%\r\n  select(\r\n    team, gpf, tails, adj_gpf\r\n  ) %>%\r\n  arrange(\r\n    desc(adj_gpf)\r\n  ) %>%\r\n  mutate(\r\n    tails = round(tails, 2),\r\n    adj_gpf = round(adj_gpf, 2),\r\n    rank = row_number()\r\n  ) %>%\r\n  knitr::kable()\r\n\r\nteam\r\ngpf\r\ntails\r\nadj_gpf\r\nrank\r\nBUF\r\n6.5\r\n0.62\r\n7.12\r\n1\r\nSF\r\n6.3\r\n-0.24\r\n6.06\r\n2\r\nDAL\r\n6.4\r\n-0.49\r\n5.91\r\n3\r\nBAL\r\n2.4\r\n0.93\r\n3.33\r\n4\r\nLA\r\n2.2\r\n0.82\r\n3.02\r\n5\r\nMIA\r\n2.4\r\n0.60\r\n3.00\r\n6\r\nPIT\r\n2.9\r\n0.00\r\n2.90\r\n7\r\nDET\r\n1.9\r\n0.58\r\n2.48\r\n8\r\nPHI\r\n2.7\r\n-0.48\r\n2.22\r\n9\r\nKC\r\n1.9\r\n-1.17\r\n0.73\r\n10\r\nGB\r\n-0.1\r\n0.68\r\n0.58\r\n11\r\nTB\r\n-0.7\r\n0.64\r\n-0.06\r\n12\r\nNO\r\n-0.2\r\n0.07\r\n-0.13\r\n13\r\nCIN\r\n-0.1\r\n-0.35\r\n-0.45\r\n14\r\nSEA\r\n-0.3\r\n-0.16\r\n-0.46\r\n15\r\nLV\r\n-2.3\r\n1.73\r\n-0.57\r\n16\r\nCLE\r\n-1.6\r\n0.91\r\n-0.69\r\n17\r\nCHI\r\n-2.3\r\n1.36\r\n-0.94\r\n18\r\nJAX\r\n-0.5\r\n-0.85\r\n-1.35\r\n19\r\nDEN\r\n-1.9\r\n-0.25\r\n-2.15\r\n20\r\nMIN\r\n-1.8\r\n-0.57\r\n-2.37\r\n21\r\nIND\r\n-2.3\r\n-0.28\r\n-2.58\r\n22\r\nHOU\r\n-2.4\r\n-0.29\r\n-2.69\r\n23\r\nATL\r\n-2.6\r\n-0.23\r\n-2.83\r\n24\r\nLAC\r\n-2.1\r\n-1.49\r\n-3.59\r\n25\r\nNE\r\n-4.8\r\n0.55\r\n-4.25\r\n26\r\nTEN\r\n-5.4\r\n0.21\r\n-5.19\r\n27\r\nNYG\r\n-5.9\r\n0.13\r\n-5.77\r\n28\r\nARI\r\n-5.6\r\n-0.41\r\n-6.01\r\n29\r\nNYJ\r\n-5.8\r\n-0.27\r\n-6.07\r\n30\r\nCAR\r\n-6.7\r\n-0.89\r\n-7.59\r\n31\r\nWAS\r\n-7.4\r\n-1.44\r\n-8.84\r\n32\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-01-02-post-week-17-tails-update/post_week_17_tails_adj_gpf.png",
    "last_modified": "2024-01-02T23:45:39-05:00",
    "input_file": "post-week-17-tails-update.knit.md"
  },
  {
    "path": "posts/2023-12-28-partiiiwinprobabilitystochasticprocess/",
    "title": "Part III: Win Probability as a Stochastic Process",
    "description": "Simulating the trajectories.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-12-28",
    "categories": [],
    "contents": "\r\nBackground\r\nIn Part I, I introduced the quantitative finance idea of valuing something on the basis of a stochastic process via a simplified Bachelier model.\r\nIn Part II, I discussed some theoretical properties of binary forecast streams, and I provided evidence of them holding for the Vegas-informed nflfastR Win Probability (WP) model estimates. I also developed a simple stochastic process model for Win Probability Added (WPA) that has these properties and match the nflfastR data well enough to provide a toy dynamic for WP streams.\r\n\r\n\\(WPA \\sim N(\\mu = 0, SD = \\sigma_{WP, t})\\)\r\n\\(\\sigma_{WP, t} = 0.07 (0.5 - \\lvert{WP - 0.5}\\rvert) + \\frac{75 ((0.5 - \\lvert{WP - 0.5}\\rvert))}{3875 - t}\\)\r\nHere, t is game seconds played.\r\nThe “0.5 - |WP - 0.5|” term is just the distance from certainty (i.e, WP = 0 or WP = 1).\r\n\r\nAn Example\r\nInitial conditions:\r\nInitial WP = 67%\r\nTime-in-game: Opening Kickoff\r\nThe code below simulates a single WP stream using these initial conditions and the dynamic above. A discrete stochastic process is used: a WPA is simulated every 24 seconds. (Note: To ensure a simulated WPA does not result in a WP outside the bounds of [0, 1], resulting WPs get checked each step and an out of bounds WP manually set at 0.5% or 99.5%.)\r\n\r\n\r\nShow code\r\n\r\n# Jaylen Warren\r\nset.seed(30)\r\n\r\nt_single = seq(0, 3600, by = 24)\r\nwp_single = rep(NA_real_, length(t_single))\r\nwp_single[1] = 2/3\r\n\r\n# i - 1 steps  \r\nfor(i in 1:(length(t_single) - 1)){\r\n  dist_from_certainty = 0.5 - abs(wp_single[i] - 0.5)\r\n    \r\n  wpa_sd_single = (0.07*dist_from_certainty) + ((75*dist_from_certainty)/(3875 - t_single[i]))\r\n    \r\n  wp_single[i + 1] = wp_single[i] + rnorm(n = 1, mean = 0, sd = wpa_sd_single)\r\n    \r\n  if(wp_single[i + 1] > 1){wp_single[i + 1] = 0.995}\r\n  if(wp_single[i + 1] < 0){wp_single[i + 1] = 0.005}\r\n}\r\n\r\ndf_single <-\r\n  data.frame(t_single, wp_single)\r\n\r\n\r\nThe single (losing) trajectory is plotted below.\r\n\r\n\r\nShow code\r\n\r\ndf_single %>%\r\n  ggplot(aes(\r\n    x = t_single,\r\n    y = wp_single\r\n  )) +\r\n  geom_hline(yintercept = c(0, 1)) +\r\n  geom_vline(xintercept = c(0, 60 * 60)) +\r\n  geom_vline(xintercept = c(60 * 15, 60 * 30, 60 * 45),\r\n             linetype = \"dashed\") +\r\n  scale_y_continuous(labels = scales::percent) +\r\n  scale_x_continuous(breaks = seq(0, 3600, by = 5 * 60),\r\n                     minor_breaks = NULL) +\r\n  geom_path(color = \"blue\") +\r\n  theme_light() +\r\n  labs(x = \"Seconds Played\",\r\n       y = \"Simulated WP\",\r\n       title = \"A Single Simulated Trajectory\",\r\n       subtitle = \"Assumed Pre-game WP: 67%\")\r\n\r\n\r\n\r\nSimilar to the Bachelier model in Part I, I’m really interested in an ensemble of final WPs. I’ll put the simulation into a function (with parameters for initial conditions for WP and time-in-game.)\r\n\r\n\r\nShow code\r\n\r\nsim_stream <- function(game_seconds_played = 0, wp = 0.5){\r\n  t = seq(game_seconds_played, 3600, by = 24)\r\n  wp = rep(wp, length(t))\r\n  \r\n  for(i in 1:(length(t) - 1)){\r\n    dist_from_certainty = 0.5 - abs(wp[i] - 0.5)\r\n    \r\n    wpa_sd = (0.07*dist_from_certainty) + ((75*dist_from_certainty)/(3875 - t[i]))\r\n    \r\n    wp[i + 1] = wp[i] + rnorm(n = 1, mean = 0, sd = wpa_sd)\r\n    \r\n    if(wp[i + 1] > 1){wp[i + 1] = 0.995}\r\n    if(wp[i + 1] < 0){wp[i + 1] = 0.005}\r\n  }\r\n  \r\n  return(wp[length(t)])\r\n}\r\n\r\n\r\nLet’s simulate 10k results (initial WP = 67%, opening kickoff) and plot the distribution.\r\n\r\n\r\nShow code\r\n\r\nwp_67_t_0 <-\r\n  replicate(\r\n    10000,\r\n    sim_stream(\r\n      game_seconds_played = 0,\r\n      wp = 2/3\r\n    )\r\n  )\r\n\r\nwp_67_t_0 <-\r\n  data.frame(\r\n    sim_id = 1:length(wp_67_t_0),\r\n    wp_final = wp_67_t_0\r\n  )\r\n\r\nwp_67_t_0 %>%\r\n  ggplot(aes(x = wp_final)) +\r\n  geom_histogram(\r\n    fill = \"blue\",\r\n    color = \"black\",\r\n    binwidth = 0.05\r\n  ) +\r\n  theme_light() +\r\n  scale_x_continuous(minor_breaks = NULL,\r\n                     labels = scales::percent) +\r\n  scale_y_continuous(minor_breaks = NULL) +\r\n  labs(x = \"Final Simulated WP\",\r\n       y = \"Count\",\r\n       title = \"Distribution of Final WP for 10k Simulated Trajectories\",\r\n       subtitle = \"Assumed Pre-game WP: 67%\")\r\n\r\n\r\n\r\n(Visually, this reminds me of a beta distribution with both shape parameters < 1.) The mean final WP is 66.4%, and the median final WP is 88.4%.\r\nLet’s compare this distribution with a simulation with the same initial WP (67%) but with only the 4th quarter left to play (2712 seconds played, 14.8 minutes remaining).\r\n\r\n\r\nShow code\r\n\r\nwp_67_t_2712 <-\r\n  replicate(\r\n    10000,\r\n    sim_stream(\r\n      game_seconds_played = 2712,\r\n      wp = 2/3\r\n    )\r\n  )\r\n\r\nwp_67_t_2712 <-\r\n  data.frame(\r\n    sim_id = 1:length(wp_67_t_2712),\r\n    wp_final = wp_67_t_2712\r\n  )\r\n\r\nwp_67_t_2712 %>%\r\n  ggplot(aes(x = wp_final)) +\r\n  geom_histogram(\r\n    fill = \"blue\",\r\n    color = \"black\",\r\n    binwidth = 0.05\r\n  ) +\r\n  theme_light() +\r\n  scale_x_continuous(minor_breaks = NULL,\r\n                     labels = scales::percent) +\r\n  scale_y_continuous(minor_breaks = NULL) +\r\n  labs(x = \"Final Simulated WP\",\r\n       y = \"Count\",\r\n       title = \"Distribution of Final WP for 10k Simulated Trajectories\",\r\n       subtitle = \"Start of 4th Quarater, Initial WP: 67%\")\r\n\r\n\r\n\r\nThe mean final WP of this distribution is 66.8%, and the median final WP is 81%.\r\nFinally, let’s compare this distribution with a simulation with the same initial WP (67%) but with only 2 minutes left to play (3480 seconds played, 2 minutes remaining).\r\n\r\n\r\nShow code\r\n\r\nwp_67_t_3480 <-\r\n  replicate(\r\n    10000,\r\n    sim_stream(\r\n      game_seconds_played = 3480,\r\n      wp = 2/3\r\n    )\r\n  )\r\n\r\nwp_67_t_3480 <-\r\n  data.frame(\r\n    sim_id = 1:length(wp_67_t_3480),\r\n    wp_final = wp_67_t_3480\r\n  )\r\n\r\nwp_67_t_3480 %>%\r\n  ggplot(aes(x = wp_final)) +\r\n  geom_histogram(\r\n    fill = \"blue\",\r\n    color = \"black\",\r\n    binwidth = 0.05\r\n  ) +\r\n  theme_light() +\r\n  scale_x_continuous(minor_breaks = NULL,\r\n                     labels = scales::percent) +\r\n  scale_y_continuous(minor_breaks = NULL) +\r\n  labs(x = \"Final Simulated WP\",\r\n       y = \"Count\",\r\n       title = \"Distribution of Final WP for 10k Simulated Trajectories\",\r\n       subtitle = \"Start of 4th Quarater, Initial WP: 67%\")\r\n\r\n\r\n\r\n(Visually, this reminds me of a beta distribution with both shape parameters > 1. This suggests that WP could be thought of as resolving to a near point estimate with magnitude of shape parameters increasing as time remaining goes to 0.) The mean final WP of this distribution is 66.5%, and the median final WP is 71.5%.\r\nTime Invariance of WP?\r\nThis stochastic process approach let’s us model questions like time preference.\r\nWhich situation would you prefer?\r\n* Option 1: WP 67%, opening kickoff\r\n* Option 2: WP 67% with 2 minutes remaining\r\nThe examples above show a divergence between the mean and median of 10k trajectories. Since the median is the 50th percentile result, the stochastic process model supports a preference for the Option 1. This time element appears to be absent from more public WP discussions I’ve encountered.\r\nThere’s a intuitive story that can support the conclusion as well: If you’re a 67% WP favorite at kickoff, you’d likely be disappointed to only be a 67% WP favorite at the 2-minute warning in the 4th quarter. It reminds me of a Buffet quote:\r\n\r\n“Time is the friend of the wonderful business:\r\nyou keep compounding, it keeps doing more business,\r\nand keeps making more money. Time is the enemy of\r\nthe lousy business.”\r\nWarren Buffet\r\n\r\nAnyways, the stochastic process approach allows you to simulate a bunch of streams at different times-in-game with a fixed WP and compare the results.\r\nHere’s a comparison of median and mean for 67% WP, from opening kickoff to the 4th quarter 2-minute warning in two minute increments.\r\n\r\n\r\nShow code\r\n\r\nmean_10k_streams <- function(initial_t, initial_wp){\r\n  replicate(10000,\r\n            sim_stream(\r\n              game_seconds_played = initial_t,\r\n              wp = initial_wp\r\n            )\r\n           ) %>%\r\n    mean() %>%\r\n    return()\r\n}\r\n\r\n\r\nmedian_10k_streams <- function(initial_t, initial_wp){\r\n  replicate(10000,\r\n            sim_stream(\r\n              game_seconds_played = initial_t,\r\n              wp = initial_wp\r\n              )\r\n            ) %>%\r\n    median() %>%\r\n    return()\r\n}\r\n\r\n\r\ninitial_t <- seq(0, 3600 - (24*5), by = 24*5)\r\ninitial_wp <- 2/3\r\nwp_final <- rep(NA_real_, length(initial_t))\r\n\r\ndf_mean_final_wp <-\r\n  data.frame(initial_t, initial_wp, wp_final) %>%\r\n  mutate(metric = \"Mean\",\r\n         wp_final = map2_dbl(initial_t, initial_wp,\r\n                             mean_10k_streams))\r\n\r\ndf_median_final_wp <-\r\n  data.frame(initial_t, initial_wp, wp_final) %>%\r\n  mutate(metric = \"Median\",\r\n         wp_final = map2_dbl(initial_t, initial_wp,\r\n                             median_10k_streams))\r\n\r\ndf_metric_comparison <-\r\n  bind_rows(df_mean_final_wp,\r\n            df_median_final_wp)\r\n\r\np_time_value <-\r\n  df_metric_comparison %>%\r\n  ggplot(\r\n    aes(x = initial_t/60,\r\n        y = wp_final,\r\n        group = metric,\r\n        color = metric)\r\n  ) +\r\n  geom_vline(xintercept = c(0, 3600)/60) +\r\n  geom_vline(xintercept = c(900, 1800, 2700)/60,\r\n             linetype = \"dashed\") +\r\n  geom_hline(yintercept = c(0, 1)) +\r\n  geom_point() +\r\n  geom_path(\r\n    arrow = arrow(type = \"closed\",\r\n                  length = unit(0.1, \"inches\"))\r\n  ) +\r\n  theme_light() +\r\n  scale_y_continuous(\r\n    minor_breaks = NULL,\r\n    labels = scales::percent\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = seq(0, 60, by = 5)\r\n  ) +\r\n  labs(x = \"Starting Time-in-Game of Sims (Minutes Played)\",\r\n       y = \"Mean/Median of Final WP for 10k Sims\",\r\n       color = \"Metric:\",\r\n       title = \"Summary Statistics of Final WP for Varying Time-in-Game\",\r\n       subtitle = \"10k Sims Each Point: Various Time-in-Game, Initial WP: 67%\") +\r\n  theme(legend.position = \"bottom\")\r\n\r\nggsave(\r\n  filename = \"wp_time_value.png\",\r\n  plot = p_time_value,\r\n  width = 6,\r\n  height = 4.5,\r\n  units = \"in\"\r\n)\r\n\r\n\r\n\r\n(Visually, this plot reminds me of the concept of time decay in options trading, of which I know nearly nothing about.)\r\nImage from Merrill Edge Option Education Material linked above.An alternative approach might be to simulate trajectories between two different conditions and compute a contrast. Here’s a comparison of the Option 1 and Option 2 above.\r\n\r\n\r\nShow code\r\n\r\noption_1 <-\r\n  replicate(\r\n    10000,\r\n    sim_stream(\r\n      game_seconds_played = 0,\r\n      wp = 2/3\r\n    )\r\n  )\r\n\r\noption_2 <-\r\n  replicate(\r\n    10000,\r\n    sim_stream(\r\n      game_seconds_played = 3600 - 120,\r\n      wp = 2/3\r\n    )\r\n  )\r\n\r\ncontrast <-\r\n  sample(option_1, size = 2000, replace = T) - sample(option_2, size = 2000, replace = T)\r\n\r\ndata.frame(delta_wp = contrast) %>%\r\n  ggplot(aes(x = delta_wp)) +\r\n  geom_vline(xintercept = 0,\r\n             linetype = \"dashed\") +\r\n  geom_boxplot(fill = \"blue\",\r\n               alpha = 0.4) +\r\n  geom_jitter(aes(y = 1),\r\n              alpha = 0.15,\r\n              width = 0) +\r\n  scale_x_continuous(labels = scales::percent) +\r\n  scale_y_continuous(labels = NULL, breaks = NULL) +\r\n  theme_light() +\r\n  labs(x = \"Advantage of Option 1 Draw over Option 2 Draw\",\r\n       y = NULL,\r\n       title = \"Contrast Between Options\",\r\n       subtitle = \"Option 1: WP 67%, Opening Kickoff; Option 2: WP 67%; Final 2-minute Warning\")\r\n\r\n\r\n\r\nFix Time-in-Game, Vary Initial WP\r\nPreviously, we fixed the starting WP for our simulations and varied the time-in-game. Estimating the resulting median final WP revealed what could be a time value effect.\r\nHere, we reverse that: fix the amount of time remaining, and vary the starting WP. Estimating the resulting median final WP reveals what looks like a basis for departures from risk-neutrality and the emergence of a sigmoid that tends toward linearity as time remaining goes to 0. The implications here would be really significant (e.g., 4th down decisions, Jensen’ inequality impact on EV calculations).\r\nThe plots below use the same three milestones as before: opening kickoff, start of the 4th quarter, and the final 2-minute warning.\r\n\r\n\r\nShow code\r\n\r\ninitial_wp <- seq(0.0, 1.0, by = 0.05)\r\ninitial_t <- rep(0, length(initial_wp))\r\nwp_final <- rep(NA_real_, length(initial_wp))\r\n\r\ndf_median_final_wp <-\r\n  data.frame(initial_t, initial_wp, wp_final) %>%\r\n  mutate(wp_final = map2_dbl(initial_t, initial_wp,\r\n                             median_10k_streams))\r\n\r\np_final_wp_kickoff <-\r\n  df_median_final_wp %>%\r\n  ggplot(\r\n    aes(x = initial_wp,\r\n        y = wp_final)\r\n  ) +\r\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\r\n  geom_vline(xintercept = c(0, 1)) +\r\n  geom_hline(yintercept = c(0, 1)) +\r\n  geom_path(color = \"blue\") +\r\n  geom_point() +\r\n  scale_x_continuous(labels = scales::percent,\r\n                     minor_breaks = NULL) +\r\n  scale_y_continuous(labels = scales::percent,\r\n                     minor_breaks = NULL) +\r\n  theme_light() +\r\n  labs(\r\n    x = \"WP (Opening Kickoff)\",\r\n    y = \"Median Final WP\",\r\n    title = \"Median Final WP: Opening Kickoff\",\r\n    subtitle = \"10k Sims Each Point\"\r\n  )\r\n\r\nggsave(\r\n  filename = \"final_wp_kickoff.png\",\r\n  plot = p_final_wp_kickoff,\r\n  height = 5.25,\r\n  width = 5,\r\n  units = \"in\"\r\n)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ninitial_wp <- seq(0.0, 1.0, by = 0.05)\r\ninitial_t <- rep(2700, length(initial_wp))\r\nwp_final <- rep(NA_real_, length(initial_wp))\r\n\r\ndf_median_final_wp <-\r\n  data.frame(initial_t, initial_wp, wp_final) %>%\r\n  mutate(wp_final = map2_dbl(initial_t, initial_wp,\r\n                             median_10k_streams))\r\n\r\np_final_wp_start_4th <-\r\n  df_median_final_wp %>%\r\n  ggplot(\r\n    aes(x = initial_wp,\r\n        y = wp_final)\r\n  ) +\r\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\r\n  geom_vline(xintercept = c(0, 1)) +\r\n  geom_hline(yintercept = c(0, 1)) +\r\n  geom_path(color = \"blue\") +\r\n  geom_point() +\r\n  scale_x_continuous(labels = scales::percent,\r\n                     minor_breaks = NULL) +\r\n  scale_y_continuous(labels = scales::percent,\r\n                     minor_breaks = NULL) +\r\n  theme_light() +\r\n  labs(\r\n    x = \"WP (Start of 4th Quarter)\",\r\n    y = \"Median Final WP\",\r\n    title = \"Median Final WP: Start 4th Quarter\",\r\n    subtitle = \"10k Sims Each Point\"\r\n  )\r\n\r\nggsave(\r\n  filename = \"final_wp_start_4th.png\",\r\n  plot = p_final_wp_start_4th,\r\n  height = 5.25,\r\n  width = 5,\r\n  units = \"in\"\r\n)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ninitial_wp <- seq(0.0, 1.0, by = 0.05)\r\ninitial_t <- rep(3480, length(initial_wp))\r\nwp_final <- rep(NA_real_, length(initial_wp))\r\n\r\ndf_median_final_wp <-\r\n  data.frame(initial_t, initial_wp, wp_final) %>%\r\n  mutate(wp_final = map2_dbl(initial_t, initial_wp,\r\n                             median_10k_streams))\r\n\r\np_final_wp_final_2_minutes <-\r\n  df_median_final_wp %>%\r\n  ggplot(\r\n    aes(x = initial_wp,\r\n        y = wp_final)\r\n  ) +\r\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\r\n  geom_vline(xintercept = c(0, 1)) +\r\n  geom_hline(yintercept = c(0, 1)) +\r\n  geom_path(color = \"blue\") +\r\n  geom_point() +\r\n  scale_x_continuous(labels = scales::percent,\r\n                     minor_breaks = NULL) +\r\n  scale_y_continuous(labels = scales::percent,\r\n                     minor_breaks = NULL) +\r\n  theme_light() +\r\n  labs(\r\n    x = \"WP (4th Quarter, 2-minute Warning)\",\r\n    y = \"Median Final WP\",\r\n    title = \"Median Final WP: Final 2 Minutes\",\r\n    subtitle = \"10k Sims Each Point\"\r\n  )\r\n\r\nggsave(\r\n  filename = \"final_wp_final_2_minutes.png\",\r\n  plot = p_final_wp_final_2_minutes,\r\n  height = 5.25,\r\n  width = 5,\r\n  units = \"in\"\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-12-28-partiiiwinprobabilitystochasticprocess/partiiiwinprobabilitystochasticprocess_files/figure-html5/plot_single_trajectory-1.png",
    "last_modified": "2024-01-02T01:01:36-05:00",
    "input_file": "partiiiwinprobabilitystochasticprocess.knit.md"
  },
  {
    "path": "posts/2023-12-26-partiiwinprobabilitystochasticprocess/",
    "title": "Part II: Win Probability as a Stochastic Process",
    "description": "Toward a Dynamic: Some statistical properties of Win Probability Added.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-12-27",
    "categories": [],
    "contents": "\r\nBackground\r\nIn Part I of this series, I provided the most basic of introductions to options pricing, describing a simplified Bachelier model for a binary call option. The real point was to introduce the quantitative finance idea of valuing something on the basis of a stochastic process: simulate many possible paths for an assumed dynamic, and determine the value of an instrument based on the distribution of the simulated outcomes.\r\nHere, I examine the dynamics of NFL win probability forecasts: specifically, I look at the nflfastR win probability model that incorporates the pre-game Vegas line (i.e., vegas_wpa, vegas_home_wpa, vegas_wp, vegas_home_wp). For a technical description of the model (or something close to it), see it’s creator’s (Ben Baldwin) article here.\r\nMy interest in the topic was sparked by a Nassim Taleb video on YouTube, summarized by the opening quote in the introduction to a Taleb paper on election forecasting dynamics.\r\n\r\n“A standard result in quantitative finance is that when the\r\nvolatility of the underlying security increases, arbitrage\r\npressures push the corresponding binary option to trade closer\r\nto 50%, and become less variable over the remaining time\r\nto expiration. Counterintuitively, the higher the uncertainty of\r\nthe underlying security, the lower the volatility of the binary\r\noption.”\r\nNassim Taleb\r\n\r\nPreviously, I explored some tests for excess movement in win probability streams for NFL (nflfastR model) and MLB (MLB.com data) games based on a paper by Ned Augenblick and Matthew Rabin.\r\nIntuition\r\n\r\n“Volatility involves uncertain changes of state… Time and\r\nchance happen to us all. A stock price that moves unpredictably\r\nup and down is volatile. Its volatility reflects the unstable\r\nequilibrium between two simultaneous but conflicting occurrences:\r\ngood news and bad news, the desire to buy and the desire to sell,\r\nin permanent conflict. As in politics, there is no volatility\r\nwithout conflict.”\r\nEmanual Derman\r\n\r\nConsider each in-game win probability (WP) estimate as a price. The threat of arbitrage forces the WP stream to be a martingale: “knowledge of the past will be of no use in predicting the future” and “the direction of anticipated future swings… should be already baked into the current prediction” (from Andrew Gelman et al, 2020). Therefore, Win Probability Added (WPA) should be 0 in expectation.\r\nThe Taleb quote describes some effects of uncertainty.\r\nAs uncertainty increases, WP estimates get pulled toward 0.5 (a coin flip, maximum ignorance).\r\nAs uncertainty increases, the volatility of a WP stream should decrease.\r\nWhat are the systematic sources of uncertainty in an NFL WP stream?\r\nTime remaining. The greater the amount of time remaining, the greater the uncertainty in the final outcome. Therefore, for a given pre-snap WP, we should expect lower volatility (i.e., smaller variance in WPA) early in games and increasing volatility as the game draws to a close.\r\nPre-snap Win Probability. The pre-snap WP should already incorporate the outstanding uncertainty in the contest. Therefore, for a given amount of time remaining, we should expect lower volatility (i.e., smaller variance in WPA) for plays with pre-snap WP closer to either 1 or 0 than plays with pre-snap WP closer to 0.5.\r\nAcquire Data\r\nI’ll acquire the data from nflfastR and then prepare it for plotting.\r\nFor convenience, I’ll look at win probabilities for home teams only, from 2017 until the time time of publishing.\r\nAs described above, I want to be able to look across time for a given WP. Therefore, I do some grouping by time remaining and pre-snap WP to get a more reasonable number of points in each “bucket.”\r\nThen, I find some empirical WPA quantiles in each pre-snap WP/Time-in-Game bucket.\r\n\r\n\r\nShow code\r\n\r\npbp_df <- load_pbp(2017:2023)\r\n\r\ndf <- pbp_df %>%\r\n  filter(is.na(vegas_home_wpa) == F) %>%\r\n  mutate(vegas_home_wp = cut_width(vegas_wp, width = 0.05, center = 0.5),\r\n         game_seconds_played = 3600 - game_seconds_remaining,\r\n         game_seconds_played = cut_width(game_seconds_played,\r\n                                         center = 1800,\r\n                                         width = 60)) %>%\r\n  filter(game_half != \"Overtime\") %>%\r\n  filter(game_seconds_played != 0)\r\n\r\n# Via https://stackoverflow.com/questions/22312207/how-to-assign-cut-range-midpoints-in-r\r\n# The cut_width function provides intervals\r\n# To plot, I want the midpoint.  This function\r\n# does so by removing the interval notation\r\n# (e.g., open/close brackets or parentheses,\r\n# comma between values) and taking the mean\r\n# of the interval boundaries.\r\nget_midpoint <- function(cut_label) {\r\n  mean(as.numeric(unlist(strsplit(gsub(\"\\\\(|\\\\)|\\\\[|\\\\]\", \"\",\r\n                                       as.character(cut_label)), \",\"))))\r\n}\r\n\r\ndf$vegas_home_wp <-\r\n  sapply(df$vegas_home_wp, get_midpoint)\r\n\r\ndf$game_seconds_played <-\r\n  sapply(df$game_seconds_played, get_midpoint)\r\n\r\nempirical_df <-\r\n  df %>%\r\n  group_by(game_seconds_played, vegas_home_wp) %>%\r\n  summarize(lower_bound_1 = quantile(vegas_home_wpa, pnorm(-1)),\r\n            lower_bound_2 = quantile(vegas_home_wpa, pnorm(-2)),\r\n            median = quantile(vegas_home_wpa, 0.5),\r\n            upper_bound_1 = quantile(vegas_home_wpa, pnorm(1)),\r\n            upper_bound_2 = quantile(vegas_home_wpa, pnorm(2)),\r\n            n = n())\r\n\r\n\r\nPlot Data\r\n\r\n\r\nShow code\r\n\r\np_wpa <-\r\n  empirical_df %>%\r\n  filter(vegas_home_wp %in% c(0, 1, 0.9975) == F) %>%\r\n  ggplot(aes(x = game_seconds_played/60,\r\n             y = vegas_home_wp,\r\n             group = vegas_home_wp)) +\r\n  geom_ribbon(aes(ymin = lower_bound_2,\r\n                  ymax = upper_bound_2)) +\r\n  geom_ribbon(aes(ymin = lower_bound_1,\r\n                  ymax = upper_bound_1),\r\n              fill = \"dark gray\") +\r\n  geom_path(aes(y = median),\r\n            color = \"blue\") +\r\n  theme_light() +\r\n  scale_x_continuous(breaks = c(0, 15, 30, 45, 60),\r\n                     minor_breaks = NULL) +\r\n  scale_y_continuous(breaks = seq(-0.5, 0.5, 0.25),\r\n                     minor_breaks = NULL) +\r\n  facet_wrap(~ vegas_home_wp) +\r\n  labs(x = \"Minutes Played\",\r\n       y = \"WPA (1 and 2 SD Intervals)\",\r\n       title = \"Distribution of WPA by Pre-snap WP\",\r\n       subtitle = \"nflfastR Vegas-informed WP Model\",\r\n       caption = \"Data: nflfastR\")\r\n\r\nggsave(\r\n  filename = \"wpa_by_pre-snap_wp_and_time.png\",\r\n  plot = p_wpa,\r\n  height = 5.5,\r\n  width = 5,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nObservations\r\nThe data matches the expected patterns pretty well.\r\nAcross all pre-snap WP and time-in-game combinations, the median WPA is near 0 (blue line).\r\nAs a game nears its conclusion (i.e., time remaining decreases), the variance of WPA increases significantly (empirical +/- 1 (normal) standard deviation band in gray, empirical +/- 2 (normal) standard deviation band in black).\r\nAs pre-snap WP approaches 0 or 1, WPA tends toward 0. For each slice of time, the variance of WPA is the highest at pre-snap WP of 0.5.\r\nA Toy WP Dynamic\r\nHere, I introduce a toy dynamic for WP based on the preceding observations. The point is to develop a simple model that matches the nflfastR WP model well enough to develop some novel conclusions via a stochastic process approach.\r\nFor simplicity, I’ll assume WPA follows a Normal distribution.\r\nThe dynamic will be a function of time-in-game (i.e., game seconds played) and current pre-snap WP.\r\nConsistent with the Martingale property discussed above, the mean WPA will be 0 for each play.\r\nWPA Stochastic Process Model\r\n\\(WPA \\sim N(\\mu = 0, SD = \\sigma_{WP, t})\\)\r\n\\(\\sigma_{WP, t} = 0.07 (0.5 - \\lvert{WP - 0.5}\\rvert) + \\frac{75 ((0.5 - \\lvert{WP - 0.5}\\rvert))}{3875 - t}\\)\r\nHere, t is game seconds played.\r\nThe “0.5 - |WP - 0.5|” term is just the distance from certainty (i.e, WP = 0 or WP = 1).\r\n\r\n\r\nShow code\r\n\r\nwp_vector = seq(0.0, 1.0, by = 0.05)\r\ndist_from_certain = 0.5 - abs(wp_vector - 0.5)\r\n\r\nplot_df <-\r\n  data.frame(wp_vector, dist_from_certain)\r\n\r\nplot_df %>%\r\n  ggplot(aes(\r\n    x = wp_vector,\r\n    y = dist_from_certain\r\n  )) +\r\n  geom_hline(yintercept = 0) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_path(color = \"dark gray\") +\r\n  geom_point() +\r\n  theme_light()\r\n\r\n\r\nShow code\r\n\r\nrm(wp_vector, dist_from_certain, plot_df)\r\n\r\n\r\nThe fractional term captures the growth in Var[WPA] as the game progresses. I picked a hyperbolic form to capture the rapid increase in Var[WPA] as games reach their conclusion and the digital resolution of (non-overtime) WP streams.\r\nThe three constants were picked by trial-and-error, eyeballing the fit with the plot of the nflfastR model data above.\r\nWPA Stochastic Process Model Fit to nflfastR Data\r\n\r\n\r\nShow code\r\n\r\nstochastic_model <-\r\n  expand_grid(game_seconds_played = seq(0, 3600, by = 30),\r\n              wp = seq(0.05, 0.95, by = 0.05)) %>%\r\n  mutate(dist_from_certainty = 0.5 - abs(wp - 0.5)) %>%\r\n  mutate(wpa_sd_model = (0.07*dist_from_certainty) + ((75*dist_from_certainty)/(3875 - game_seconds_played))) %>%\r\n  mutate(vegas_home_wp = wp) %>%\r\n  mutate(lower_bound_1 = -1 * wpa_sd_model,\r\n         lower_bound_2 = -2 * wpa_sd_model,\r\n         upper_bound_1 = 1 * wpa_sd_model,\r\n         upper_bound_2 = 2 * wpa_sd_model)\r\n\r\np_stochastic_fit <-\r\n  p_wpa +\r\n  geom_path(\r\n    aes(\r\n      x = game_seconds_played/60,\r\n      y = lower_bound_1\r\n    ),\r\n    data = stochastic_model,\r\n    color = \"red\"\r\n  ) +\r\n  geom_path(\r\n    aes(\r\n      x = game_seconds_played/60,\r\n      y = lower_bound_2\r\n    ),\r\n    data = stochastic_model,\r\n    color = \"red\"\r\n  ) +\r\n  geom_path(\r\n    aes(\r\n      x = game_seconds_played/60,\r\n      y = upper_bound_1\r\n    ),\r\n    data = stochastic_model,\r\n    color = \"red\"\r\n  ) +\r\n  geom_path(\r\n    aes(\r\n      x = game_seconds_played/60,\r\n      y = upper_bound_2\r\n    ),\r\n    data = stochastic_model,\r\n    color = \"red\"\r\n  )\r\n\r\nggsave(\r\n  filename = \"stochastic_fit.png\",\r\n  plot = p_stochastic_fit,\r\n  height = 8,\r\n  width = 5,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nThat’s clunky code, and I’m not really sure what is going on with the blank panels.\r\nOverall, the quantiles fit reasonably well. Again, the goal is to develop a simple model that matches the nflfastR WP model well enough to develop some novel conclusions via a stochastic process approach.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-12-26-partiiwinprobabilitystochasticprocess/wpa_by_pre-snap_wp_and_time.png",
    "last_modified": "2023-12-27T17:00:25-05:00",
    "input_file": "partiiwinprobabilitystochasticprocess.knit.md"
  },
  {
    "path": "posts/2023-12-26-post-week-16-tails-update/",
    "title": "Post Week 16 TAILS Update",
    "description": "Time Average Inpredictable Line Smidge",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-12-26",
    "categories": [],
    "contents": "\r\nLoad Libraries\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\nlibrary(nfltools)\r\nlibrary(nflplotR)\r\n\r\n\r\nGet Time Average Lead Data\r\n\r\n\r\nShow code\r\n\r\nraw_df <-\r\n  nfltools::nfl_mvt_season(2023)\r\n\r\n\r\nCalculate TAILS Adjustment\r\n\r\n\r\nShow code\r\n\r\ntails <-\r\n  raw_df %>%\r\n  filter(!(is.na(opponent))) %>%\r\n  mutate(\r\n    adj_time_avg_lead = time_avg_lead + (0.5 * team_spread)\r\n  ) %>%\r\n  group_by(team) %>%\r\n  slice_tail(n = 5) %>%\r\n  summarize(\r\n    tails = mean(adj_time_avg_lead),\r\n    .groups = \"drop\"\r\n  ) %>%\r\n  mutate(\r\n    tails = 0.2 * tails\r\n  )\r\n\r\ninpred <-\r\n  # This is a function I wrote as my first bit of\r\n  # webscraping code. It retrieves info from the\r\n  # inpredictable power ratings at the time it is run.\r\n  get_current_inpredictable()\r\n\r\ninpred <-\r\n  left_join(\r\n    inpred,\r\n    tails\r\n  ) %>%\r\n  mutate(\r\n    adj_gpf = gpf + tails\r\n  )\r\n\r\n\r\nPlot\r\n\r\n\r\nShow code\r\n\r\np_adj_gpf <-\r\n  inpred %>%\r\n  ggplot(\r\n    aes(\r\n      x = gpf,\r\n      y = reorder(factor(team),\r\n                  adj_gpf)\r\n    )\r\n  ) +\r\n  geom_vline(\r\n    xintercept = 0,\r\n    linetype = \"dashed\"\r\n  ) +\r\n  geom_segment(\r\n    aes(\r\n      x = gpf,\r\n      xend = adj_gpf,\r\n      yend = team\r\n    ),\r\n    arrow = arrow(\r\n      type = \"closed\",\r\n      length = unit(\r\n        0.1, \"cm\"\r\n      )\r\n    )\r\n  ) +\r\n  geom_point() +\r\n  geom_nfl_logos(\r\n    aes(\r\n      team_abbr = team,\r\n      x = adj_gpf),\r\n    width = 0.04,\r\n    alpha = 0.4\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = seq(-18, 18, by = 3),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_discrete(\r\n    labels = NULL,\r\n    breaks = NULL\r\n  ) +\r\n  labs(\r\n    x = \"TAILS Adjusted Generic Points Favored\",\r\n    y = NULL,\r\n    title = \"Post Week 16 NFL Power Rankings\",\r\n    subtitle = \"As of 12/26/2023.\",\r\n    caption = \"Baseline: inpredictable.com Betting Market Ratings\\nMy silly adjustment calculated using nflfastR data.\"\r\n  ) +\r\n  theme_light()\r\n\r\nggsave(\r\n  \"post_week_16_tails_adj_gpf.png\",\r\n  plot = p_adj_gpf,\r\n  width = 6,\r\n  height = 4.5,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nTable\r\n\r\n\r\nShow code\r\n\r\ninpred %>%\r\n  select(\r\n    team, gpf, tails, adj_gpf\r\n  ) %>%\r\n  arrange(\r\n    desc(adj_gpf)\r\n  ) %>%\r\n  mutate(\r\n    tails = round(tails, 2),\r\n    adj_gpf = round(adj_gpf, 2),\r\n    rank = row_number()\r\n  ) %>%\r\n  knitr::kable()\r\n\r\nteam\r\ngpf\r\ntails\r\nadj_gpf\r\nrank\r\nSF\r\n9.2\r\n0.07\r\n9.27\r\n1\r\nBAL\r\n6.4\r\n0.78\r\n7.18\r\n2\r\nBUF\r\n5.8\r\n1.05\r\n6.85\r\n3\r\nMIA\r\n5.2\r\n1.20\r\n6.40\r\n4\r\nDAL\r\n6.2\r\n-0.28\r\n5.92\r\n5\r\nKC\r\n5.0\r\n-1.16\r\n3.84\r\n6\r\nPHI\r\n3.5\r\n-0.55\r\n2.95\r\n7\r\nDET\r\n2.1\r\n-0.10\r\n2.00\r\n8\r\nLA\r\n0.3\r\n1.18\r\n1.48\r\n9\r\nCLE\r\n0.0\r\n0.21\r\n0.21\r\n10\r\nTB\r\n-1.0\r\n0.95\r\n-0.05\r\n11\r\nSEA\r\n-0.3\r\n-0.27\r\n-0.57\r\n12\r\nCHI\r\n-2.0\r\n1.31\r\n-0.69\r\n13\r\nGB\r\n-1.7\r\n0.74\r\n-0.96\r\n14\r\nJAX\r\n-0.4\r\n-1.06\r\n-1.46\r\n15\r\nLV\r\n-3.6\r\n2.14\r\n-1.46\r\n16\r\nDEN\r\n-1.5\r\n-0.12\r\n-1.62\r\n17\r\nCIN\r\n-1.3\r\n-0.48\r\n-1.78\r\n18\r\nMIN\r\n-2.2\r\n0.17\r\n-2.03\r\n19\r\nATL\r\n-2.4\r\n0.31\r\n-2.09\r\n20\r\nPIT\r\n-1.9\r\n-0.26\r\n-2.16\r\n21\r\nNO\r\n-1.7\r\n-0.61\r\n-2.31\r\n22\r\nIND\r\n-2.6\r\n-0.29\r\n-2.89\r\n23\r\nHOU\r\n-2.6\r\n-0.90\r\n-3.50\r\n24\r\nNE\r\n-4.7\r\n0.35\r\n-4.35\r\n25\r\nTEN\r\n-5.5\r\n0.83\r\n-4.67\r\n26\r\nNYJ\r\n-5.3\r\n-0.09\r\n-5.39\r\n27\r\nARI\r\n-5.1\r\n-0.47\r\n-5.57\r\n28\r\nCAR\r\n-5.9\r\n-0.70\r\n-6.60\r\n29\r\nNYG\r\n-7.7\r\n0.52\r\n-7.18\r\n30\r\nWAS\r\n-5.4\r\n-1.80\r\n-7.20\r\n31\r\nLAC\r\n-6.0\r\n-1.43\r\n-7.43\r\n32\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-12-26-post-week-16-tails-update/post_week_16_tails_adj_gpf.png",
    "last_modified": "2023-12-26T22:59:04-05:00",
    "input_file": "post-week-16-tails-update.knit.md"
  },
  {
    "path": "posts/2023-12-20-post-week-15-tails-update/",
    "title": "Post Week 15 TAILS Update",
    "description": "Time Average Inpredictable Line Smidge",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-12-20",
    "categories": [],
    "contents": "\r\nLoad Libraries\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\nlibrary(nfltools)\r\nlibrary(nflplotR)\r\n\r\n\r\nGet Time Average Lead Data\r\n\r\n\r\nShow code\r\n\r\nraw_df <-\r\n  nfltools::nfl_mvt_season(2023)\r\n\r\n\r\nCalculate TAILS Adjustment\r\n\r\n\r\nShow code\r\n\r\ntails <-\r\n  raw_df %>%\r\n  filter(!(is.na(opponent))) %>%\r\n  mutate(\r\n    adj_time_avg_lead = time_avg_lead + (0.5 * team_spread)\r\n  ) %>%\r\n  group_by(team) %>%\r\n  slice_tail(n = 5) %>%\r\n  summarize(\r\n    tails = mean(adj_time_avg_lead),\r\n    .groups = \"drop\"\r\n  ) %>%\r\n  mutate(\r\n    tails = 0.2 * tails\r\n  )\r\n\r\ninpred <-\r\n  # This is a function I wrote as my first bit of\r\n  # webscraping code. It retrieves info from the\r\n  # inpredictable power ratings at the time it is run.\r\n  get_current_inpredictable()\r\n\r\ninpred <-\r\n  left_join(\r\n    inpred,\r\n    tails\r\n  ) %>%\r\n  mutate(\r\n    adj_gpf = gpf + tails\r\n  )\r\n\r\n\r\nPlot\r\n\r\n\r\nShow code\r\n\r\np_adj_gpf <-\r\n  inpred %>%\r\n  ggplot(\r\n    aes(\r\n      x = gpf,\r\n      y = reorder(factor(team),\r\n                  adj_gpf)\r\n    )\r\n  ) +\r\n  geom_vline(\r\n    xintercept = 0,\r\n    linetype = \"dashed\"\r\n  ) +\r\n  geom_segment(\r\n    aes(\r\n      x = gpf,\r\n      xend = adj_gpf,\r\n      yend = team\r\n    ),\r\n    arrow = arrow(\r\n      type = \"closed\",\r\n      length = unit(\r\n        0.1, \"cm\"\r\n      )\r\n    )\r\n  ) +\r\n  geom_point() +\r\n  geom_nfl_logos(\r\n    aes(\r\n      team_abbr = team,\r\n      x = adj_gpf),\r\n    width = 0.04,\r\n    alpha = 0.4\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = seq(-18, 18, by = 3),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_discrete(\r\n    labels = NULL,\r\n    breaks = NULL\r\n  ) +\r\n  labs(\r\n    x = \"TAILS Adjusted Generic Points Favored\",\r\n    y = NULL,\r\n    title = \"Post Week 15 NFL Power Rankings\",\r\n    subtitle = \"As of 12/20/2023.\",\r\n    caption = \"Baseline: inpredictable.com Betting Market Ratings\\nMy silly adjustment calculated using nflfastR data.\"\r\n  ) +\r\n  theme_light()\r\n\r\nggsave(\r\n  \"post_week_15_tails_adj_gpf.png\",\r\n  plot = p_adj_gpf,\r\n  width = 6,\r\n  height = 4.5,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nTable\r\n\r\n\r\nShow code\r\n\r\ninpred %>%\r\n  select(\r\n    team, gpf, tails, adj_gpf\r\n  ) %>%\r\n  arrange(\r\n    desc(adj_gpf)\r\n  ) %>%\r\n  mutate(\r\n    tails = round(tails, 2),\r\n    adj_gpf = round(adj_gpf, 2),\r\n    rank = row_number()\r\n  ) %>%\r\n  knitr::kable()\r\n\r\nteam\r\ngpf\r\ntails\r\nadj_gpf\r\nrank\r\nSF\r\n9.3\r\n0.62\r\n9.92\r\n1\r\nBUF\r\n6.1\r\n1.05\r\n7.15\r\n2\r\nBAL\r\n6.1\r\n0.60\r\n6.70\r\n3\r\nMIA\r\n5.2\r\n0.94\r\n6.14\r\n4\r\nDAL\r\n5.8\r\n0.00\r\n5.80\r\n5\r\nKC\r\n5.4\r\n-0.62\r\n4.78\r\n6\r\nPHI\r\n3.3\r\n-0.67\r\n2.63\r\n7\r\nDET\r\n2.7\r\n-0.45\r\n2.25\r\n8\r\nLA\r\n0.5\r\n0.55\r\n1.05\r\n9\r\nCIN\r\n-0.3\r\n0.00\r\n-0.30\r\n10\r\nJAX\r\n-0.4\r\n-0.06\r\n-0.46\r\n11\r\nSEA\r\n-1.0\r\n0.19\r\n-0.81\r\n12\r\nGB\r\n-1.4\r\n0.55\r\n-0.85\r\n13\r\nCLE\r\n-0.9\r\n-0.18\r\n-1.08\r\n14\r\nDEN\r\n-1.0\r\n-0.09\r\n-1.09\r\n15\r\nMIN\r\n-1.8\r\n0.66\r\n-1.14\r\n16\r\nTB\r\n-1.4\r\n0.24\r\n-1.16\r\n17\r\nLV\r\n-3.2\r\n1.63\r\n-1.57\r\n18\r\nCHI\r\n-2.7\r\n0.90\r\n-1.80\r\n19\r\nIND\r\n-2.1\r\n-0.02\r\n-2.12\r\n20\r\nNO\r\n-1.5\r\n-0.72\r\n-2.22\r\n21\r\nATL\r\n-2.8\r\n0.07\r\n-2.73\r\n22\r\nPIT\r\n-2.8\r\n-1.07\r\n-3.87\r\n23\r\nNE\r\n-4.9\r\n0.09\r\n-4.81\r\n24\r\nHOU\r\n-4.7\r\n-0.39\r\n-5.09\r\n25\r\nTEN\r\n-5.5\r\n0.31\r\n-5.19\r\n26\r\nNYJ\r\n-4.4\r\n-0.95\r\n-5.35\r\n27\r\nARI\r\n-5.4\r\n-0.10\r\n-5.50\r\n28\r\nWAS\r\n-5.5\r\n-1.20\r\n-6.70\r\n29\r\nNYG\r\n-7.1\r\n0.16\r\n-6.94\r\n30\r\nCAR\r\n-6.9\r\n-0.70\r\n-7.60\r\n31\r\nLAC\r\n-6.3\r\n-1.72\r\n-8.02\r\n32\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-12-20-post-week-15-tails-update/post_week_15_tails_adj_gpf.png",
    "last_modified": "2023-12-20T23:30:59-05:00",
    "input_file": "post-week-15-tails-update.knit.md"
  },
  {
    "path": "posts/2023-12-12-post-week-14-tails-update/",
    "title": "Post Week 14 TAILS Update",
    "description": "Time Average Inpredictable Line Smidge",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-12-12",
    "categories": [],
    "contents": "\r\nLoad Libraries\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\nlibrary(nfltools)\r\nlibrary(nflplotR)\r\n\r\n\r\nGet Time Average Lead Data\r\n\r\n\r\nShow code\r\n\r\nraw_df <-\r\n  nfltools::nfl_mvt_season(2023)\r\n\r\n\r\nCalculate TAILS Adjustment\r\n\r\n\r\nShow code\r\n\r\ntails <-\r\n  raw_df %>%\r\n  filter(!(is.na(opponent))) %>%\r\n  mutate(\r\n    adj_time_avg_lead = time_avg_lead + (0.5 * team_spread)\r\n  ) %>%\r\n  group_by(team) %>%\r\n  slice_tail(n = 5) %>%\r\n  summarize(\r\n    tails = mean(adj_time_avg_lead),\r\n    .groups = \"drop\"\r\n  ) %>%\r\n  mutate(\r\n    tails = 0.2 * tails\r\n  )\r\n\r\ninpred <-\r\n  # This is a function I wrote as my first bit of\r\n  # webscraping code. It retrieves info from the\r\n  # inpredictable power ratings at the time it is run.\r\n  get_current_inpredictable()\r\n\r\ninpred <-\r\n  left_join(\r\n    inpred,\r\n    tails\r\n  ) %>%\r\n  mutate(\r\n    adj_gpf = gpf + tails\r\n  )\r\n\r\n\r\nPlot\r\n\r\n\r\nShow code\r\n\r\np_adj_gpf <-\r\n  inpred %>%\r\n  ggplot(\r\n    aes(\r\n      x = gpf,\r\n      y = reorder(factor(team),\r\n                  adj_gpf)\r\n    )\r\n  ) +\r\n  geom_vline(\r\n    xintercept = 0,\r\n    linetype = \"dashed\"\r\n  ) +\r\n  geom_segment(\r\n    aes(\r\n      x = gpf,\r\n      xend = adj_gpf,\r\n      yend = team\r\n    ),\r\n    arrow = arrow(\r\n      type = \"closed\",\r\n      length = unit(\r\n        0.1, \"cm\"\r\n      )\r\n    )\r\n  ) +\r\n  geom_point() +\r\n  geom_nfl_logos(\r\n    aes(\r\n      team_abbr = team,\r\n      x = adj_gpf),\r\n    width = 0.04,\r\n    alpha = 0.4\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = seq(-18, 18, by = 3),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_discrete(\r\n    labels = NULL,\r\n    breaks = NULL\r\n  ) +\r\n  labs(\r\n    x = \"TAILS Adjusted Generic Points Favored\",\r\n    y = NULL,\r\n    title = \"Post Week 14 NFL Power Rankings\",\r\n    subtitle = \"As of 12/12/2023.\",\r\n    caption = \"Baseline: inpredictable.com Betting Market Ratings\\nMy silly adjustment calculated using nflfastR data.\"\r\n  ) +\r\n  theme_light()\r\n\r\nggsave(\r\n  \"post_week_14_tails_adj_gpf.png\",\r\n  plot = p_adj_gpf,\r\n  width = 6,\r\n  height = 4.5,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nTable\r\n\r\n\r\nShow code\r\n\r\ninpred %>%\r\n  select(\r\n    team, gpf, tails, adj_gpf\r\n  ) %>%\r\n  arrange(\r\n    desc(adj_gpf)\r\n  ) %>%\r\n  mutate(\r\n    tails = round(tails, 2),\r\n    adj_gpf = round(adj_gpf, 2),\r\n    rank = row_number()\r\n  ) %>%\r\n  knitr::kable()\r\n\r\nteam\r\ngpf\r\ntails\r\nadj_gpf\r\nrank\r\nSF\r\n8.9\r\n1.08\r\n9.98\r\n1\r\nDAL\r\n5.9\r\n0.97\r\n6.87\r\n2\r\nBAL\r\n5.5\r\n0.88\r\n6.38\r\n3\r\nBUF\r\n5.7\r\n0.19\r\n5.89\r\n4\r\nMIA\r\n5.5\r\n-0.02\r\n5.48\r\n5\r\nKC\r\n5.6\r\n-0.32\r\n5.28\r\n6\r\nPHI\r\n3.9\r\n-0.74\r\n3.16\r\n7\r\nDET\r\n2.0\r\n-0.68\r\n1.32\r\n8\r\nLA\r\n0.4\r\n0.15\r\n0.55\r\n9\r\nJAX\r\n1.0\r\n-0.46\r\n0.54\r\n10\r\nGB\r\n-0.8\r\n0.76\r\n-0.04\r\n11\r\nDEN\r\n-0.9\r\n0.58\r\n-0.32\r\n12\r\nSEA\r\n-0.9\r\n0.16\r\n-0.74\r\n13\r\nCIN\r\n-1.5\r\n0.06\r\n-1.44\r\n14\r\nIND\r\n-1.8\r\n0.19\r\n-1.61\r\n15\r\nATL\r\n-2.0\r\n0.06\r\n-1.94\r\n16\r\nCLE\r\n-2.0\r\n-0.23\r\n-2.23\r\n17\r\nMIN\r\n-2.8\r\n0.41\r\n-2.39\r\n18\r\nNO\r\n-1.4\r\n-1.03\r\n-2.43\r\n19\r\nHOU\r\n-2.5\r\n0.02\r\n-2.48\r\n20\r\nPIT\r\n-1.8\r\n-0.89\r\n-2.69\r\n21\r\nTB\r\n-2.8\r\n0.10\r\n-2.70\r\n22\r\nLV\r\n-4.0\r\n1.03\r\n-2.97\r\n23\r\nCHI\r\n-4.1\r\n0.86\r\n-3.24\r\n24\r\nTEN\r\n-3.3\r\n-0.04\r\n-3.34\r\n25\r\nNYJ\r\n-3.3\r\n-0.35\r\n-3.65\r\n26\r\nNE\r\n-4.5\r\n0.09\r\n-4.41\r\n27\r\nWAS\r\n-4.7\r\n-0.87\r\n-5.57\r\n28\r\nLAC\r\n-5.3\r\n-0.67\r\n-5.97\r\n29\r\nARI\r\n-6.0\r\n-0.20\r\n-6.20\r\n30\r\nNYG\r\n-6.2\r\n-0.34\r\n-6.54\r\n31\r\nCAR\r\n-6.6\r\n-0.60\r\n-7.20\r\n32\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-12-12-post-week-14-tails-update/post_week_14_tails_adj_gpf.png",
    "last_modified": "2023-12-12T23:27:15-05:00",
    "input_file": "post-week-14-tails-update.knit.md"
  },
  {
    "path": "posts/2023-12-05-post-week-13-tails-update/",
    "title": "Post Week 13 TAILS Update",
    "description": "Time Average Inpredictable Line Smidge",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-12-05",
    "categories": [],
    "contents": "\r\nLoad Libraries\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\nlibrary(nfltools)\r\nlibrary(nflplotR)\r\n\r\n\r\nGet Time Average Lead Data\r\n\r\n\r\nShow code\r\n\r\nraw_df <-\r\n  nfltools::nfl_mvt_season(2023)\r\n\r\n\r\nCalculate TAILS Adjustment\r\n\r\n\r\nShow code\r\n\r\ntails <-\r\n  raw_df %>%\r\n  filter(!(is.na(opponent))) %>%\r\n  mutate(\r\n    adj_time_avg_lead = time_avg_lead + (0.5 * team_spread)\r\n  ) %>%\r\n  group_by(team) %>%\r\n  slice_tail(n = 5) %>%\r\n  summarize(\r\n    tails = mean(adj_time_avg_lead),\r\n    .groups = \"drop\"\r\n  ) %>%\r\n  mutate(\r\n    tails = 0.2 * tails\r\n  )\r\n\r\ninpred <-\r\n  # This is a function I wrote as my first bit of\r\n  # webscraping code. It retrieves info from the\r\n  # inpredictable power ratings at the time it is run.\r\n  get_current_inpredictable()\r\n\r\ninpred <-\r\n  left_join(\r\n    inpred,\r\n    tails\r\n  ) %>%\r\n  mutate(\r\n    adj_gpf = gpf + tails\r\n  )\r\n\r\n\r\nPlot\r\n\r\n\r\nShow code\r\n\r\np_adj_gpf <-\r\n  inpred %>%\r\n  ggplot(\r\n    aes(\r\n      x = gpf,\r\n      y = reorder(factor(team),\r\n                  adj_gpf)\r\n    )\r\n  ) +\r\n  geom_vline(\r\n    xintercept = 0,\r\n    linetype = \"dashed\"\r\n  ) +\r\n  geom_segment(\r\n    aes(\r\n      x = gpf,\r\n      xend = adj_gpf,\r\n      yend = team\r\n    ),\r\n    arrow = arrow(\r\n      type = \"closed\",\r\n      length = unit(\r\n        0.1, \"cm\"\r\n      )\r\n    )\r\n  ) +\r\n  geom_point() +\r\n  geom_nfl_logos(\r\n    aes(\r\n      team_abbr = team,\r\n      x = adj_gpf),\r\n    width = 0.04,\r\n    alpha = 0.4\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = seq(-18, 18, by = 3),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_discrete(\r\n    labels = NULL,\r\n    breaks = NULL\r\n  ) +\r\n  labs(\r\n    x = \"TAILS Adjusted Generic Points Favored\",\r\n    y = NULL,\r\n    title = \"Post Week 13 NFL Power Rankings\",\r\n    subtitle = \"As of 12/5/2023.\",\r\n    caption = \"Baseline: inpredictable.com Betting Market Ratings\\nMy silly adjustment calculated using nflfastR data.\"\r\n  ) +\r\n  theme_light()\r\n\r\nggsave(\r\n  \"post_week_13_tails_adj_gpf.png\",\r\n  plot = p_adj_gpf,\r\n  width = 6,\r\n  height = 4.5,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nTable\r\n\r\n\r\nShow code\r\n\r\ninpred %>%\r\n  select(\r\n    team, gpf, tails, adj_gpf\r\n  ) %>%\r\n  arrange(\r\n    desc(adj_gpf)\r\n  ) %>%\r\n  mutate(\r\n    tails = round(tails, 2),\r\n    adj_gpf = round(adj_gpf, 2),\r\n    rank = row_number()\r\n  ) %>%\r\n  knitr::kable()\r\n\r\nteam\r\ngpf\r\ntails\r\nadj_gpf\r\nrank\r\nSF\r\n8.8\r\n0.84\r\n9.64\r\n1\r\nDAL\r\n6.1\r\n0.56\r\n6.66\r\n2\r\nMIA\r\n6.3\r\n0.27\r\n6.57\r\n3\r\nBAL\r\n4.8\r\n1.05\r\n5.85\r\n4\r\nKC\r\n6.1\r\n-0.47\r\n5.63\r\n5\r\nBUF\r\n4.9\r\n0.04\r\n4.94\r\n6\r\nPHI\r\n4.3\r\n-0.65\r\n3.65\r\n7\r\nDET\r\n2.4\r\n-0.27\r\n2.13\r\n8\r\nLAC\r\n0.8\r\n0.15\r\n0.95\r\n9\r\nGB\r\n-0.2\r\n1.08\r\n0.88\r\n10\r\nHOU\r\n0.3\r\n0.19\r\n0.49\r\n11\r\nDEN\r\n-0.6\r\n0.57\r\n-0.03\r\n12\r\nSEA\r\n-0.6\r\n-0.37\r\n-0.97\r\n13\r\nMIN\r\n-1.8\r\n0.83\r\n-0.97\r\n14\r\nLA\r\n-0.4\r\n-0.59\r\n-0.99\r\n15\r\nPIT\r\n-0.6\r\n-0.49\r\n-1.09\r\n16\r\nIND\r\n-1.7\r\n0.48\r\n-1.22\r\n17\r\nATL\r\n-1.7\r\n-0.07\r\n-1.77\r\n18\r\nCLE\r\n-1.9\r\n-0.31\r\n-2.21\r\n19\r\nTB\r\n-2.5\r\n0.08\r\n-2.42\r\n20\r\nCHI\r\n-3.1\r\n0.14\r\n-2.96\r\n21\r\nJAX\r\n-3.0\r\n0.03\r\n-2.97\r\n22\r\nNO\r\n-1.8\r\n-1.20\r\n-3.00\r\n23\r\nCIN\r\n-3.6\r\n0.04\r\n-3.56\r\n24\r\nLV\r\n-5.1\r\n0.88\r\n-4.22\r\n25\r\nTEN\r\n-4.5\r\n-0.18\r\n-4.68\r\n26\r\nNE\r\n-4.5\r\n-0.44\r\n-4.94\r\n27\r\nWAS\r\n-4.7\r\n-0.87\r\n-5.57\r\n28\r\nARI\r\n-6.0\r\n-0.20\r\n-6.20\r\n29\r\nCAR\r\n-5.7\r\n-0.73\r\n-6.43\r\n30\r\nNYJ\r\n-5.8\r\n-1.05\r\n-6.85\r\n31\r\nNYG\r\n-7.7\r\n-0.45\r\n-8.15\r\n32\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-12-05-post-week-13-tails-update/post_week_13_tails_adj_gpf.png",
    "last_modified": "2023-12-05T22:06:45-05:00",
    "input_file": "post-week-13-tails-update.knit.md"
  },
  {
    "path": "posts/2023-12-04-partiwinprobabilitystochasticprocess/",
    "title": "Part I: Win Probability as a Stochastic Process",
    "description": "A novice's description of the Bachelier model for option pricing.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-12-04",
    "categories": [],
    "contents": "\r\nA Novice’s Guide to the Simplest Option Pricing Model\r\nBinary European Call Option\r\nLet’s unpack that.\r\nOption - The right, but not the obligation, to buy (call) or sell (put) an underlying asset at a predetermined “strike” price.\r\nEuropean - Type of option where the option can only be exercised at its expiration date.\r\nBinary - Option that is worth either 1 (e.g., underlying asset price is greater than strike at expiration for a call) or 0.\r\nThe Bachelier Model\r\nHow do you price an option now that may or may not have value at a pre-defined expiration date? A common approach in quantitative finance is to assign a dynamic to the underlying asset price. Given this assumed dynamic, one can either use stochastic calculus or simulate several trajectories of the underlying to determine a price based on this ensemble of possible trajectories.\r\nThe Bachelier Model is the simplest such model.\r\nIgnore the opportunity cost of risk-free return on the option’s price (“premium”).\r\nAssume T discrete periods until expiration.\r\nAssume each period is independent.\r\nThe threat of arbitrage forces the underlying’s price to be a martingale: that is, in expectation, the price at the next period must be the price at the current period (otherwise, a risk-free profit could be obtained).\r\nBy assumption, we’ll fix the volatility of the asset and assume each period’s change in follows a Normal distribution.\r\nTherefore, the change in price each period can be modeled as a random normal with mean 0 and a fixed standard deviation.\r\nAnalytical Approach\r\nAssume the change in price of the underlying at each period is normally distributed with mean 0 and standard deviation .\r\nAssume there are T discrete periods until expiration.\r\nA basic result from probability is that the sum of independent normals is itself a normal, where the summation’s mean is the sum of the combined means and the summation’s variance is the sum of the combined variances.\r\nThen the price of the underlying asset at time T is the initial price plus a normally distributed variable.\r\n\\[\r\nP_t = P_0 + \\sum_{i = 1}^{T}(N(0, \\sigma))\r\n\\]\r\nEach period’s variance is \\[\\sigma ^ 2\\].\r\nThe variance for T periods is \\[T(\\sigma ^ 2)\\].\r\nThe standard deviation for T periods is \\[\\sqrt{T} * \\sigma\\].\r\nConcrete Example\r\nAssume the current price of an underlying is $100. Assume the underlying’s daily price movements follow a normal distribution with mean 0 and standard deviation 1. Price a binary call option that expires after 25 days and is either (1) worth $1 if the underlying’s price exceeds $110 or (2) expires worthless otherwise.\r\nAfter 25 days, the change in the underlying’s prices is normally distributed with standard deviation \\[\\sqrt{25}*1 = 5\\].\r\nWhat’s the probability of a $10 price increase for the underlying? That’s two standard deviations above the mean of 0. Using the usual rules of thumb, about 4% of observations are greater than 2 standard deviations away from the mean (above or below). So a $10 increase should be have about a 2% chance.\r\n\r\n\r\nShow code\r\n\r\nsd <-\r\n  sqrt(25)\r\n\r\npnorm(10, mean = 0, sd = sd, lower.tail = F) %>%\r\n  round(., digits = 3)\r\n\r\n[1] 0.023\r\n\r\nTherefore, the expected value of this binary call (under this model’s assumptions) is roughly $0.02.\r\nSimulation Approach\r\n\r\n\r\nShow code\r\n\r\nsim_trajectory <- function(price_init = 100, mean = 0, period_sd = 1, periods = 25){\r\n  \r\n  p_0 <- price_init\r\n  \r\n  p_t <- rep(NA_real_, periods)\r\n  \r\n  period_price_changes <- rnorm(n = periods, mean = mean, sd = period_sd)\r\n  \r\n  for(i in 1:periods){\r\n    if(i == 1){\r\n      p_t[i] = p_0 + period_price_changes[i]\r\n    } else {\r\n      p_t[i] = p_t[i -1] + period_price_changes[i]\r\n    }\r\n  }\r\n  \r\n  return(p_t[periods])\r\n}\r\n\r\nfinal_prices <-\r\n  replicate(10000, sim_trajectory())\r\n\r\nhist(final_prices)\r\n\r\n\r\nShow code\r\n\r\nsum(final_prices > 110)/length(final_prices)\r\n\r\n[1] 0.02\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-12-04-partiwinprobabilitystochasticprocess/partiwinprobabilitystochasticprocess_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2023-12-06T00:37:25-05:00",
    "input_file": "partiwinprobabilitystochasticprocess.knit.md"
  },
  {
    "path": "posts/2023-11-29-post-week-12-tails-update/",
    "title": "Post Week 12 TAILS Update",
    "description": "Time Average Inpredictable Line Smidge",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-11-29",
    "categories": [],
    "contents": "\r\nLoad Libraries\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\nlibrary(nfltools)\r\nlibrary(nflplotR)\r\n\r\n\r\nGet Time Average Lead Data\r\n\r\n\r\nShow code\r\n\r\nraw_df <-\r\n  nfltools::nfl_mvt_season(2023)\r\n\r\n\r\nCalculate TAILS Adjustment\r\n\r\n\r\nShow code\r\n\r\ntails <-\r\n  raw_df %>%\r\n  filter(!(is.na(opponent))) %>%\r\n  mutate(\r\n    adj_time_avg_lead = time_avg_lead + (0.5 * team_spread)\r\n  ) %>%\r\n  group_by(team) %>%\r\n  slice_tail(n = 5) %>%\r\n  summarize(\r\n    tails = mean(adj_time_avg_lead),\r\n    .groups = \"drop\"\r\n  ) %>%\r\n  mutate(\r\n    tails = 0.2 * tails\r\n  )\r\n\r\ninpred <-\r\n  # This is a function I wrote as my first bit of\r\n  # webscraping code. It retrieves info from the\r\n  # inpredictable power ratings at the time it is run.\r\n  get_current_inpredictable()\r\n\r\ninpred <-\r\n  left_join(\r\n    inpred,\r\n    tails\r\n  ) %>%\r\n  mutate(\r\n    adj_gpf = gpf + tails\r\n  )\r\n\r\n\r\nPlot\r\n\r\n\r\nShow code\r\n\r\np_adj_gpf <-\r\n  inpred %>%\r\n  ggplot(\r\n    aes(\r\n      x = gpf,\r\n      y = reorder(factor(team),\r\n                  adj_gpf)\r\n    )\r\n  ) +\r\n  geom_vline(\r\n    xintercept = 0,\r\n    linetype = \"dashed\"\r\n  ) +\r\n  geom_segment(\r\n    aes(\r\n      x = gpf,\r\n      xend = adj_gpf,\r\n      yend = team\r\n    ),\r\n    arrow = arrow(\r\n      type = \"closed\",\r\n      length = unit(\r\n        0.1, \"cm\"\r\n      )\r\n    )\r\n  ) +\r\n  geom_point() +\r\n  geom_nfl_logos(\r\n    aes(\r\n      team_abbr = team,\r\n      x = adj_gpf),\r\n    width = 0.04,\r\n    alpha = 0.4\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = seq(-18, 18, by = 3),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_discrete(\r\n    labels = NULL,\r\n    breaks = NULL\r\n  ) +\r\n  labs(\r\n    x = \"TAILS Adjusted Generic Points Favored\",\r\n    y = NULL,\r\n    title = \"Post Week 12 NFL Power Rankings\",\r\n    subtitle = \"As of 11/28/2023.\",\r\n    caption = \"Baseline: inpredictable.com Betting Market Ratings\\nMy silly adjustment calculated using nflfastR data.\"\r\n  ) +\r\n  theme_light()\r\n\r\nggsave(\r\n  \"post_week_12_tails_adj_gpf.png\",\r\n  plot = p_adj_gpf,\r\n  width = 6,\r\n  height = 4.5,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nTable\r\n\r\n\r\nShow code\r\n\r\ninpred %>%\r\n  select(\r\n    team, gpf, tails, adj_gpf\r\n  ) %>%\r\n  arrange(\r\n    desc(adj_gpf)\r\n  ) %>%\r\n  mutate(\r\n    tails = round(tails, 2),\r\n    adj_gpf = round(adj_gpf, 2),\r\n    rank = row_number()\r\n  ) %>%\r\n  knitr::kable()\r\n\r\nteam\r\ngpf\r\ntails\r\nadj_gpf\r\nrank\r\nSF\r\n8.6\r\n0.25\r\n8.85\r\n1\r\nDAL\r\n6.5\r\n1.32\r\n7.82\r\n2\r\nBAL\r\n5.8\r\n1.05\r\n6.85\r\n3\r\nKC\r\n6.2\r\n-0.06\r\n6.14\r\n4\r\nMIA\r\n5.8\r\n-0.45\r\n5.35\r\n5\r\nPHI\r\n5.2\r\n-0.27\r\n4.93\r\n6\r\nBUF\r\n4.2\r\n0.04\r\n4.24\r\n7\r\nJAX\r\n2.8\r\n0.53\r\n3.33\r\n8\r\nLAC\r\n1.5\r\n0.51\r\n2.01\r\n9\r\nDET\r\n3.3\r\n-1.55\r\n1.75\r\n10\r\nHOU\r\n0.1\r\n-0.08\r\n0.02\r\n11\r\nDEN\r\n-1.0\r\n0.98\r\n-0.02\r\n12\r\nIND\r\n-1.3\r\n0.61\r\n-0.69\r\n13\r\nLA\r\n-0.1\r\n-0.68\r\n-0.78\r\n14\r\nGB\r\n-1.2\r\n0.39\r\n-0.81\r\n15\r\nPIT\r\n-0.7\r\n-0.32\r\n-1.02\r\n16\r\nSEA\r\n-0.6\r\n-0.54\r\n-1.14\r\n17\r\nATL\r\n-1.8\r\n-0.01\r\n-1.81\r\n18\r\nCLE\r\n-1.6\r\n-0.32\r\n-1.92\r\n19\r\nMIN\r\n-2.8\r\n0.83\r\n-1.97\r\n20\r\nNO\r\n-0.9\r\n-1.17\r\n-2.07\r\n21\r\nTB\r\n-2.1\r\n-0.12\r\n-2.22\r\n22\r\nCHI\r\n-3.6\r\n0.14\r\n-3.46\r\n23\r\nCIN\r\n-3.9\r\n0.25\r\n-3.65\r\n24\r\nTEN\r\n-3.7\r\n-0.02\r\n-3.72\r\n25\r\nLV\r\n-5.0\r\n0.88\r\n-4.12\r\n26\r\nNE\r\n-5.0\r\n0.01\r\n-4.99\r\n27\r\nWAS\r\n-5.1\r\n-0.03\r\n-5.13\r\n28\r\nARI\r\n-5.7\r\n-0.56\r\n-6.26\r\n29\r\nCAR\r\n-6.0\r\n-0.61\r\n-6.61\r\n30\r\nNYJ\r\n-5.7\r\n-1.06\r\n-6.76\r\n31\r\nNYG\r\n-8.7\r\n-0.45\r\n-9.15\r\n32\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-29-post-week-12-tails-update/post_week_12_tails_adj_gpf.png",
    "last_modified": "2023-11-29T00:33:33-05:00",
    "input_file": "post-week-12-tails-update.knit.md"
  },
  {
    "path": "posts/2023-11-22-post-week-11-tails-update/",
    "title": "Post Week 11 TAILS Update",
    "description": "Time Average Inpredictable Line Smidge",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-11-22",
    "categories": [],
    "contents": "\r\nLoad Libraries\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\nlibrary(nfltools)\r\nlibrary(nflplotR)\r\n\r\n\r\nGet Time Average Lead Data\r\n\r\n\r\nShow code\r\n\r\nraw_df <-\r\n  nfltools::nfl_mvt_season(2023)\r\n\r\n\r\nCalculate TAILS Adjustment\r\n\r\n\r\nShow code\r\n\r\ntails <-\r\n  raw_df %>%\r\n  filter(!(is.na(opponent))) %>%\r\n  mutate(\r\n    adj_time_avg_lead = time_avg_lead + (0.5 * team_spread)\r\n  ) %>%\r\n  group_by(team) %>%\r\n  slice_tail(n = 5) %>%\r\n  summarize(\r\n    tails = mean(adj_time_avg_lead),\r\n    .groups = \"drop\"\r\n  ) %>%\r\n  mutate(\r\n    tails = 0.2 * tails\r\n  )\r\n\r\ninpred <-\r\n  # This is a function I wrote as my first bit of\r\n  # webscraping code. It retrieves info from the\r\n  # inpredictable power ratings at the time it is run.\r\n  get_current_inpredictable()\r\n\r\ninpred <-\r\n  left_join(\r\n    inpred,\r\n    tails\r\n  ) %>%\r\n  mutate(\r\n    adj_gpf = gpf + tails\r\n  )\r\n\r\n\r\nPlot\r\n\r\n\r\nShow code\r\n\r\np_adj_gpf <-\r\n  inpred %>%\r\n  ggplot(\r\n    aes(\r\n      x = gpf,\r\n      y = reorder(factor(team),\r\n                  adj_gpf)\r\n    )\r\n  ) +\r\n  geom_vline(\r\n    xintercept = 0,\r\n    linetype = \"dashed\"\r\n  ) +\r\n  geom_segment(\r\n    aes(\r\n      x = gpf,\r\n      xend = adj_gpf,\r\n      yend = team\r\n    ),\r\n    arrow = arrow(\r\n      type = \"closed\",\r\n      length = unit(\r\n        0.1, \"cm\"\r\n      )\r\n    )\r\n  ) +\r\n  geom_point() +\r\n  geom_nfl_logos(\r\n    aes(\r\n      team_abbr = team,\r\n      x = adj_gpf),\r\n    width = 0.04,\r\n    alpha = 0.4\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = seq(-18, 18, by = 3),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_discrete(\r\n    labels = NULL,\r\n    breaks = NULL\r\n  ) +\r\n  labs(\r\n    x = \"TAILS Adjusted Generic Points Favored\",\r\n    y = NULL,\r\n    title = \"Post Week 11 NFL Power Rankings\",\r\n    subtitle = \"As of 11/21/2023.\",\r\n    caption = \"Baseline: inpredictable.com Betting Market Ratings\\nMy silly adjustment calculated using nflfastR data.\"\r\n  ) +\r\n  theme_light()\r\n\r\nggsave(\r\n  \"post_week_11_tails_adj_gpf.png\",\r\n  plot = p_adj_gpf,\r\n  width = 6,\r\n  height = 4.5,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nTable\r\n\r\n\r\nShow code\r\n\r\ninpred %>%\r\n  select(\r\n    team, gpf, tails, adj_gpf\r\n  ) %>%\r\n  arrange(\r\n    desc(adj_gpf)\r\n  ) %>%\r\n  mutate(\r\n    tails = round(tails, 2),\r\n    adj_gpf = round(adj_gpf, 2),\r\n    rank = row_number()\r\n  ) %>%\r\n  knitr::kable()\r\n\r\nteam\r\ngpf\r\ntails\r\nadj_gpf\r\nrank\r\nBAL\r\n6.2\r\n1.82\r\n8.02\r\n1\r\nSF\r\n7.3\r\n-0.12\r\n7.18\r\n2\r\nDAL\r\n5.6\r\n1.16\r\n6.76\r\n3\r\nKC\r\n6.4\r\n0.24\r\n6.64\r\n4\r\nPHI\r\n6.4\r\n-0.15\r\n6.25\r\n5\r\nMIA\r\n5.5\r\n-0.73\r\n4.77\r\n6\r\nBUF\r\n4.2\r\n-0.47\r\n3.73\r\n7\r\nDET\r\n3.7\r\n-0.73\r\n2.97\r\n8\r\nJAX\r\n2.1\r\n0.75\r\n2.85\r\n9\r\nLAC\r\n1.8\r\n0.54\r\n2.34\r\n10\r\nSEA\r\n-0.1\r\n-0.25\r\n-0.35\r\n11\r\nHOU\r\n-0.8\r\n0.32\r\n-0.48\r\n12\r\nDEN\r\n-1.4\r\n0.55\r\n-0.85\r\n13\r\nNO\r\n0.0\r\n-1.23\r\n-1.23\r\n14\r\nCLE\r\n-1.4\r\n-0.07\r\n-1.47\r\n15\r\nMIN\r\n-3.1\r\n1.33\r\n-1.77\r\n16\r\nPIT\r\n-1.6\r\n-0.20\r\n-1.80\r\n17\r\nGB\r\n-2.0\r\n-0.41\r\n-2.41\r\n18\r\nIND\r\n-2.5\r\n0.05\r\n-2.45\r\n19\r\nATL\r\n-2.1\r\n-0.48\r\n-2.58\r\n20\r\nCIN\r\n-2.9\r\n0.24\r\n-2.66\r\n21\r\nTB\r\n-2.7\r\n-0.06\r\n-2.76\r\n22\r\nLA\r\n-1.7\r\n-1.12\r\n-2.82\r\n23\r\nWAS\r\n-3.2\r\n-0.20\r\n-3.40\r\n24\r\nNE\r\n-3.6\r\n0.00\r\n-3.60\r\n25\r\nLV\r\n-4.0\r\n0.22\r\n-3.78\r\n26\r\nTEN\r\n-3.7\r\n-0.33\r\n-4.03\r\n27\r\nCHI\r\n-4.9\r\n0.49\r\n-4.41\r\n28\r\nARI\r\n-4.6\r\n-0.15\r\n-4.75\r\n29\r\nNYJ\r\n-5.1\r\n-0.85\r\n-5.95\r\n30\r\nCAR\r\n-6.4\r\n-0.37\r\n-6.77\r\n31\r\nNYG\r\n-7.9\r\n-0.26\r\n-8.16\r\n32\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-22-post-week-11-tails-update/post_week_11_tails_adj_gpf.png",
    "last_modified": "2023-11-22T00:23:55-05:00",
    "input_file": "post-week-11-tails-update.knit.md"
  },
  {
    "path": "posts/2023-11-15-post-week-10-tails-update/",
    "title": "Post Week 10 TAILS Update",
    "description": "Time Average Inpredictable Line Smidge",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-11-15",
    "categories": [],
    "contents": "\r\nLoad Libraries\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\nlibrary(nfltools)\r\nlibrary(nflplotR)\r\n\r\n\r\nGet Time Average Lead Data\r\n\r\n\r\nShow code\r\n\r\nraw_df <-\r\n  nfltools::nfl_mvt_season(2023)\r\n\r\n\r\nCalculate TAILS Adjustment\r\n\r\n\r\nShow code\r\n\r\ntails <-\r\n  raw_df %>%\r\n  filter(!(is.na(opponent))) %>%\r\n  mutate(\r\n    adj_time_avg_lead = time_avg_lead + (0.5 * team_spread)\r\n  ) %>%\r\n  group_by(team) %>%\r\n  slice_tail(n = 5) %>%\r\n  summarize(\r\n    tails = mean(adj_time_avg_lead),\r\n    .groups = \"drop\"\r\n  ) %>%\r\n  mutate(\r\n    tails = 0.2 * tails\r\n  )\r\n\r\ninpred <-\r\n  # This is a function I wrote as my first bit of\r\n  # webscraping code. It retrieves info from the\r\n  # inpredictable power ratings at the time it is run.\r\n  get_current_inpredictable()\r\n\r\ninpred <-\r\n  left_join(\r\n    inpred,\r\n    tails\r\n  ) %>%\r\n  mutate(\r\n    adj_gpf = gpf + tails\r\n  )\r\n\r\n\r\nPlot\r\n\r\n\r\nShow code\r\n\r\np_adj_gpf <-\r\n  inpred %>%\r\n  ggplot(\r\n    aes(\r\n      x = gpf,\r\n      y = reorder(factor(team),\r\n                  adj_gpf)\r\n    )\r\n  ) +\r\n  geom_vline(\r\n    xintercept = 0,\r\n    linetype = \"dashed\"\r\n  ) +\r\n  geom_segment(\r\n    aes(\r\n      x = gpf,\r\n      xend = adj_gpf,\r\n      yend = team\r\n    ),\r\n    arrow = arrow(\r\n      type = \"closed\",\r\n      length = unit(\r\n        0.1, \"cm\"\r\n      )\r\n    )\r\n  ) +\r\n  geom_point() +\r\n  geom_nfl_logos(\r\n    aes(\r\n      team_abbr = team,\r\n      x = adj_gpf),\r\n    width = 0.04,\r\n    alpha = 0.4\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = seq(-18, 18, by = 3),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_discrete(\r\n    labels = NULL,\r\n    breaks = NULL\r\n  ) +\r\n  labs(\r\n    x = \"TAILS Adjusted Generic Points Favored\",\r\n    y = NULL,\r\n    title = \"Post Week 10 NFL Power Rankings\",\r\n    subtitle = \"As of 11/14/2023.\",\r\n    caption = \"Baseline: inpredictable.com Betting Market Ratings\\nMy silly adjustment calculated using nflfastR data.\"\r\n  ) +\r\n  theme_light()\r\n\r\nggsave(\r\n  \"post_week_10_tails_adj_gpf.png\",\r\n  plot = p_adj_gpf,\r\n  width = 6,\r\n  height = 4.5,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nTable\r\n\r\n\r\nShow code\r\n\r\ninpred %>%\r\n  select(\r\n    team, gpf, tails, adj_gpf\r\n  ) %>%\r\n  arrange(\r\n    desc(adj_gpf)\r\n  ) %>%\r\n  mutate(\r\n    tails = round(tails, 2),\r\n    adj_gpf = round(adj_gpf, 2),\r\n    rank = row_number()\r\n  ) %>%\r\n  knitr::kable()\r\n\r\nteam\r\ngpf\r\ntails\r\nadj_gpf\r\nrank\r\nSF\r\n7.6\r\n0.35\r\n7.95\r\n1\r\nBAL\r\n5.6\r\n1.72\r\n7.32\r\n2\r\nKC\r\n6.6\r\n0.33\r\n6.93\r\n3\r\nDAL\r\n5.5\r\n0.39\r\n5.89\r\n4\r\nPHI\r\n5.8\r\n-0.03\r\n5.77\r\n5\r\nMIA\r\n4.9\r\n-0.35\r\n4.55\r\n6\r\nCIN\r\n3.4\r\n0.61\r\n4.01\r\n7\r\nDET\r\n3.3\r\n-0.02\r\n3.28\r\n8\r\nLAC\r\n2.2\r\n0.57\r\n2.77\r\n9\r\nJAX\r\n1.5\r\n0.74\r\n2.24\r\n10\r\nBUF\r\n2.9\r\n-1.27\r\n1.63\r\n11\r\nCLE\r\n1.3\r\n-0.09\r\n1.21\r\n12\r\nSEA\r\n0.4\r\n-0.58\r\n-0.18\r\n13\r\nNO\r\n0.4\r\n-1.23\r\n-0.83\r\n14\r\nDEN\r\n-1.9\r\n0.61\r\n-1.29\r\n15\r\nPIT\r\n-1.2\r\n-0.24\r\n-1.44\r\n16\r\nMIN\r\n-3.0\r\n1.33\r\n-1.67\r\n17\r\nWAS\r\n-2.2\r\n0.45\r\n-1.75\r\n18\r\nHOU\r\n-2.4\r\n0.34\r\n-2.06\r\n19\r\nTB\r\n-2.0\r\n-0.17\r\n-2.17\r\n20\r\nIND\r\n-2.3\r\n0.05\r\n-2.25\r\n21\r\nATL\r\n-1.9\r\n-0.48\r\n-2.38\r\n22\r\nGB\r\n-2.2\r\n-0.47\r\n-2.67\r\n23\r\nLA\r\n-2.1\r\n-0.88\r\n-2.98\r\n24\r\nNE\r\n-3.2\r\n0.00\r\n-3.20\r\n25\r\nTEN\r\n-3.2\r\n-0.11\r\n-3.31\r\n26\r\nNYJ\r\n-3.0\r\n-0.36\r\n-3.36\r\n27\r\nLV\r\n-4.8\r\n0.17\r\n-4.63\r\n28\r\nCHI\r\n-5.4\r\n0.03\r\n-5.37\r\n29\r\nARI\r\n-6.0\r\n-0.07\r\n-6.07\r\n30\r\nCAR\r\n-6.6\r\n-0.54\r\n-7.14\r\n31\r\nNYG\r\n-9.4\r\n-0.22\r\n-9.62\r\n32\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-15-post-week-10-tails-update/post_week_10_tails_adj_gpf.png",
    "last_modified": "2023-11-15T00:26:36-05:00",
    "input_file": "post-week-10-tails-update.knit.md"
  },
  {
    "path": "posts/2023-11-07-post-week-9-tails-update/",
    "title": "Post Week 9 TAILS Update",
    "description": "Time Average Inpredictable Line Smidge",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-11-07",
    "categories": [],
    "contents": "\r\nLoad Libraries\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\nlibrary(nfltools)\r\nlibrary(nflplotR)\r\n\r\n\r\nGet Time Average Lead Data\r\n\r\n\r\nShow code\r\n\r\nraw_df <-\r\n  nfltools::nfl_mvt_season(2023)\r\n\r\n\r\nCalculate TAILS Adjustment\r\n\r\n\r\nShow code\r\n\r\ntails <-\r\n  raw_df %>%\r\n  filter(!(is.na(opponent))) %>%\r\n  mutate(\r\n    adj_time_avg_lead = time_avg_lead + (0.5 * team_spread)\r\n  ) %>%\r\n  group_by(team) %>%\r\n  slice_tail(n = 5) %>%\r\n  summarize(\r\n    tails = mean(adj_time_avg_lead),\r\n    .groups = \"drop\"\r\n  ) %>%\r\n  mutate(\r\n    tails = 0.2 * tails\r\n  )\r\n\r\ninpred <-\r\n  # This is a function I wrote as my first bit of\r\n  # webscraping code. It retrieves info from the\r\n  # inpredictable power ratings at the time it is run.\r\n  get_current_inpredictable()\r\n\r\ninpred <-\r\n  left_join(\r\n    inpred,\r\n    tails\r\n  ) %>%\r\n  mutate(\r\n    adj_gpf = gpf + tails\r\n  )\r\n\r\n\r\nPlot\r\n\r\n\r\nShow code\r\n\r\np_adj_gpf <-\r\n  inpred %>%\r\n  ggplot(\r\n    aes(\r\n      x = gpf,\r\n      y = reorder(factor(team),\r\n                  adj_gpf)\r\n    )\r\n  ) +\r\n  geom_vline(\r\n    xintercept = 0,\r\n    linetype = \"dashed\"\r\n  ) +\r\n  geom_segment(\r\n    aes(\r\n      x = gpf,\r\n      xend = adj_gpf,\r\n      yend = team\r\n    ),\r\n    arrow = arrow(\r\n      type = \"closed\",\r\n      length = unit(\r\n        0.1, \"cm\"\r\n      )\r\n    )\r\n  ) +\r\n  geom_point() +\r\n  geom_nfl_logos(\r\n    aes(\r\n      team_abbr = team,\r\n      x = adj_gpf),\r\n    width = 0.04,\r\n    alpha = 0.4\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = seq(-18, 18, by = 3),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_discrete(\r\n    labels = NULL,\r\n    breaks = NULL\r\n  ) +\r\n  labs(\r\n    x = \"TAILS Adjusted Generic Points Favored\",\r\n    y = NULL,\r\n    title = \"Post Week 9 NFL Power Rankings\",\r\n    subtitle = \"As of 11/7/2023.\",\r\n    caption = \"Baseline: inpredictable.com Betting Market Ratings\\nMy silly adjustment calculated using nflfastR data.\"\r\n  ) +\r\n  theme_light()\r\n\r\nggsave(\r\n  \"post_week_9_tails_adj_gpf.png\",\r\n  plot = p_adj_gpf,\r\n  width = 6,\r\n  height = 4.5,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nTable\r\n\r\n\r\nShow code\r\n\r\ninpred %>%\r\n  select(\r\n    team, gpf, tails, adj_gpf\r\n  ) %>%\r\n  arrange(\r\n    desc(adj_gpf)\r\n  ) %>%\r\n  mutate(\r\n    tails = round(tails, 2),\r\n    adj_gpf = round(adj_gpf, 2),\r\n    rank = row_number()\r\n  ) %>%\r\n  knitr::kable()\r\n\r\nteam\r\ngpf\r\ntails\r\nadj_gpf\r\nrank\r\nBAL\r\n6.1\r\n1.57\r\n7.67\r\n1\r\nSF\r\n6.5\r\n-0.11\r\n6.39\r\n2\r\nKC\r\n6.0\r\n0.33\r\n6.33\r\n3\r\nDAL\r\n4.7\r\n0.58\r\n5.28\r\n4\r\nPHI\r\n5.2\r\n-0.03\r\n5.17\r\n5\r\nCIN\r\n4.2\r\n0.26\r\n4.46\r\n6\r\nJAX\r\n2.0\r\n1.63\r\n3.63\r\n7\r\nMIA\r\n3.8\r\n-0.35\r\n3.45\r\n8\r\nDET\r\n3.1\r\n0.31\r\n3.41\r\n9\r\nLAC\r\n1.2\r\n1.00\r\n2.19\r\n10\r\nBUF\r\n3.5\r\n-1.36\r\n2.14\r\n11\r\nCLE\r\n1.9\r\n-0.25\r\n1.65\r\n12\r\nNO\r\n0.9\r\n0.03\r\n0.93\r\n13\r\nSEA\r\n1.0\r\n-0.13\r\n0.87\r\n14\r\nHOU\r\n-1.9\r\n0.67\r\n-1.23\r\n15\r\nTEN\r\n-1.9\r\n0.57\r\n-1.33\r\n16\r\nPIT\r\n-1.0\r\n-0.83\r\n-1.83\r\n17\r\nIND\r\n-2.1\r\n0.12\r\n-1.98\r\n18\r\nTB\r\n-2.3\r\n-0.02\r\n-2.32\r\n19\r\nDEN\r\n-2.4\r\n0.01\r\n-2.39\r\n20\r\nMIN\r\n-3.4\r\n0.73\r\n-2.67\r\n21\r\nATL\r\n-2.3\r\n-0.48\r\n-2.78\r\n22\r\nNYJ\r\n-2.6\r\n-0.43\r\n-3.03\r\n23\r\nGB\r\n-2.3\r\n-0.90\r\n-3.20\r\n24\r\nWAS\r\n-3.0\r\n-0.32\r\n-3.32\r\n25\r\nCHI\r\n-4.3\r\n0.76\r\n-3.54\r\n26\r\nLA\r\n-3.0\r\n-0.88\r\n-3.88\r\n27\r\nNE\r\n-3.3\r\n-0.74\r\n-4.04\r\n28\r\nLV\r\n-4.7\r\n0.23\r\n-4.47\r\n29\r\nARI\r\n-5.6\r\n-0.26\r\n-5.86\r\n30\r\nCAR\r\n-6.2\r\n-0.48\r\n-6.68\r\n31\r\nNYG\r\n-8.7\r\n0.02\r\n-8.68\r\n32\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-07-post-week-9-tails-update/post_week_9_tails_adj_gpf.png",
    "last_modified": "2023-11-07T23:02:59-05:00",
    "input_file": "post-week-9-tails-update.knit.md"
  },
  {
    "path": "posts/2023-10-30-tails-time-average-inpredictable-line-smidge/",
    "title": "TAILS: Time Average Inpredictable Line Smidgen",
    "description": "A Time Average Adjustment to inpredictable Market-based Rankings",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-10-30",
    "categories": [],
    "contents": "\r\nLoad Libraries\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\nlibrary(nfltools)\r\nlibrary(nflplotR)\r\n\r\n\r\nGet Time Average Lead Data\r\n\r\n\r\nShow code\r\n\r\nseasons <-\r\n  2018:2023\r\n\r\nraw_df <-\r\n  purrr::map_df(\r\n    seasons,\r\n    nfl_mvt_season\r\n  )\r\n\r\n\r\nIntro\r\nPreviously, I made some code that calculates the (time) average lead for an NFL game using nflfastR play-by-play data. Based on this data, I developed a simple retrospective expected win percentage model. Here, I play with another use of time average lead data: in-season handicap adjustments.\r\nIt is a common practice for traditional handicappers to adjust team power ratings weekly based on performance and health.\r\nAdjusted Time Average Lead Metric\r\nThe spread provides an estimate of the median final score differential. To provide a context adjustment (e.g., opponent, home field advantage, weather), assume linear growth between (1) a 0 point differential at opening kickoff and (2) final score differential implied by the spread.\r\nThe time average lead implied by this linear model is 1/2 of the spread.\r\nToy Example\r\nAssume Team A is favored by 6 points (-6 point spread). Then this simple adjustment model expects Team A to lead, on average, by 3 points (1/2 of the 6 point spread). The actual calculated time average lead for the game reveals Team A led, on average, by 2 points. Although Team A held a lead, on average, for the course of their game, they slightly underperformed the Vegas expectation (i.e., 2 point actual average lead - 3 point expected average lead = -1). This difference between actual and expected time average lead could provide some information for a power rating adjustment.\r\nCalculate Adjustment\r\n\r\n\r\nShow code\r\n\r\ndf <-\r\n  raw_df %>%\r\n  # This is a convenient way to ensure each\r\n  # game shows up in data once.\r\n  filter(home_away == \"home\") %>%\r\n  # Removes bye weeks entries\r\n  filter(is.na(home_away) == F) %>%\r\n  # The spread provides an estimate of the median\r\n  # final score differential (and incorporates home\r\n  # field advantage). To provide an adjustment, assume linear\r\n  # growth from (1) a 0 point differential  at opening kickoff\r\n  # to (2) final score differential implied by spread. The\r\n  # time average lead implied by this linear model is\r\n  # 1/2 of the spread.\r\n  mutate(\r\n    adj_time_avg_lead = time_avg_lead + (0.5 * team_spread)\r\n  )\r\n\r\n\r\nDistribution of Adjusted Time Average Leads\r\nTo understand the distribution of adjusted time average leads, a density plot is created and some summary statistics are calculated.\r\nIdeally, our adjustment would yield a distribution centered at 0. It does!\r\nIn addition, the standard deviation is about 8 points (one possession). This is an easy value to remember and interpret.\r\n\r\n\r\nShow code\r\n\r\np_adj_distribution <-\r\n  df %>%\r\n  ggplot(aes(x = adj_time_avg_lead)) +\r\n  geom_density() +\r\n  geom_vline(\r\n    xintercept = \r\n      c(-16, -8, 0, 8, 16),\r\n    linetype = \"dashed\"\r\n  ) +\r\n  labs(\r\n    x = \"Adjusted Time Average Lead\",\r\n    y = \"Density\",\r\n    title = \"Vegas Spread Adjusted Time Average Lead\",\r\n    subtitle = paste0(\r\n      \"Mean = \",\r\n      mean(df$adj_time_avg_lead) %>%\r\n        round(., digits = 1),\r\n      \". Median = \",\r\n      median(df$adj_time_avg_lead) %>%\r\n        round(., digits = 1),\r\n      \". Std Deviation = \",\r\n      sd(df$adj_time_avg_lead) %>%\r\n        round(., digits = 1)\r\n    ),\r\n    caption = paste0(\r\n      \"2018 - 2023 Regular Season Games (n = \",\r\n      nrow(df),\r\n      \")\"\r\n    )\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = seq(-32, 32, by = 4),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_continuous(\r\n    minor_breaks = NULL\r\n  ) +\r\n  theme_light()\r\n\r\nggsave(\r\n  \"adjusted_time_avg_lead.png\",\r\n  plot = p_adj_distribution,\r\n  width = 6,\r\n  height = 4,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nTAILS: Time Averge inpredictable Line Smidgen\r\nHere, I put together a toy adjustment.\r\nA common discussion point early in a season is the question: How should you best incorporate early results with your preseason priors? A single game isn’t much use. As a first guess, I’ll consider the most recent four or five games to be an appropriate number to develop an average that is recent enough to still be relevant.\r\nAdjustments should be modest (e.g., 3 points should be extremely rare). The distribution of an average of five games should be about half as wide as the underlying distribution. That would put a result that is two standard deviations above the mean at roughly +8 points, which is way too big for power rating adjustment. Because this is just a toy project, I’m going to choose to scale the results by 0.2 to pull this hypothetical two standard deviations above the mean result to under 2 points. This will yield a result to adjust inpredictable’s market-based ratings (that are derived from game lines for the entire season) by an algorithmically-defined smidgen.\r\nA key element any modern sports metrics is an asinine acronym. My daughter loves Miles “Tails” Prower of the Sonic the Hedgehog series. Therefore, I am obliged to shoehorn my idea into the TAILS acronym: Time Average inpredictable Line Smidgen.\r\nCredit: JC Thornton at DeviantArt.comCurrent 2023 TAILS Adjustment\r\n\r\n\r\nShow code\r\n\r\ntails <-\r\n  raw_df %>%\r\n  filter(season == 2023) %>%\r\n  filter(!(is.na(opponent))) %>%\r\n  mutate(\r\n    adj_time_avg_lead = time_avg_lead + (0.5 * team_spread)\r\n  ) %>%\r\n  group_by(team) %>%\r\n  slice_tail(n = 5) %>%\r\n  summarize(\r\n    tails = mean(adj_time_avg_lead),\r\n    .groups = \"drop\"\r\n  ) %>%\r\n  mutate(\r\n    tails = 0.2 * tails\r\n  )\r\n\r\ninpred <-\r\n  # This is a function I wrote as my first bit of\r\n  # webscraping code. It retrieves info from the\r\n  # inpredictable power ratings at the time it is run.\r\n  get_current_inpredictable()\r\n\r\ninpred <-\r\n  left_join(\r\n    inpred,\r\n    tails\r\n  ) %>%\r\n  mutate(\r\n    adj_gpf = gpf + tails\r\n  )\r\n\r\n\r\nPlot\r\n\r\n\r\nShow code\r\n\r\np_adj_gpf <-\r\n  inpred %>%\r\n  ggplot(\r\n    aes(\r\n      x = gpf,\r\n      y = reorder(factor(team),\r\n                  adj_gpf)\r\n    )\r\n  ) +\r\n  geom_vline(\r\n    xintercept = 0,\r\n    linetype = \"dashed\"\r\n  ) +\r\n  geom_segment(\r\n    aes(\r\n      x = gpf,\r\n      xend = adj_gpf,\r\n      yend = team\r\n    ),\r\n    arrow = arrow(\r\n      type = \"closed\",\r\n      length = unit(\r\n        0.1, \"cm\"\r\n      )\r\n    )\r\n  ) +\r\n  geom_point() +\r\n  geom_nfl_logos(\r\n    aes(\r\n      team_abbr = team,\r\n      x = adj_gpf),\r\n    width = 0.04,\r\n    alpha = 0.4\r\n  ) +\r\n  scale_x_continuous(\r\n    breaks = seq(-18, 18, by = 3),\r\n    minor_breaks = NULL\r\n  ) +\r\n  scale_y_discrete(\r\n    labels = NULL,\r\n    breaks = NULL\r\n  ) +\r\n  labs(\r\n    x = \"TAILS Adjusted Generic Points Favored\",\r\n    y = NULL,\r\n    title = \"Post Week 8 NFL Power Rankings\",\r\n    subtitle = \"As of 10/31/2023.\",\r\n    caption = \"Baseline: inpredictable.com Betting Market Ratings\\nMy silly adjustment calculated using nflfastR data.\"\r\n  ) +\r\n  theme_light()\r\n\r\nggsave(\r\n  \"tails_adj_gpf.png\",\r\n  plot = p_adj_gpf,\r\n  width = 6,\r\n  height = 4.5,\r\n  units = \"in\",\r\n  dpi = \"retina\"\r\n)\r\n\r\n\r\n\r\nTable\r\n\r\n\r\nShow code\r\n\r\ninpred %>%\r\n  select(\r\n    team, gpf, tails, adj_gpf\r\n  ) %>%\r\n  arrange(\r\n    desc(adj_gpf)\r\n  ) %>%\r\n  mutate(\r\n    tails = round(tails, 2),\r\n    adj_gpf = round(adj_gpf, 2),\r\n    rank = row_number()\r\n  ) %>%\r\n  knitr::kable()\r\n\r\nteam\r\ngpf\r\ntails\r\nadj_gpf\r\nrank\r\nSF\r\n6.5\r\n-0.11\r\n6.39\r\n1\r\nBAL\r\n4.7\r\n1.57\r\n6.27\r\n2\r\nKC\r\n5.8\r\n-0.01\r\n5.79\r\n3\r\nPHI\r\n5.4\r\n-0.32\r\n5.08\r\n4\r\nCIN\r\n5.0\r\n-0.04\r\n4.96\r\n5\r\nBUF\r\n4.9\r\n-0.58\r\n4.32\r\n6\r\nDAL\r\n4.1\r\n0.06\r\n4.16\r\n7\r\nJAX\r\n2.3\r\n1.63\r\n3.93\r\n8\r\nMIA\r\n4.1\r\n-0.46\r\n3.64\r\n9\r\nDET\r\n2.2\r\n0.31\r\n2.51\r\n10\r\nLAC\r\n1.7\r\n0.73\r\n2.43\r\n11\r\nSEA\r\n0.7\r\n0.33\r\n1.03\r\n12\r\nNO\r\n0.1\r\n-0.05\r\n0.05\r\n13\r\nATL\r\n0.6\r\n-0.80\r\n-0.20\r\n14\r\nHOU\r\n-1.6\r\n1.38\r\n-0.22\r\n15\r\nMIN\r\n-1.2\r\n0.58\r\n-0.62\r\n16\r\nCLE\r\n-0.5\r\n-0.13\r\n-0.63\r\n17\r\nPIT\r\n-1.3\r\n-0.49\r\n-1.79\r\n18\r\nDEN\r\n-1.8\r\n0.01\r\n-1.79\r\n19\r\nTEN\r\n-2.3\r\n0.20\r\n-2.10\r\n20\r\nTB\r\n-2.2\r\n-0.37\r\n-2.57\r\n21\r\nLA\r\n-2.4\r\n-0.24\r\n-2.64\r\n22\r\nNYJ\r\n-2.8\r\n-0.24\r\n-3.04\r\n23\r\nGB\r\n-1.7\r\n-1.43\r\n-3.13\r\n24\r\nWAS\r\n-3.1\r\n-0.15\r\n-3.25\r\n25\r\nNE\r\n-2.2\r\n-1.27\r\n-3.47\r\n26\r\nIND\r\n-3.2\r\n-0.71\r\n-3.91\r\n27\r\nLV\r\n-3.8\r\n-0.65\r\n-4.45\r\n28\r\nCHI\r\n-5.6\r\n0.90\r\n-4.70\r\n29\r\nNYG\r\n-5.4\r\n0.31\r\n-5.09\r\n30\r\nARI\r\n-6.9\r\n-0.16\r\n-7.06\r\n31\r\nCAR\r\n-6.9\r\n-0.17\r\n-7.07\r\n32\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-10-31T23:11:12-04:00",
    "input_file": "tails-time-average-inpredictable-line-smidge.knit.md"
  },
  {
    "path": "posts/2023-05-31-buffet-and-munger/",
    "title": "Buffet and Munger",
    "description": "A collection of quotes.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-05-31",
    "categories": [],
    "contents": "\r\nIntro\r\nThis isn’t sports related content, and contains no analysis.\r\nHowever, I’ve recently been listening to clips of Charlie Munger and Warren Buffett pretty often. Along the way, I pulled out some interesting quotes, many which pertain to decision making or sense-making under uncertainty.\r\nQuotes\r\n\r\n“I think he also asked, ‘How do you forecast these improvements in P/E ratios?’\r\nAround here, I would say, ‘If our predictions have been a little better than\r\nother people’s, it’s because we tried to make fewer of them.’”\r\n— Charlie Munger, 1998 Berkshire Hathaway Annual Meeting\r\n\r\n\r\n“All options have value, and people that get options usually understand that\r\nbetter than people who give options… It is the nature of prices in this world\r\nto change and economic conditions to change, and an option is a chance to\r\nparticipate in the change without give up anything other than that original\r\npremium you pay.”\r\n— Warren Buffett, 2002 Berkshire Hathaway Annual Meeting\r\n\r\n\r\n“If you had to double your money by the end of the year or be shot, you know\r\nthen I would head for the futures market because you need to do it. You have to\r\nintroduce borrowed money.”\r\n— Warren Buffett, 1997 Berkshire Hathaway Annual Meeting\r\n\r\n\r\n“The world changed. Before he died… Ben Graham recognized that the exact\r\nway he sought under-valued companies wouldn’t necessarily work for all times\r\nunder all conditions. That’s certainly the way it worked for us. We gradually\r\nmorphed into trying to buy the better companies when they were underpriced\r\ninstead of the lousy companies when they were underpriced.”\r\n— Charlie Munger, 2018 Berkshire Hathaway Annual Meeting\r\n\r\n\r\n“Time is the friend of the wonderful business: you keep compounding, it keeps\r\ndoing more business, and keeps making more money. Time is the enemy of the\r\nlousy business.”\r\n— Warren Buffett\r\n\r\n\r\n“I think in the last analysis everything we do come back to opportunity cost…\r\nTo some considerable extent, we are guessing at our future opportunity cost.\r\nWarren is basically saying that he’s guessing that [he’ll have opportunities]\r\nin due course to put out money at pretty attractive rates of return, and therefore\r\nhe’s not going to waste a lot of firepower at lower returns. But that’s an\r\nopportunity cost calculation.”\r\n— Charlie Munger, 2003 Berkshire Hathaway Annual Meeting\r\n\r\n\r\n“A business, or any economic asset, is going to be worth what it produces in the\r\nway of cash over its lifetime… All investment is is laying out some money now\r\nto get more money back in the future. Now there’s two ways to look at the getting\r\nmoney back. One is from what the asset itself will produce: that’s investment. One\r\nis from what somebody else will pay you for it later on, irrespective of what the\r\nasset produces, and I call that speculation.”\r\n— Warren Buffett, 2002 Berkshire Hathaway Annual Meeting\r\n\r\n\r\n“As a general matter, there are only two reasons to buy insurance. One is to\r\nprotect yourself against a loss that you are unable or unwilling to bear\r\nyourself… The second reason… is if you think the insurance company is actually\r\nselling you a policy that is too cheap… We try to avoid selling the second kind\r\nand concentrate on selling the first kind.”\r\n— Warren Buffett, 1995 Berkshire Hathaway Annual Meeting\r\n\r\n\r\n“There are certain companies that are exposing themselves… to losses that would\r\nwipe them out, and they prefer not to be re-insurance because it’s ‘expensive.’\r\nWhat they are really doing is betting on something that won’t happen very often\r\nhappening not at all.”\r\n— Warren Buffett, 1995 Berkshire Hathaway Annual Meeting\r\n\r\n\r\n“You do something, and the market goes up, and you get paid and rewarded and\r\napplauded and what have you: you’re getting a lot reinforcement if you make a\r\nbet in the market and the market goes with you. Also, there’s social proof…\r\nthe prices in the market are the ultimate form of social proof, reflecting what\r\nother people think. The combination is very powerful. Why would you expect general\r\nmarket levels to always be totally efficient?”\r\n— Charlie Munger, The Psychology of Human Misjudgement\r\n\r\n\r\n“You’ve got to array facts on theory structures, answering the question, ‘Why?’.\r\nIf you don’te do that… you cannot handle the world.”\r\n— Charlie Munger, The Psychology of Human Misjudgement\r\n\r\n\r\n“Clinical training in medical schools… here’s a profoundly correct way of\r\nunderstanding psychology. The standard practice is watch one, do one, teach one.\r\nBoy does that pound in what you want pounded in!”\r\n— Charlie Munger, The Psychology of Human Misjudgement\r\n\r\n\r\n“Someone once said, ‘The chains of habit are too light to be felt until they are\r\ntoo heavy to be broken.’”\r\n— Warren Buffett\r\n\r\n\r\n“I’d rather buy a wonderful business at a fair price than a fair business at a\r\nwonderful price.”\r\n— Warren Buffett\r\n\r\n\r\n“You have to understand one thing about the Fed: it’s not as powerful as the\r\nmystique would make it. It’s brake is better than it’s gas pedal. When the Fed\r\nwants to put it’s foot on the brake, we go through the windshield… Stepping on\r\nthe gas does not necessarily get the same result.”\r\n— Warren Buffett\r\n\r\n\r\n“It’s a terrible mistake to sleep-walk through life… If you are in a position\r\nto make choices, I always tell the kids that come visit me to go to work for an\r\norganization that you admire or an individual that you admire. That means that\r\nmany of them become self-employed, but…”\r\n— Warren Buffett, 2008 Berkshire Hathaway Annual Meeting\r\n\r\n\r\n“What the human being is best at doing is interpreting all new information so\r\nthat their prior conclusions remain intact… That is a talent that everyone\r\nseems to have mastered.”\r\n— Warren Buffett, 2002 Berkshire Hathaway Annual Meeting\r\n\r\n\r\n“Black-Scholes is what I would call a ‘know-nothing’ value system. If you don’t\r\nknow anything at all about value compared with price—in other words, if price\r\nis teaching you all that can be known—then Black-Scholes, on a very short-term\r\nbasis, is a pretty good guess… The minute you get into longer-term options,\r\nwhere you don’t have the ‘know-nothing’ factor so extreme, it’s crazy to use\r\nBlack-Scholes. People use it because they want some kind of mechanical system.”\r\n— Charlie Munger\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-05-31T13:51:33-04:00",
    "input_file": "buffet-and-munger.knit.md"
  },
  {
    "path": "posts/2023-05-09-2022-nfl-regular-season-win-luck/",
    "title": "2022 NFL Regular Season Win Luck",
    "description": "Estimates of NFL Regular Season Record Luck",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2023-05-09",
    "categories": [],
    "contents": "\r\nPackages Used\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(nflfastR)\r\nlibrary(nflplotR)\r\nlibrary(nfltools)\r\n\r\n\r\nTwo Measures of Win ‘Luck’\r\nThis post takes a look at two estimates of win luck, or roughly, actual record compared with “expected” record.\r\nFirst, Pythagorean Win Expectation. I’ll use the Daryl Morey formulation, which built on Bill James’s work for baseball. The basic logic of Pythagorean Win Expectation is as follows: total points scored and allowed for an entire regular season are more reliable indicator of “true” ability than the sum of individual game outcomes (i.e., win/loss/tie). Thus, I think of Pythagorean Win Expectation as an across game estimate of expected regular season record.\r\nSecond, I’ll use my time average lead concept. This approach estimates the expected win percentage for each individual game, and estimates expected wins for a season using the sum of these individual game expected win percentage estimates. Since the estimation is done at the individual game level, I’ll call this a within game estimate.\r\nTime Average Win Expectancy Model\r\nThe code below fits a logistic model to the 2015 to 2021 regular seasons.\r\n\r\n\r\nShow code\r\n\r\ntrain <- map_dfr(\r\n  2015:2021,\r\n  nfl_mvt_season) %>%\r\n  filter(home_away == \"home\")\r\n\r\ntrain_results <- load_pbp(2015:2021) %>%\r\n  filter(season_type == \"REG\") %>%\r\n  mutate(home_result = case_when(\r\n    result > 0 ~ \"Win\",\r\n    result == 0 ~ \"Tie\",\r\n    result < 0 ~ \"Loss\"\r\n  )) %>%\r\n  group_by(game_id) %>%\r\n  summarize(\r\n    season = season,\r\n    week = week,\r\n    team = home_team,\r\n    opponent = away_team,\r\n    result = home_result,\r\n    .groups = \"drop\") %>%\r\n  distinct() %>%\r\n  mutate(game_id = NULL)\r\n\r\ntrain_df <- left_join(\r\n  train,\r\n  train_results) %>%\r\n  filter(result != \"Tie\") %>%\r\n  filter(home_away == \"home\") %>%\r\n  mutate(\r\n    result = ifelse(\r\n      result == \"Win\",\r\n      1L, 0L),\r\n    home_away = as.factor(home_away)\r\n  )\r\n\r\nmodel <- rstanarm::stan_glm(\r\n  result ~ -1 + time_avg_lead,\r\n  data = train_df,\r\n  family = \"binomial\",\r\n  refresh = 0\r\n)\r\n\r\ntest <- nfl_mvt_season(2022)\r\n\r\nbeta <- coef(model)\r\n\r\ntest_df <- test %>%\r\n  mutate(exp_wp = 1/(1 + exp(-beta * time_avg_lead)))\r\n\r\ndf_2022 <- test_df %>%\r\n  group_by(team) %>%\r\n  summarize(exp_wp = mean(exp_wp, na.rm = T),\r\n            .groups = \"drop\")\r\n\r\n\r\nPythagorean Win Expectancy\r\nNext, I assemble the information to calculate the Pythagorean Win Expectation, Actual Wins, and the tow “Win Luck” estimates.\r\n\r\n\r\nShow code\r\n\r\nhome_2022 <- load_pbp(2022) %>%\r\n  filter(season_type == \"REG\") %>%\r\n  group_by(game_id) %>%\r\n  slice_head(n = 1) %>%\r\n  ungroup() %>%\r\n  mutate(home_win = ifelse(result > 0,\r\n                           1L, 0L)) %>%\r\n  group_by(home_team) %>%\r\n  summarize(games_played = n(),\r\n            points_scored = sum(home_score),\r\n            points_allowed = sum(away_score),\r\n            wins = sum(home_win),\r\n            .groups = \"drop\") %>%\r\n  rename(team = home_team)\r\n\r\naway_2022 <- load_pbp(2022) %>%\r\n  filter(season_type == \"REG\") %>%\r\n  group_by(game_id) %>%\r\n  slice_head(n = 1) %>%\r\n  ungroup() %>%\r\n  mutate(away_win = ifelse(result < 0,\r\n                           1L, 0L)) %>%\r\n  group_by(away_team) %>%\r\n  summarize(games_played = n(),\r\n            points_scored = sum(away_score),\r\n            points_allowed = sum(home_score),\r\n            wins = sum(away_win),\r\n            .groups = \"drop\") %>%\r\n  rename(team = away_team)\r\n\r\nteam_2022 <- rbind(\r\n  home_2022,\r\n  away_2022) %>%\r\n  group_by(team) %>%\r\n  summarize(\r\n    games_played = sum(games_played),\r\n    points_scored = sum(points_scored),\r\n    points_allowed = sum(points_allowed),\r\n    wins = sum(wins),\r\n    .groups = \"drop\") %>%\r\n  mutate(actual_wp = wins/games_played,\r\n         pythag_wp = (points_scored ^ 2.37)/((points_scored ^ 2.37) + (points_allowed) ^ 2.37)) %>%\r\n  left_join(df_2022) %>%\r\n  mutate(time_avg_win_luck = (actual_wp - exp_wp) * 17,\r\n         pythag_win_luck = (actual_wp - pythag_wp) * 17)\r\n\r\n\r\nNote, the “actual” wins for BUF and CIN are estimates too, in that their 16 game win percentage is extrapolated to 17 games to calculate actual wins.\r\nModel Summary\r\nThe summary for the time average win expectation model:\r\n\r\n\r\nShow code\r\n\r\nsummary(model,\r\n        digits = 3)\r\n\r\n\r\nModel Info:\r\n function:     stan_glm\r\n family:       binomial [logit]\r\n formula:      result ~ -1 + time_avg_lead\r\n algorithm:    sampling\r\n sample:       4000 (posterior sample size)\r\n priors:       see help('prior_summary')\r\n observations: 1801\r\n predictors:   1\r\n\r\nEstimates:\r\n                mean   sd    10%   50%   90%\r\ntime_avg_lead 0.322  0.015 0.304 0.322 0.341\r\n\r\nFit Diagnostics:\r\n           mean   sd    10%   50%   90%\r\nmean_PPD 0.547  0.008 0.536 0.547 0.557\r\n\r\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\r\n\r\nMCMC diagnostics\r\n              mcse  Rhat  n_eff\r\ntime_avg_lead 0.000 1.001 1520 \r\nmean_PPD      0.000 1.000 3862 \r\nlog-posterior 0.021 1.001 1152 \r\n\r\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\r\n\r\nPlot\r\n\r\n\r\nShow code\r\n\r\np <- team_2022 %>%\r\n  ggplot(aes(x = time_avg_win_luck,\r\n             y = pythag_win_luck)) +\r\n  geom_hline(yintercept = 0) +\r\n  geom_vline(xintercept = 0) +\r\n  geom_nfl_logos(aes(team_abbr = team),\r\n                 width = 0.05) +\r\n  geom_label(aes(x = -2,\r\n                y = 3,\r\n                label = paste0(\"Correlation: \",\r\n                               cor(team_2022$time_avg_win_luck,\r\n                                   team_2022$pythag_win_luck) %>%\r\n                                 round(2)))) +\r\n  scale_x_continuous(breaks = seq(-6, 6, by = 1),\r\n                     minor_breaks = NULL) +\r\n  scale_y_continuous(breaks = seq(-6, 6, by = 1),\r\n                     minor_breaks = NULL) +\r\n  labs(x = \"Time Average Win Luck\",\r\n       y = \"Pythagorean Win Luck\",\r\n       caption = \"Data via nflfastR. Plot via nflplotR.\",\r\n       title = \"NFL 2022 Regular Season Win 'Luck'\",\r\n       subtitle = \"Within Game (Time Average) and Across Game (Pythagorean)\") +\r\n  theme_light()\r\n\r\nggsave(plot = p,\r\n       filename = \"nfl_2022_win_luck.png\",\r\n       height = 5.25,\r\n       width = 5,\r\n       units = \"in\",\r\n       dpi = \"retina\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-09-2022-nfl-regular-season-win-luck/nfl_2022_win_luck.png",
    "last_modified": "2023-05-20T23:10:21-04:00",
    "input_file": "2022-nfl-regular-season-win-luck.knit.md"
  },
  {
    "path": "posts/2022-10-15-nflfastr-win-prob-excessmovement/",
    "title": "Analysis of Excess Movement in nflfastR's Win Probability Prediction Streams",
    "description": "Applying the Augenblick and Rabin (2020) Tests for Rational Bayesian Updating.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-10-15",
    "categories": [],
    "contents": "\r\nPacakges Used\r\nI will use the tidyverse, packages from the nflverse, and the\r\nbaseballR package for this post.\r\nBayesian Updating\r\nof Win Probability Streams\r\nWin probability (WP) models are becoming an increasingly component of\r\nsports analysis, including mainstream media and commentary. In the NFL,\r\nwin probability models are at the heart of the 4th down and 2-point\r\nconversion decision analyses that seem to overwhelm discussion of\r\ncoaching quality and fuel a significant amount of Monday morning\r\nquarterbacking.\r\nLike election forecasts, win probabilities are essentially the\r\nexpected value of a zero(loss)-one(win) process. However, a proper\r\nBayesian updating scheme is a martingale: “knowledge of the past will be\r\nof no use in predicting the future” and “the direction of anticipated\r\nfuture swings… should be already baked into the current prediction.”1\r\nThe martingale property implies that a stream of forecasted\r\nprobabilities should not change too much prior to resolution (e.g., end\r\nof game, election result decided). When the outcome is highly uncertain,\r\nthe forecast should be near 50%. Some what counter-intuitively, high\r\nuncertainty about the outcome should also imply low volatility in the\r\nforecast estimate.2\r\nIn 2020, Augenblick and Rabin3 proposed tests that\r\ncompare the movements in a belief stream (e.g., the squared difference\r\nbetween WP forecasts from play to play within an NFL game) to the change\r\nin uncertainty implied by the change in WP forecasts from play to play\r\n(e.g., uncertainty = WP * (1 - WP)).\r\n\r\n\r\nShow code\r\n\r\nx <- seq(0, 1, by = 0.001)\r\nplot_df <- data.frame(x)\r\nggplot(plot_df,\r\n       aes(x = x)) +\r\n  stat_function(fun=function(x) (x * (1-x))) +\r\n  labs(x = \"Win Probability (WP)\",\r\n       y = \"Uncertainty (= WP * (1 - WP))\") +\r\n  theme_light()\r\n\r\n\r\n\r\nThe Augenblick and Rabin tests are agnostic about what the correct\r\nbelief about a process should be. Rather, the tests evaluate the\r\ninformation processing implied by movements in the belief stream: on\r\naverage, the (squared) movement of probability forecasts should be equal\r\nin magnitude to the change in uncertainty implied by the probability\r\nforecasts.\r\nUsing its proposed statistics, the Augenblick and Rabin paper\r\nevaluated the “Bayesianess” of (1) estimates from human forecasters, (2)\r\nFangraphs in-game WP estimates, and (3) “market beliefs” implied by\r\nmarket odds from the prediction market Betfair.\r\nTo summarize their Table 3 results, all three sources exhibited some\r\namount of excess movement (i.e., statistically significant difference\r\nbetween movement and uncertainty reduction). Whereas the probabilities\r\nproduced by individual forecasters were somewhat over-reactive\r\n(normalized excess movement > 1), the algorithmic Fangraphs WP\r\npredictions were somewhat under-reactive (normalized excess movement\r\n< 1).\r\nStatistic\r\nForecasters\r\nFanGraphs WP\r\nBetfair\r\nExcess Movement Z Score\r\n4.22\r\n9.40\r\n19.55\r\nNormalized Excess Movement\r\n1.20\r\n0.931\r\n1.046\r\nEvaluation of nflfastR WP\r\nStreams\r\nHere, I apply some Augenblick and Rabin’s techniques to the\r\nnflfastR’s WP model (specifically, vegas_home_wp). The 2021 season is\r\nreviewed, excluding games that went to overtime (i.e., belief streams\r\nthat are not resolved in regulation). The results do not identify\r\n“excess or insufficient movement of beliefs relative to uncertainty\r\nreduction”, on average, in the vegas_home_wp streams considered.\r\nExcess Movement Statistics\r\n\r\n\r\nShow code\r\n\r\n# Function that calculates WP movement stats for a data.frame containing a single game\r\n# Function expects data.frame with home WP (home_p) and its complement (home_q) each observation\r\nwp_movement_nfl <- function(df){\r\n  for(i in 1:nrow(df)){\r\n    # No movement in WP for first row, so stays NA\r\n    if(i == 1){\r\n      df$unc_reduction[i] = NA_real_\r\n      df$movement[i] = NA_real_\r\n    }\r\n    else {\r\n      df$unc_reduction[i] = (df$home_p[i - 1]*df$home_q[i - 1]) - (df$home_p[i]*df$home_q[i])\r\n      df$movement[i] = (df$home_p[i] - df$home_p[i-1])^2\r\n    }\r\n  }\r\n  \r\n  return(df)\r\n}\r\n\r\nraw_nfl_df <- load_pbp(2021)\r\n\r\n# Find games with plays in OT\r\not_games <- raw_nfl_df %>%\r\n  group_by(game_id, game_half) %>%\r\n  summarize(n = n()) %>%\r\n  filter(game_half == \"Overtime\") %>%\r\n  ungroup() %>%\r\n  pull(game_id)\r\n\r\n\r\nnfl_df <- raw_nfl_df %>%\r\n  # Remove games that went to OT\r\n  filter((game_id %in% ot_games) == F) %>%\r\n  # Create p and q variables that are more intuitive to me\r\n  rename(home_p = vegas_home_wp) %>%\r\n  mutate(home_q = 1 - home_p) %>%\r\n  select(game_id, home_p, home_q) %>%\r\n  # Add variable to calculate movement and uncertainty reduction\r\n  mutate(movement = NA_real_,\r\n         unc_reduction = NA_real_)\r\n\r\n# Nest season by game\r\n# Apply WP movement function to each game\r\n# Unnest back to long data.frame\r\nnfl_season_df <- nfl_df %>%\r\n  group_nest(game_id) %>%\r\n  mutate(movement_df = map(data, wp_movement_nfl)) %>%\r\n  select(game_id, movement_df) %>%\r\n  unnest(cols = movement_df)\r\n\r\n# For convenience, remove rows with NA values in movement statistics\r\nnfl_test_df <- nfl_season_df %>%\r\n  filter(is.na(movement) == F) %>%\r\n  filter(is.na(unc_reduction) == F)\r\n\r\n\r\nFirst, let’s look at a density plot of the excess movement, which\r\nshows the mode near zero.\r\n\r\n\r\nShow code\r\n\r\nnfl_test_df %>%\r\n  ggplot(aes(x = movement - unc_reduction)) +\r\n  geom_density(fill = \"gray\") +\r\n  labs(x = \"Excess Movement\",\r\n       y = \"Density\") +\r\n  theme_light()\r\n\r\n\r\n\r\nThen, we compute summary statistics for average excess movement and\r\nnormalized excess movement. The normalized excess movement suggests that\r\nvegas_home_wp is very slightly under-reactive (i.e., 0.960 slightly less\r\nthan 1).\r\n\r\n\r\nShow code\r\n\r\nnfl_test_df %>%\r\n  summarize(average_excess_movement = mean(movement - unc_reduction),\r\n            normalized_excess_movement = mean(movement) / mean(unc_reduction)) %>%\r\n  print()\r\n\r\n# A tibble: 1 x 2\r\n  average_excess_movement normalized_excess_movement\r\n                    <dbl>                      <dbl>\r\n1              -0.0000463                      0.960\r\n\r\nNext, we test to see if the average excess movement is statistically\r\ndifferent from zero. Unlike the streams tested by Auckenblick and Rabin\r\nin their paper, the average difference between movement and uncertainty\r\nreduction is not statistically different from zero (t = -0.70, p-value =\r\n0.486) for vegas_home_wp.\r\n\r\n\r\nShow code\r\n\r\nt.test(nfl_test_df$movement,\r\n       nfl_test_df$unc_reduction,\r\n       paired = T)\r\n\r\n\r\n    Paired t-test\r\n\r\ndata:  nfl_test_df$movement and nfl_test_df$unc_reduction\r\nt = -0.69684, df = 45779, p-value = 0.4859\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -1.763756e-04  8.385582e-05\r\nsample estimates:\r\nmean of the differences \r\n           -4.62599e-05 \r\n\r\nTotal Movement and\r\nInitial Uncertainty\r\nFinally, we consider the relationship between initial uncertainty (in\r\nthis case, uncertainty determined by vegas line used in the model) and\r\ntotal movement, which in expectation should be equal (Corollary 1 in\r\nAugenblick and Rabin). Following the previous pattern for excess\r\nmovement, we consider a density then perform a t test.\r\n\r\n\r\nShow code\r\n\r\ninitial_uncertainty <- nfl_season_df %>%\r\n  group_by(game_id) %>%\r\n  slice_head(n = 1) %>%\r\n  mutate(init_uncertainty = home_p * home_q) %>%\r\n  select(game_id, init_uncertainty)\r\n\r\ngame_movement <- nfl_season_df %>%\r\n  filter(is.na(movement) == F) %>%\r\n  group_by(game_id) %>%\r\n  summarize(total_movement = sum(movement))\r\n\r\ngame_summary <- left_join(initial_uncertainty,\r\n                          game_movement,\r\n                          by = \"game_id\")\r\n\r\ngame_summary %>%\r\n  ggplot(aes(x = total_movement - init_uncertainty)) +\r\n  geom_density(fill = \"gray\") +\r\n  labs(x = \"Total Movement - Initial Uncertainty\",\r\n       y = \"Density\") +\r\n  theme_light()\r\n\r\n\r\n\r\nWhile the difference between total movement and initial uncertainty\r\nis right skewed, their mean difference is not significantly different\r\nthan 0 (t = -0.82, p-value = 0.412).\r\n\r\n\r\nShow code\r\n\r\nt.test(game_summary$total_movement,\r\n       game_summary$init_uncertainty,\r\n       paired = T)\r\n\r\n\r\n    Paired t-test\r\n\r\ndata:  game_summary$total_movement and game_summary$init_uncertainty\r\nt = -0.82257, df = 261, p-value = 0.4115\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -0.02743286  0.01126661\r\nsample estimates:\r\nmean of the differences \r\n           -0.008083123 \r\n\r\nEvaluation of MLB.com WP\r\nStreams\r\nFor comparison, I perform the same process for the MLB.com\r\nWP model via the baseballr package. For time efficiency in a random\r\nsample of 500 games in the 2021 MLB season.\r\n\r\n\r\nShow code\r\n\r\n# Roberto Clemente: \"Any time you have an opportunity to make a difference\r\n# in this world and you don't, then you are wasting your time on Earth.\"\r\nset.seed(21)\r\n\r\n# This is probably not the best workflow to get this data.\r\n# I'm really not well versed with the baseballr package.\r\nmlb_game_pks <- baseballr::mlb_schedule(2021) %>%\r\n  filter(game_type == \"R\") %>%\r\n  pull(game_pk) %>%\r\n  sample(size = 500)\r\n\r\nraw_mlb_df <- map_df(mlb_game_pks, mlb_game_wp)\r\n\r\nmlb_df <- raw_mlb_df %>%\r\n  rename(home_p = home_team_win_probability,\r\n         home_q = away_team_win_probability) %>%\r\n  mutate(home_p = home_p / 100,\r\n         home_q = home_q / 100) %>%\r\n  select(at_bat_index, home_p, home_q) %>%\r\n  mutate(movement = NA_real_,\r\n         reduction = NA_real_)\r\n\r\nwp_movement_mlb <- function(df){\r\n  for(i in 1:nrow(df)){\r\n    if(df$at_bat_index[i] == 0){\r\n      df$reduction[i] = NA_real_\r\n      df$movement[i] = NA_real_\r\n    }\r\n    else {\r\n      df$reduction[i] = (df$home_p[i - 1]*df$home_q[i - 1]) - (df$home_p[i]*df$home_q[i])\r\n      df$movement[i] = (df$home_p[i] - df$home_p[i-1])^2\r\n    }\r\n  }\r\n  \r\n  return(df)\r\n}\r\n\r\nmlb_excess <- mlb_df %>%\r\n  wp_movement_mlb()\r\n\r\n\r\nExcess Movement Statistics\r\nWe look at the density plot for excess movement, which looks similar\r\nto the plot for nflfastR’s WP model.\r\n\r\n\r\nShow code\r\n\r\nmlb_excess %>%\r\n  filter(is.na(movement) == F) %>%\r\n  ggplot(aes(x = movement - reduction)) +\r\n  geom_density(fill = \"gray\") +\r\n  theme_light() +\r\n  labs(x = \"Excess Movement\",\r\n       y = \"Density\")\r\n\r\n\r\nShow code\r\n\r\nmlb_excess %>%\r\n  summarize(average_excess_movement = mean(movement - reduction, na.rm = TRUE),\r\n            normalized_excess_movement = mean(movement, na.rm = TRUE) / mean(reduction, na.rm = TRUE)) %>%\r\n  print()\r\n\r\n# A tibble: 1 x 2\r\n  average_excess_movement normalized_excess_movement\r\n                    <dbl>                      <dbl>\r\n1              -0.0000213                      0.994\r\n\r\nThe normalized excess movement for the MLB.com WP model is even\r\ncloser to 1. Similar to nflfastR’s WP model, the average excess movement\r\nis not significantly different from 0 (t = -0.164, p-value = 0.87).\r\n\r\n\r\nShow code\r\n\r\nmlb_test_df <- mlb_excess %>%\r\n  filter(is.na(movement) == F) %>%\r\n  filter(is.na(reduction) == F)\r\n\r\nt.test(mlb_test_df$movement,\r\n       mlb_test_df$reduction,\r\n       paired = T)\r\n\r\n\r\n    Paired t-test\r\n\r\ndata:  mlb_test_df$movement and mlb_test_df$reduction\r\nt = -0.16401, df = 36915, p-value = 0.8697\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -0.0002761741  0.0002335249\r\nsample estimates:\r\nmean of the differences \r\n          -2.132457e-05 \r\n\r\nTotal Movement and\r\nInitial Uncertainty\r\nFinally, we consider the relationship between initial uncertainty and\r\ntotal movement for the MLB.com WP model, which in expectation should be\r\nequal.\r\n\r\n\r\nShow code\r\n\r\ngame_index = 0\r\n\r\nmlb_excess <- mlb_excess %>%\r\n  mutate(game_index = NA_integer_)\r\n\r\nfor(i in 1:nrow(mlb_excess)){\r\n  if(mlb_excess$at_bat_index[i] == 0){\r\n    game_index = game_index + 1\r\n  }\r\n  mlb_excess$game_index[i] = game_index\r\n}\r\n\r\nmlb_init_uncertainty <- mlb_excess %>%\r\n  group_by(game_index) %>%\r\n  slice_head(n = 1) %>%\r\n  mutate(init_uncertainty = home_p * home_q) %>%\r\n  select(game_index, init_uncertainty)\r\n\r\nmlb_total_movement <- mlb_excess %>%\r\n  group_by(game_index) %>%\r\n  summarize(total_movement = sum(movement, na.rm = T))\r\n\r\nmlb_total_compare <- left_join(\r\n  mlb_init_uncertainty,\r\n  mlb_total_movement,\r\n  by = \"game_index\"\r\n)\r\n\r\nmlb_total_compare %>%\r\n  ggplot(aes(x = total_movement - init_uncertainty)) +\r\n  geom_density(fill = \"gray\") +\r\n  theme_light() +\r\n  labs(x = \"Total Movement - Initial Uncertainty\",\r\n       y = \"Density\")\r\n\r\n\r\n\r\nAs with the nflfastR model, the difference between total movement and\r\ninitial uncertainty is right skewed, but the mean difference is not\r\nsignificantly different than 0 (t = -0.174, p-value = 0.862).\r\n\r\n\r\nShow code\r\n\r\nt.test(mlb_total_compare$total_movement,\r\n       mlb_total_compare$init_uncertainty,\r\n       paired = T)\r\n\r\n\r\n    Paired t-test\r\n\r\ndata:  mlb_total_compare$total_movement and mlb_total_compare$init_uncertainty\r\nt = -0.17433, df = 499, p-value = 0.8617\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -0.01931849  0.01616961\r\nsample estimates:\r\nmean of the differences \r\n           -0.001574436 \r\n\r\nConclusion\r\nThe papers in the footnotes are really interesting to me because they\r\nillustrate that a WP conveys more than the expected relative frequency\r\nof winning given the current game state. The point estimate also conveys\r\nthe uncertainty of the outcome of the process.\r\nThis blog post applied some of Augenblick and Rabin’s information\r\nprocessing diagnostics for popular WP models. According to these\r\ndiagnostics, both nflfastR’s vegas_home_wp model and MLB.com’s WP model\r\ndemonstrate coherent Bayesian updating, on average. Additionally, both\r\nmodels appear to outperform the model’s evaluated by Augenblick and\r\nRabin in this respect.\r\n\r\nGelman, Hullman, Wlezien, and Morris;\r\n2020. Information,\r\nincentives, and goals in election forecasts.↩︎\r\nTaleb; 2017. Election predictions as\r\nmartingales: An arbitrage approach.↩︎\r\nAugenblick and Rabin; 2020. Belief\r\nMovement, Uncertainty Reduction, & Rational Updating.↩︎\r\n",
    "preview": "posts/2022-10-15-nflfastr-win-prob-excessmovement/nflfastr-win-prob-excessmovement_files/figure-html5/wp_uncertianty_plot-1.png",
    "last_modified": "2022-10-15T22:06:03-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-12-nfl-basic-multilevel-models/",
    "title": "Empirical Bayes Estimates of NFL Offense and Defense Quality",
    "description": "Replacing averages with random intercepts from a basic multilevel model.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-10-12",
    "categories": [],
    "contents": "\r\nPackages Used\r\nI will use the tidyverse, packages from the nflverse, and lme4 for\r\nthe multilevel model.\r\nEmpirical Bayes\r\nEstimates of Yards/Play\r\nYards per play is a basic measure of snap-to-snap team quality. In\r\nPittsburgh, analyst Matt\r\nWilliamson frequently cites yards/play in his, noting that former\r\nSteelers GM Kevin Colbert found great value in the metric. This makes\r\nsense because there are many plays in an NFL game: a true per snap\r\nadvantage is likely to build to a larger advantage over the course of a\r\ngame. The yard/play idea was also likely inspired by The\r\nSuccess Equation: Untangling Skill and Luck in Business, Sports, and\r\nInvesting by Michael J.\r\nMauboussin (Chapter 4), which I read earlier in 2022.\r\nBasic yards/play averages do not fully capture a true advantage. Some\r\nteams play a much weaker slate of opponents. There is sampling error or\r\nluck, good and bad, in different measures. This post hopes to improve on\r\nthe simple arithmetic mean via a very basic multilevel model.\r\nWhile penalty assessment is a bit arbitrary in the NFL, penalty\r\nyardage is earned by poor technique and related to the tactics employed.\r\nTherefore, net penalty yardage is considered just the same as yards\r\ngained or lost conventionally, even if the down is replayed. This is a\r\nbit like preferring plate appearances to at bats in baseball (since at\r\nbats exclude walks, scorer assessed sacrifices, etc.).\r\n\r\n\r\nShow code\r\n\r\npenalty_yards_gained <- function(df){\r\n  df %>%\r\n    mutate(yards_gained = ifelse(play_type_nfl == \"PENALTY\",\r\n                                 ifelse(penalty_team == posteam,\r\n                                        -1 * penalty_yards,\r\n                                        penalty_yards),\r\n                                 yards_gained)) %>%\r\n    return()\r\n}\r\n\r\n\r\nSince they are on the field for most of the snaps in a game, analysis\r\nis confined to the offensive and defensive units. Plays spikes and\r\nkneels, where the purpose of the down is not gain yardage, are\r\ndiscarded.\r\n\r\n\r\nShow code\r\n\r\npbp_df <- nflfastR::load_pbp(2021) %>%\r\n  filter(season_type == \"REG\") %>%\r\n  filter(play_type_nfl %in% c(\"GAME_START\",\r\n                              \"KICK_OFF\",\r\n                              \"PUNT\",\r\n                              \"TIMEOUT\",\r\n                              \"FIELD_GOAL\",\r\n                              \"XP_KICK\",\r\n                              \"END_QUARTER\",\r\n                              \"END_GAME\",\r\n                              \"PAT2\",\r\n                              \"FREE_KICK\",\r\n                              \"COMMENT\") == F) %>%\r\n  filter(is.na(play_type_nfl) == F) %>%\r\n  filter(play_type %in% c(\"qb_kneel\",\r\n                          \"qb_spike\") == F) %>%\r\n  filter(grepl(\"(Punt formation)\", desc) == F) %>%\r\n  filter(grepl(\", offsetting.\", desc) == F) %>%\r\n  penalty_yards_gained()\r\n\r\n\r\nThe Advantages of a\r\nMultilevel Approach\r\n\r\n“… multilevel regression deserves to be the default form of\r\nregression. Papers that do not use multilevel models should have to\r\njustify not using a multilevel approach.” - McElreath, Statistical\r\nRethinking, pg. 15\r\n\r\nMultilevel models have a number of advantages over simpler regression\r\nmodels. Random intercepts models are the simplest multilevel models and\r\nare generally used as a baseline for which to compare more complicated\r\nmultilevel models against. Here, a random intercepts model is proposed\r\nas an improvement on averaging.\r\nRealistic Assumptions\r\nAbout Independence\r\nImportantly, multilevel models make more realistic assumptions about\r\nthe independence of observations in a data set. Context rightly shapes\r\nthe meaning we derive from statistics. For example, gaining 5 yards/play\r\nagainst the NFL’s best defense reflects more positively on an offense\r\nthan gaining 5 yards/play against the league’s worst defense. For some\r\ngames, weather conditions severely depress offense (think the\r\nBills/Patriots game in the 2021 regular season). Averages and simpler\r\nregression models, however, do not recognize any such context; all\r\nobservations are assumed to be independent.\r\nClustering is expected in yards/play data, along a number of\r\ndifferent dimensions. Consider the following examples.\r\nTwo offenses playing in inclement weather are likely to be more\r\nsimilar to one another for that game than the other NFL offenses playing\r\nin more typical weather conditions that week.\r\nOffenses in 2022 are likely to be more similar to the one another\r\nthan they are to offenses in 1995.\r\nMultilevel models acknowledge and take advantage of these\r\ndependencies in the structure of the data.\r\nParitial Pooling and\r\nRegularization\r\nThe observations in one cluster of our data are also informative for\r\nother clusters. This may be less intuitive, but consider the\r\nfollowing.\r\nIf I asked you how many yards/play the Dallas Cowboys offense\r\nproduced in 2021, your guess would probably improve if I told you the\r\nanswer for the Carolina Panthers.\r\nIf I asked you how many yards/play the Green Bay Packers defense\r\nsurrendered in 2021, your guess would probably improve if I told you how\r\nmany yards/play in all NFL games played that season.\r\nSharing of information between the results of all plays across the\r\nleague, regardless of cluster, and the individual clusters in the data\r\nprovides regularized estimates. The regularization (or shrinkage)\r\nperformed by multilevel models tempers the conclusions drawn in each\r\ncluster of the data by considering all clusters, thus minimizing the\r\nadverse affects of overfitting (roughly, taking the noise in the sample\r\ntoo seriously). Regularization can thus improve out-of-sample\r\nprediction.\r\n\r\nComplete pooling -> predict grand mean for all teams -> maximum\r\nunderfitting  No pooling -> predict group mean for each team\r\n-> maximum overfitting  Partial pooling -> predict random\r\nintercept for each team -> a principled balance.\r\n\r\nReaders of the Football Outsiders may be familiar with the Plexiglass\r\nPrinciple, which roughly states that a big improvement (or decline) in\r\nperformance will on average be followed by a relapse (or bounce back).\r\nThe Plexiglass Principle was coined by the legendary baseball\r\nsabermetrician Bill James, who later renamed it the Whirlpool Principle:\r\n“All teams are drawn forcefully toward the center. Most of the teams\r\nwhich had winning records in 1982 will decline in 1983; most of the\r\nteams which had losing records in 1982 will improve in 1983.” 1\r\nTraditional statisticians call this the Paradox of Regression to the\r\nMean. Exceptionally good performance is likely the result of a\r\ncombination of high ability and good luck; subsequent performance is\r\nlikely to be better than average but not as good as the previous\r\nexceptional outcome. 2\r\nPartial Pooling: An\r\nIntuitive Approach\r\nWe will use Empirical Bayes estimates, or shrinkage estimates, to\r\nregularize NFL offensive and defensive yards/play averages.3\r\nHow much shrinkage is appropriate? The multilevel model will\r\ndetermine the strength of the whirlpool.\r\nWe begin by building a league-wide normal distribution centered on\r\nthe mean yards gained for all observations in the data set. The standard\r\ndeviation of this distribution is the standard error of the league-wide\r\nmean, assuming n = 1125 (a nominal number of plays for a team’s\r\noffensive or defensive unit that is representative for the 2021 season)\r\nrather than the total number of plays in the regular season. We’ll plot\r\nthis curve as a league-wide nominal distribution, since its parameters\r\nare estimated considering all plays in the 2021 NFL regular season.\r\nNext, we plot the average yards/play calculated for each offense and\r\ndefense under the league-wide nominal distribution. This allows us to\r\ncompare the distribution of offensive unit averages and the distribution\r\nof the defensive unit averages against all plays.\r\n\r\n\r\nShow code\r\n\r\n# Estimate NFL Distribution using mean for all plays...\r\navg_yards_per_play <- pbp_df %>%\r\n  summarize(mean = mean(yards_gained)) %>%\r\n  pull(mean)\r\n\r\n# and standard error of the mean, which assumes\r\n# 1125 plays in a season for a unit.\r\nsd_yards_per_play <- pbp_df %>%\r\n  summarize(sd = sd(yards_gained)) %>%\r\n  pull(sd)\r\nse_unit <- sd_yards_per_play/sqrt(1125)\r\n\r\n# Build data frame to plot estimated distribution\r\nypp <- seq(0, 10, by = 0.01)\r\nd_ypp <- dnorm(ypp, mean = avg_yards_per_play, sd = se_unit)\r\ndf_ypp <- data.frame(ypp, d_ypp)\r\nrm(ypp, d_ypp)\r\n\r\n# Calculate (Raw, Unadjusted) Unit Averages\r\noff_avg_df <- pbp_df %>%\r\n  group_by(posteam) %>%\r\n  summarize(off_yards_per_play = mean(yards_gained),\r\n            off_n = n()) %>%\r\n  rename(team = posteam)\r\ndef_avg_df <- pbp_df %>%\r\n  group_by(defteam) %>%\r\n  summarize(def_yards_per_play = mean(yards_gained),\r\n            def_n = n()) %>%\r\n  rename(team = defteam)\r\n\r\n\r\n# Plot unit averages against league-wide estimated distribution\r\n# Stack of plots will show much greater spread among offenses\r\n# compared to defenses.\r\np_top <- df_ypp %>%\r\n  ggplot(aes(x = ypp,\r\n             y = d_ypp)) +\r\n  geom_vline(xintercept = c(avg_yards_per_play,\r\n                            avg_yards_per_play - 2*se_unit,\r\n                            avg_yards_per_play + 2*se_unit),\r\n             color = \"dark gray\",\r\n             linetype = \"dashed\") +\r\n  geom_line() +\r\n  geom_point(data = off_avg_df,\r\n             aes(x = off_yards_per_play,\r\n                 y = 0,\r\n                 color = team)) +\r\n  scale_color_nfl() +\r\n  theme_light() +\r\n  scale_x_continuous(minor_breaks = NULL) +\r\n  scale_y_continuous(minor_breaks = NULL) +\r\n  coord_cartesian(xlim = c(4.0, 6.25)) +\r\n  labs(x = \"Offensive Yards Per Play\",\r\n       y = \"Density\",\r\n       title = \"Team Unit Averages Compared to NFL Distribution\",\r\n       subtitle = \"NFL Standard Error Assumes 1125 Plays for Unit\")\r\np_bottom <- df_ypp %>%\r\n  ggplot(aes(x = ypp,\r\n             y = d_ypp)) +\r\n  geom_vline(xintercept = c(avg_yards_per_play,\r\n                            avg_yards_per_play - 2*se_unit,\r\n                            avg_yards_per_play + 2*se_unit),\r\n             color = \"dark gray\",\r\n             linetype = \"dashed\") +\r\n  geom_line() +\r\n  geom_point(data = def_avg_df,\r\n             aes(x = def_yards_per_play,\r\n                 y = 0,\r\n                 color = team)) +\r\n  scale_color_nfl() +\r\n  theme_light() +\r\n  scale_x_continuous(minor_breaks = NULL) +\r\n  scale_y_continuous(minor_breaks = NULL) +\r\n  coord_cartesian(xlim = c(4.0, 6.25)) +\r\n  labs(x = \"Defensive Yards Per Play\",\r\n       y = \"Density\")\r\nlibrary(patchwork)\r\np_units <- p_top / p_bottom\r\nggsave(\"nfl_2021_unit_dist.png\",\r\n       plot = p_units,\r\n       height = 5.25,\r\n       width = 5,\r\n       units = \"in\",\r\n       dpi = \"retina\")\r\n\r\n\r\n\r\nA few observations:\r\nThe defensive average are much more closely packed around the league\r\nwide mean than the offensive averages. This indicates that the offense\r\nidentity more reliably accounts for variation in the data than defense\r\nidentity. Therefore, we expect our model shrink offensive deviations\r\nfrom average less than defensive deviations.\r\nLooking at the offenses, about 9 or 10 offenses (of 32) are greater\r\nthan 2 standard errors from the league wide mean (i.e., outside of gray\r\ndashed lines). Since the standard errors should be similar, about 2\r\nwould be expected (0.05 * 32 = about 2). This suggests that the offense\r\naverages are too greatly dispersed, so some modest shrinkage is\r\nexpected.\r\nSince each unit will have run a similar number of plays, the\r\nstandard error of the estimates of the offensive/defensive unit averages\r\nshould be similar. Therefore, shrinkage should mostly be dominated by\r\ndeviations from league average rather than differences in sample size\r\n(e.g., above average performance by some metric of a backup QB would be\r\nshrunk more than similar performance for a full time QB because the\r\nbackup much smaller sample size).\r\nLet’s look at the actual shrinkage in the Empirical Bayes\r\nestimate.\r\n\r\n\r\nShow code\r\n\r\nmod_rand_int <- lmer(yards_gained ~ 1 + (1|posteam) + (1|defteam) + (1|game_id),\r\n                     data = pbp_df)\r\n\r\n\r\nTo visualize the “whirlpool” effect of the multilevel model, we build\r\na data frame with the unadjusted unit averages and their Empirical Bayes\r\n(random intercepts) estimates.\r\n\r\n\r\nShow code\r\n\r\noff_rand_int <- coef(mod_rand_int)$posteam %>%\r\n  rownames_to_column(var = \"team\") %>%\r\n  rename(off_estimate = `(Intercept)`)\r\n\r\noff_avg_df <- off_avg_df %>%\r\n  left_join(off_rand_int,\r\n            by = \"team\") %>%\r\n  mutate(off_estimate_adjust = off_estimate - off_yards_per_play)\r\n\r\n\r\ndef_rand_int <- coef(mod_rand_int)$defteam %>%\r\n  rownames_to_column(var = \"team\") %>%\r\n  rename(def_estimate = `(Intercept)`)\r\n\r\ndef_avg_df <- def_avg_df %>%\r\n  left_join(def_rand_int,\r\n            by = \"team\") %>%\r\n  mutate(def_estimate_adjust = def_estimate - def_yards_per_play)\r\n\r\nest_df <- left_join(off_avg_df,\r\n                    def_avg_df,\r\n                    by = \"team\") %>%\r\n  mutate(combined_estimate = off_estimate - def_estimate) %>%\r\n  arrange(desc(combined_estimate)) %>%\r\n  mutate(estimate_rank = row_number(),\r\n         percentile_estimate = scale(combined_estimate)) %>%\r\n  mutate(percentile_estimate = pnorm(percentile_estimate))\r\n\r\nrm(off_avg_df,\r\n   def_avg_df)\r\n\r\n\r\nThen, we plot the unadjusted unit averages and their regularized\r\nestimate to show the shrinkage.\r\n\r\n\r\nShow code\r\n\r\np_pooling <- est_df %>%\r\n  ggplot(aes(x = off_yards_per_play,\r\n             y = def_yards_per_play)) +\r\n  geom_hline(yintercept = avg_yards_per_play) +\r\n  geom_vline(xintercept = avg_yards_per_play) +\r\n  geom_point(aes(color = team)) +\r\n  geom_segment(aes(xend = off_estimate,\r\n                   yend = def_estimate,\r\n                   color = team)) +\r\n  geom_nfl_logos(aes(x = off_estimate,\r\n                     y = def_estimate,\r\n                     team_abbr = team),\r\n                 width = 0.05) +\r\n  scale_y_reverse(breaks = seq(4.0, 6.0, by = 0.2),\r\n                  minor_breaks = NULL) +\r\n  scale_x_continuous(breaks = seq(4.0, 6.0, by = 0.2),\r\n                     minor_breaks = NULL) +\r\n  scale_color_nfl() +\r\n  labs(x = \"Offensive Yards Per Play\",\r\n       y = \"Defensive Yards Per Play\",\r\n       title = \"Adjusted Yards Per Play\",\r\n       subtitle = \"The Plexiglass Principle Adaptively Applied Via Partial Pooling\",\r\n       caption = \"Data: nflfastR | Plot: nflplotR | Model: lme4\") +\r\n  theme_light() +\r\n  coord_fixed()\r\n\r\nggsave(\"nfl_2021_yards_per_play_pooling.png\",\r\n       plot = p_pooling,\r\n       units = \"in\",\r\n       height = 5.25,\r\n       width = 5,\r\n       dpi = \"retina\")\r\n\r\n\r\n\r\nTo better quantify the average shrinkage applied to deviations from\r\naverage for offenses, we can fit an ordinary least squares regression to\r\na centered data frame (i.e., the league average is subtracted from both\r\nthe regularized estimates and the unadjusted averages).\r\n\r\n\r\nShow code\r\n\r\ncentered_df <- est_df %>%\r\n  mutate(off_estimate = off_estimate - avg_yards_per_play,\r\n         off_yards_per_play = off_yards_per_play - avg_yards_per_play,\r\n         def_estimate = def_estimate - avg_yards_per_play,\r\n         def_yards_per_play = def_yards_per_play - avg_yards_per_play)\r\n\r\nlm(off_estimate ~ off_yards_per_play,\r\n   data = centered_df) %>%\r\n  coefficients()\r\n\r\n       (Intercept) off_yards_per_play \r\n      -0.001170907        0.666408276 \r\n\r\nShow code\r\n\r\nest_df %>%\r\n  ggplot(aes(x = off_yards_per_play - avg_yards_per_play,\r\n             y = off_estimate - avg_yards_per_play)) +\r\n  geom_smooth(method = \"lm\",\r\n              se = FALSE) +\r\n  geom_text(aes(color = team,\r\n                label = team)) +\r\n  scale_color_nfl() +\r\n  scale_x_continuous(breaks = seq(-10, 10, by = 0.2),\r\n                     minor_breaks = NULL) +\r\n  scale_y_continuous(breaks = seq(-10, 10, by = 0.2),\r\n                     minor_breaks = NULL) +\r\n  theme_light() +\r\n  coord_fixed(xlim = c(-0.9, 0.9),\r\n              ylim = c(-0.8, 0.8))\r\n\r\n\r\n\r\nWe repeat for the defensive units. As anticipated, more shrinkage is\r\napplied to the defensive units (i.e., the slope of the defensive line is\r\ncloser to 0 than the offensive line).\r\n\r\n\r\nShow code\r\n\r\nlm(def_estimate ~ def_yards_per_play,\r\n   data = centered_df) %>%\r\n  coefficients()\r\n\r\n       (Intercept) def_yards_per_play \r\n      -0.001126752        0.309755311 \r\n\r\nShow code\r\n\r\nest_df %>%\r\n  ggplot(aes(x = def_yards_per_play - avg_yards_per_play,\r\n             y = def_estimate - avg_yards_per_play)) +\r\n  geom_smooth(method = \"lm\",\r\n              se = FALSE) +\r\n  geom_text(aes(color = team,\r\n                label = team)) +\r\n  scale_color_nfl() +\r\n  scale_x_continuous(breaks = seq(-10, 10, by = 0.2),\r\n                     minor_breaks = NULL) +\r\n  scale_y_continuous(breaks = seq(-10, 10, by = 0.2),\r\n                     minor_breaks = NULL) +\r\n  theme_light() +\r\n  coord_fixed(xlim = c(-0.9, 0.9),\r\n              ylim = c(-0.8, 0.8))\r\n\r\n\r\n\r\nA Point Estimate of Team\r\nQuality\r\nAs our point estimate of team quality, we consider adjusted yard/play\r\ndifferential (i.e., offense yards/play regularized estimate - defense\r\nyards/play regularized estimate). To more easily visualize the relative\r\nspread in team quality by this metric, we plot the results in terms of\r\npercentiles.\r\n\r\n\r\nShow code\r\n\r\np_2021 <- est_df %>%\r\n  ggplot(aes(x = percentile_estimate,\r\n             y = estimate_rank)) +\r\n  geom_hline(yintercept = c(0.5, 8.5, 16.5, 24.5, 32.5),\r\n             color = \"dark gray\",\r\n             linetype = \"dashed\") +\r\n  geom_vline(xintercept = c(0, 0.25, 0.5, 0.75, 1),\r\n             color = \"dark gray\") +\r\n  geom_nfl_logos(aes(team_abbr = team),\r\n                 width = 0.05) +\r\n  scale_x_continuous(minor_breaks = NULL,\r\n                     labels = scales::percent) +\r\n  scale_y_reverse(breaks = NULL,\r\n                  minor_breaks = NULL) +\r\n  scale_color_nfl() +\r\n  theme_light() +\r\n  labs(title = \"2021 NFL Regular Season Per Play Rankings\",\r\n       subtitle = \"Excludes Special Teams/Kneels/Spikes; Includes Penalty Yardage\",\r\n       x = \"Adjusted Yards Per Play Differential: Percentile\",\r\n       y = NULL,\r\n       caption = \"Data: nflfastR | Plot: nflplotR | Model: lme4\")\r\n\r\nggsave(\"nfl_2021_reg_season_per_play.png\",\r\n       plot = p_2021,\r\n       units = \"in\",\r\n       height = 5.25,\r\n       width = 5,\r\n       dpi = \"retina\")\r\n\r\n\r\n\r\nQuick\r\nBack Test: Adjusted Yard/Play Differential as a Predictor of Playoff\r\nSuccess\r\nAs a quick back test, we compare how predictive regular season\r\nadjusted yard/play differential was for the 2021 NFL playoffs. The Vegas\r\nspreads below4 show the winning team was favored in\r\n8 of 13 games. In this small sample, the adjusted yard/play differential\r\nmodel preferred the winning team in 12 of 13 games. This is not a\r\nrigorous test by any means, but it demonstrates the model likely carries\r\nsome genuine insight.\r\nAway\r\nHome\r\nResult\r\nVegas Spread\r\nModel\r\nCIN\r\nLV\r\nLV 19, CIN 26\r\nCIN -5.5 (Hit)\r\nCIN (Hit)\r\nNE\r\nBUF\r\nNE 17, BUF 47\r\nBUF -4 (Hit)\r\nBUF (Hit)\r\nPHI\r\nTB\r\nPHI 15, TB 31\r\nTB -8.5 (Hit)\r\nTB (Hit)\r\nSF\r\nDAL\r\nSF 23, DAL 17\r\nDAL -3 (Miss)\r\nSF (Hit)\r\nPIT\r\nKC\r\nPIT 21, KC 42\r\nKC -12.5 (Hit)\r\nKC (Hit)\r\nARI\r\nLAR\r\nARI 11, LAR 34\r\nLAR -3.5 (Hit)\r\nLAR (Hit)\r\nCIN\r\nTEN\r\nCIN 19, TEN 16\r\nTEN -4 (Miss)\r\nCIN (Hit)\r\nSF\r\nGB\r\nSF 13, GB 10\r\nGB -5.5 (Miss)\r\nSF (Hit)\r\nLAR\r\nTB\r\nLAR 30, TB 27\r\nTB -3 (Miss)\r\nLAR (Hit)\r\nBUF\r\nKC\r\nBUF 36, KC 42\r\nKC -2 (Hit)\r\nBUF (Miss)\r\nCIN\r\nKC\r\nCIN 27, KC 24\r\nKC -7 (Miss)\r\nCIN (Hit)\r\nSF\r\nLAR\r\nSF 17, LAR 20\r\nLAR -3.5 (Hit)\r\nLAR (Hit)\r\nLAR\r\nCIN\r\nLAR 23, CIN 20\r\nLAR -4.5 (Hit)\r\nLAR (Hit)\r\n\r\nQuotation and background repeated\r\nfrom this excellent\r\nblog post.↩︎\r\nSee Section 6.5 in Regression and Other\r\nStories by Gelman, Hill, and Vehtari, from which this summary of\r\nregression to the mean is adapted.↩︎\r\nSee Section 8.7.1 of Beyond\r\nMultiple Linear Regression by Roback and Legler or this NBER seminar\r\non Empirical Bayes\r\nmethods.↩︎\r\nRetrieved from Sports Odds\r\nHistory.com.↩︎\r\n",
    "preview": "posts/2022-09-12-nfl-basic-multilevel-models/nfl_2021_yards_per_play_pooling.png",
    "last_modified": "2022-10-12T00:37:29-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-17-time-average-qb-effects/",
    "title": "QB Quality and Time Average Evaluations",
    "description": "Incoprporating Mike Sando's QB Tiers with Time Averages to Predict Win/Loss Outcomes.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-04-21",
    "categories": [],
    "contents": "\r\nIntro\r\nPast posts have explored uses of NFL time average leads as a single-point summary of overall game performance.\r\nUsing a simple logistic regression model, I looked at 2021 regular season win luck. Notably, the “luckiest teams” had veteran/high quality QBs.\r\n\r\nMy intuition: quality QBs perform well in comeback and high leverage situations. The residuals from the single feature model reflect that the direct effect of QBs is significant.\r\nHere, I expand the model to include QB quality, using an approach inspired by Mike Sando’s QB Tiers.\r\nMike Sando’s QB Tiers\r\nMike Sando is probably my favorite national NFL writer. He’s one of a few writers that make me a subscriber to the Athletic. His recent tribute to the late John Clayton was tremendous.\r\nEach summer since 2014, Sando has released his QB Tier article, where several NFL team personnel evaluate the league’s QBs using a simple rubric. From the 2021 edition:\r\n\r\nTIER 1\r\nA Tier 1 quarterback can carry his team each week. The team wins because of him. He expertly handles pure passing situations. He has no real holes in his game…\r\nTIER 2\r\nA Tier 2 quarterback can carry his team sometimes but not as consistently. He can handle pure passing situations in doses and/or possesses other dimensions that are special enough to elevate him above Tier 3. He has a hole or two in his game…\r\nTIER 3\r\nA Tier 3 quarterback is a legitimate starter but needs a heavier running game and/or defensive component to win. A low-volume dropback passing offense suits him best…\r\nTIER 4\r\nA Tier 4 quarterback could be an unproven player with some upside or a veteran who is ultimately best suited as a backup…\r\n— Mike Sando\r\n\r\nI really like the tier approach: it aggregates subject matter expert opinion in a straightforward, meaningful way without requiring a false level of precision.\r\nTime Average Lead + QB Quality Model\r\nA summary of the model, fit on regular season games that occured between 2013 and 2020.\r\nIn addition to the game’s time average lead, a team’s Sando QB Tier advantage is included as a feature in a logistic regression.\r\nI’ve assumed that the following year’s QB Tier is a better indicator of QB quality for a season than their preseason QB Tier.\r\nThis has the tradeoff of reducing the size of the data set.\r\nIf one of the QBs is not evaluated the next season (e.g., retires), that game is not included in the data set.\r\n\r\n\r\nFor each game, the QB tier rating for QB with the most snap counts played for each team is used.\r\nFor an unranked QB that was the planned opening day starter, I assumed a QB rank of 4.0.\r\nFor an unranked QB that was not a planned opening day starter, I assumed a QB rank of 4.5.\r\nModel Summary\r\nA summary of the coefficients and standard error.\r\nTime Average Lead: 0.306\r\nStandard Error: 0.018\r\n\r\nQB Tier Advantage: 0.337\r\nStandard Error: 0.07\r\n\r\n\r\n\r\n\r\nEnd of Season 2021 Tier Rankings\r\nUsing (1) the logistic model above and (2) some estimates I made of the Sando QB Tier grade of the QBs that finished the season (i.e., not L. Jackson for BAL, Big Ben rated Tier 3.5 vice 2.64 from 2021 grades), I made a plot of an end of season power ranking based on a team’s season time average lead and QB quality.\r\nThe contour lines represent the model’s expected win probability (“exp_wp” in table) against an average team with an average QB. From the plot, think end of season Cleveland Browns led by an injury-plagued Baker Mayfield as roughly representing the baseline.\r\n\r\n\r\nrank\r\n\r\n\r\nteam\r\n\r\n\r\ntime_avg_lead\r\n\r\n\r\nqb_adv\r\n\r\n\r\nexp_wp\r\n\r\n\r\n1\r\n\r\n\r\nBUF\r\n\r\n\r\n5.1\r\n\r\n\r\n1.7\r\n\r\n\r\n89.5\r\n\r\n\r\n2\r\n\r\n\r\nKC\r\n\r\n\r\n4.4\r\n\r\n\r\n2.0\r\n\r\n\r\n88.2\r\n\r\n\r\n3\r\n\r\n\r\nTB\r\n\r\n\r\n4.2\r\n\r\n\r\n1.7\r\n\r\n\r\n86.6\r\n\r\n\r\n4\r\n\r\n\r\nDAL\r\n\r\n\r\n4.8\r\n\r\n\r\n0.7\r\n\r\n\r\n84.7\r\n\r\n\r\n5\r\n\r\n\r\nIND\r\n\r\n\r\n4.6\r\n\r\n\r\n-0.3\r\n\r\n\r\n78.9\r\n\r\n\r\n6\r\n\r\n\r\nGB\r\n\r\n\r\n2.0\r\n\r\n\r\n2.0\r\n\r\n\r\n78.3\r\n\r\n\r\n7\r\n\r\n\r\nLA\r\n\r\n\r\n2.8\r\n\r\n\r\n1.0\r\n\r\n\r\n76.6\r\n\r\n\r\n8\r\n\r\n\r\nARI\r\n\r\n\r\n3.3\r\n\r\n\r\n0.5\r\n\r\n\r\n76.4\r\n\r\n\r\n9\r\n\r\n\r\nSEA\r\n\r\n\r\n1.7\r\n\r\n\r\n1.2\r\n\r\n\r\n71.8\r\n\r\n\r\n10\r\n\r\n\r\nTEN\r\n\r\n\r\n2.4\r\n\r\n\r\n0.5\r\n\r\n\r\n71.0\r\n\r\n\r\n11\r\n\r\n\r\nNE\r\n\r\n\r\n3.1\r\n\r\n\r\n-0.8\r\n\r\n\r\n66.6\r\n\r\n\r\n12\r\n\r\n\r\nLAC\r\n\r\n\r\n1.1\r\n\r\n\r\n1.0\r\n\r\n\r\n66.1\r\n\r\n\r\n13\r\n\r\n\r\nMIN\r\n\r\n\r\n1.7\r\n\r\n\r\n0.2\r\n\r\n\r\n64.5\r\n\r\n\r\n14\r\n\r\n\r\nSF\r\n\r\n\r\n1.7\r\n\r\n\r\n0.0\r\n\r\n\r\n62.6\r\n\r\n\r\n15\r\n\r\n\r\nCIN\r\n\r\n\r\n-0.1\r\n\r\n\r\n0.7\r\n\r\n\r\n55.4\r\n\r\n\r\n16\r\n\r\n\r\nCLE\r\n\r\n\r\n0.8\r\n\r\n\r\n-0.5\r\n\r\n\r\n51.8\r\n\r\n\r\n17\r\n\r\n\r\nLV\r\n\r\n\r\n-1.4\r\n\r\n\r\n0.5\r\n\r\n\r\n43.4\r\n\r\n\r\n18\r\n\r\n\r\nPHI\r\n\r\n\r\n-0.7\r\n\r\n\r\n-0.5\r\n\r\n\r\n40.4\r\n\r\n\r\n19\r\n\r\n\r\nBAL\r\n\r\n\r\n-0.4\r\n\r\n\r\n-1.0\r\n\r\n\r\n38.6\r\n\r\n\r\n20\r\n\r\n\r\nNO\r\n\r\n\r\n-0.4\r\n\r\n\r\n-1.3\r\n\r\n\r\n36.6\r\n\r\n\r\n21\r\n\r\n\r\nMIA\r\n\r\n\r\n-0.8\r\n\r\n\r\n-1.0\r\n\r\n\r\n35.7\r\n\r\n\r\n22\r\n\r\n\r\nCAR\r\n\r\n\r\n-0.8\r\n\r\n\r\n-1.3\r\n\r\n\r\n33.8\r\n\r\n\r\n23\r\n\r\n\r\nDEN\r\n\r\n\r\n-2.0\r\n\r\n\r\n-0.8\r\n\r\n\r\n29.5\r\n\r\n\r\n24\r\n\r\n\r\nCHI\r\n\r\n\r\n-2.2\r\n\r\n\r\n-0.8\r\n\r\n\r\n28.3\r\n\r\n\r\n25\r\n\r\n\r\nATL\r\n\r\n\r\n-3.5\r\n\r\n\r\n0.2\r\n\r\n\r\n27.1\r\n\r\n\r\n26\r\n\r\n\r\nPIT\r\n\r\n\r\n-3.1\r\n\r\n\r\n-0.5\r\n\r\n\r\n24.6\r\n\r\n\r\n27\r\n\r\n\r\nWAS\r\n\r\n\r\n-2.9\r\n\r\n\r\n-0.8\r\n\r\n\r\n24.1\r\n\r\n\r\n28\r\n\r\n\r\nHOU\r\n\r\n\r\n-2.8\r\n\r\n\r\n-1.0\r\n\r\n\r\n23.2\r\n\r\n\r\n29\r\n\r\n\r\nDET\r\n\r\n\r\n-4.9\r\n\r\n\r\n-0.3\r\n\r\n\r\n17.0\r\n\r\n\r\n30\r\n\r\n\r\nNYG\r\n\r\n\r\n-5.0\r\n\r\n\r\n-1.5\r\n\r\n\r\n11.5\r\n\r\n\r\n31\r\n\r\n\r\nNYJ\r\n\r\n\r\n-6.2\r\n\r\n\r\n-1.0\r\n\r\n\r\n9.6\r\n\r\n\r\n32\r\n\r\n\r\nJAX\r\n\r\n\r\n-6.6\r\n\r\n\r\n-0.8\r\n\r\n\r\n9.3\r\n\r\n\r\nQuick Backtest on the 2021 Season\r\nModel accuracy for 2021 regular season games, using\r\n2020 end of season time average lead data\r\n2021 preseason Sando QB Tiers.\r\n\r\n\r\nAccuracy\r\n\r\n\r\n62.1\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-17-time-average-qb-effects/time-average-qb-effects_files/figure-html5/model_plots-1.png",
    "last_modified": "2022-04-30T23:32:18-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-10-2015-to-2021-nfl-regular-season-time-average-win-totals/",
    "title": "NFL Time Average Win Total Estimates: 2015 to 2021",
    "description": "Comparing season win totals to projections based on observed individual game time average lead.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-03-10",
    "categories": [],
    "contents": "\r\nMethod\r\nIn previous posts, I have looked explored the concept of the time average lead for NFL football games.\r\nHere, I make simple simulation-based plots that estimate where actual win totals\r\nFor a given team in a given season,\r\nCalculate the time average lead for each regular season game.\r\nGiven a team’s time average lead, estimate expected win percentage for each game (based on previous logistic regression).\r\nSimulate 10k seasons (each game a Bernoulli trial with p = expected win percentage from Item 2 above; no ties).\r\nPlot actual win total against 5th, 25th, 50th, 75th, and 95th percentiles of win total from the 10k simulated seasons (ties counted as 0.5 a win for actual win totals)\r\nExample: 2021 Pittsburgh Steelers\r\n\r\n\r\n\r\nTo start, I use a function in my hacky package nfltools to calculate the time average lead for all 2021 Steelers games.\r\nBased on the time average lead, an expected win probability is calculated for each week using the following equation.\r\n\\[E[Win Prob|Time Avg Lead] = \\frac{e^{0.31(Time Avg Lead)}}{1 + e^{0.31(Time Avg Lead)}}\\]\r\nThe weekly results are presented in the following table.\r\n\r\n\r\nWeek\r\n\r\n\r\nTeam\r\n\r\n\r\nOpponent\r\n\r\n\r\nTime Avg Lead\r\n\r\n\r\nE[Win Prob|Time Avg Lead]\r\n\r\n\r\n1\r\n\r\n\r\nPIT\r\n\r\n\r\n@ BUF\r\n\r\n\r\n-2.1\r\n\r\n\r\n34.8\r\n\r\n\r\n2\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. LV\r\n\r\n\r\n-3.6\r\n\r\n\r\n24.9\r\n\r\n\r\n3\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. CIN\r\n\r\n\r\n-8.8\r\n\r\n\r\n6.3\r\n\r\n\r\n4\r\n\r\n\r\nPIT\r\n\r\n\r\n@ GB\r\n\r\n\r\n-6.2\r\n\r\n\r\n13.1\r\n\r\n\r\n5\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. DEN\r\n\r\n\r\n8.8\r\n\r\n\r\n90.5\r\n\r\n\r\n6\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. SEA\r\n\r\n\r\n3.7\r\n\r\n\r\n75.0\r\n\r\n\r\n8\r\n\r\n\r\nPIT\r\n\r\n\r\n@ CLE\r\n\r\n\r\n-0.6\r\n\r\n\r\n45.6\r\n\r\n\r\n9\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. CHI\r\n\r\n\r\n8.6\r\n\r\n\r\n90.2\r\n\r\n\r\n10\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. DET\r\n\r\n\r\n0.6\r\n\r\n\r\n54.7\r\n\r\n\r\n11\r\n\r\n\r\nPIT\r\n\r\n\r\n@ LAC\r\n\r\n\r\n-6.7\r\n\r\n\r\n11.5\r\n\r\n\r\n12\r\n\r\n\r\nPIT\r\n\r\n\r\n@ CIN\r\n\r\n\r\n-21.2\r\n\r\n\r\n0.2\r\n\r\n\r\n13\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. BAL\r\n\r\n\r\n-2.7\r\n\r\n\r\n30.6\r\n\r\n\r\n14\r\n\r\n\r\nPIT\r\n\r\n\r\n@ MIN\r\n\r\n\r\n-13.6\r\n\r\n\r\n1.5\r\n\r\n\r\n15\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. TEN\r\n\r\n\r\n-4.6\r\n\r\n\r\n20.0\r\n\r\n\r\n16\r\n\r\n\r\nPIT\r\n\r\n\r\n@ KC\r\n\r\n\r\n-18.9\r\n\r\n\r\n0.3\r\n\r\n\r\n17\r\n\r\n\r\nPIT\r\n\r\n\r\nvs. CLE\r\n\r\n\r\n6.5\r\n\r\n\r\n85.8\r\n\r\n\r\n18\r\n\r\n\r\nPIT\r\n\r\n\r\n@ BAL\r\n\r\n\r\n-0.8\r\n\r\n\r\n44.3\r\n\r\n\r\nGiven these weekly expected win probabilities, 10k seasons are simulated. Here’s the distribution of season win totals for the 10k simulations.\r\nThe red lines represent the quantiles used to summarize the distribution (i.e., 5th, 25th, 50th, 75th, and 95th quantiles).\r\n\r\nThis information gets collapsed into a single row of the league-wide plots.\r\nThe red line represent 0.500 winning percentage (i.e., 8.5 wins for 2021).\r\nThe interval extends from the 5th quantile to the 95th quantile.\r\nBars (i.e., “|”) are provided to mark the 25th and 75th quantiles.\r\nThe team logo is placed at the 50th quantile (i.e., median of simulations).\r\nAn “X” marks the actual win totals (where ties are counted as a 0.5 wins to reflect impact on standings).\r\n\r\nThe “X” at “9.5” wins here shows the Steelers 9 wins matched the 95th percentile outcome predicted by the model. It is unlikely to expect similar play, as measured by the time average lead, over the course of a season to result in 9 wins (let alone 9 wins and 1 tie).\r\nSeason Plots\r\n2021\r\n\r\n2020\r\n\r\n2019\r\n\r\n2018\r\n\r\n2017\r\n\r\n2016\r\n\r\n2015\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-10-2015-to-2021-nfl-regular-season-time-average-win-totals/2021_win_plot.png",
    "last_modified": "2022-03-14T23:49:19-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-28-2021-nfl-regular-season-win-luck/",
    "title": "2021 NFL Regular Season Win Luck",
    "description": "An estimate of record outperformance of underlying play based on game time average leads.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-01-28",
    "categories": [],
    "contents": "\r\nIntro\r\nThe following is an assessment of the agreement between 2021 regular season play and 2021 season win totals. The basic unit of analysis the time average lead for each game played in 2021.\r\nUsing 2011-2020 data, I made a simple model estimating the expected win percentage for a given time average lead (and also account for whether game is home or away game). As noted in a previous post, the time average lead for a game is a metric that summarizes its overall competitiveness.\r\nTen Least Likely Wins\r\n\r\n\r\nWk\r\n\r\n\r\nTeam\r\n\r\n\r\nOpponent\r\n\r\n\r\nTime Avg Lead (Points)\r\n\r\n\r\nAvg Win Percentage (%)\r\n\r\n\r\n5\r\n\r\n\r\nBAL\r\n\r\n\r\nvs IND\r\n\r\n\r\n-9.0\r\n\r\n\r\n6.5\r\n\r\n\r\n16\r\n\r\n\r\nCHI\r\n\r\n\r\n@ SEA\r\n\r\n\r\n-6.3\r\n\r\n\r\n12.2\r\n\r\n\r\n17\r\n\r\n\r\nTB\r\n\r\n\r\n@ NYJ\r\n\r\n\r\n-5.6\r\n\r\n\r\n14.5\r\n\r\n\r\n2\r\n\r\n\r\nTEN\r\n\r\n\r\n@ SEA\r\n\r\n\r\n-5.6\r\n\r\n\r\n14.6\r\n\r\n\r\n9\r\n\r\n\r\nBAL\r\n\r\n\r\nvs MIN\r\n\r\n\r\n-5.9\r\n\r\n\r\n15.2\r\n\r\n\r\n5\r\n\r\n\r\nPHI\r\n\r\n\r\n@ CAR\r\n\r\n\r\n-5.4\r\n\r\n\r\n15.6\r\n\r\n\r\n18\r\n\r\n\r\nSF\r\n\r\n\r\n@ LA\r\n\r\n\r\n-5.4\r\n\r\n\r\n15.6\r\n\r\n\r\n2\r\n\r\n\r\nBAL\r\n\r\n\r\nvs KC\r\n\r\n\r\n-5.7\r\n\r\n\r\n16.2\r\n\r\n\r\n5\r\n\r\n\r\nNE\r\n\r\n\r\n@ HOU\r\n\r\n\r\n-5.1\r\n\r\n\r\n16.5\r\n\r\n\r\n1\r\n\r\n\r\nKC\r\n\r\n\r\nvs CLE\r\n\r\n\r\n-5.3\r\n\r\n\r\n18.0\r\n\r\n\r\nGraphical Summary\r\n\r\n\r\nTable\r\n\r\n\r\nTm\r\n\r\n\r\nWins\r\n\r\n\r\nPythag Wins\r\n\r\n\r\nTime Avg Wins\r\n\r\n\r\nPythag Win Luck\r\n\r\n\r\nTime Avg Win Luck\r\n\r\n\r\nARI\r\n\r\n\r\n11\r\n\r\n\r\n10.5\r\n\r\n\r\n10.8\r\n\r\n\r\n0.2\r\n\r\n\r\n0.5\r\n\r\n\r\nATL\r\n\r\n\r\n7\r\n\r\n\r\n4.9\r\n\r\n\r\n7.1\r\n\r\n\r\n-0.1\r\n\r\n\r\n2.1\r\n\r\n\r\nBAL\r\n\r\n\r\n8\r\n\r\n\r\n8.4\r\n\r\n\r\n8.1\r\n\r\n\r\n-0.1\r\n\r\n\r\n-0.4\r\n\r\n\r\nBUF\r\n\r\n\r\n11\r\n\r\n\r\n13.1\r\n\r\n\r\n12.3\r\n\r\n\r\n-1.3\r\n\r\n\r\n-2.1\r\n\r\n\r\nCAR\r\n\r\n\r\n5\r\n\r\n\r\n5.7\r\n\r\n\r\n7.3\r\n\r\n\r\n-2.3\r\n\r\n\r\n-0.7\r\n\r\n\r\nCHI\r\n\r\n\r\n6\r\n\r\n\r\n5.9\r\n\r\n\r\n6.9\r\n\r\n\r\n-0.9\r\n\r\n\r\n0.1\r\n\r\n\r\nCIN\r\n\r\n\r\n10\r\n\r\n\r\n10.5\r\n\r\n\r\n8.7\r\n\r\n\r\n1.3\r\n\r\n\r\n-0.5\r\n\r\n\r\nCLE\r\n\r\n\r\n8\r\n\r\n\r\n7.9\r\n\r\n\r\n9.5\r\n\r\n\r\n-1.5\r\n\r\n\r\n0.1\r\n\r\n\r\nDAL\r\n\r\n\r\n12\r\n\r\n\r\n12.2\r\n\r\n\r\n10.3\r\n\r\n\r\n1.7\r\n\r\n\r\n-0.2\r\n\r\n\r\nDEN\r\n\r\n\r\n7\r\n\r\n\r\n8.9\r\n\r\n\r\n7.9\r\n\r\n\r\n-0.9\r\n\r\n\r\n-1.9\r\n\r\n\r\nDET\r\n\r\n\r\n3\r\n\r\n\r\n5.1\r\n\r\n\r\n5.3\r\n\r\n\r\n-2.3\r\n\r\n\r\n-2.1\r\n\r\n\r\nGB\r\n\r\n\r\n13\r\n\r\n\r\n10.4\r\n\r\n\r\n10.6\r\n\r\n\r\n2.4\r\n\r\n\r\n2.6\r\n\r\n\r\nHOU\r\n\r\n\r\n4\r\n\r\n\r\n4.1\r\n\r\n\r\n6.3\r\n\r\n\r\n-2.3\r\n\r\n\r\n-0.1\r\n\r\n\r\nIND\r\n\r\n\r\n9\r\n\r\n\r\n10.6\r\n\r\n\r\n11.1\r\n\r\n\r\n-2.1\r\n\r\n\r\n-1.6\r\n\r\n\r\nJAX\r\n\r\n\r\n3\r\n\r\n\r\n3.4\r\n\r\n\r\n4.0\r\n\r\n\r\n-1.0\r\n\r\n\r\n-0.4\r\n\r\n\r\nKC\r\n\r\n\r\n12\r\n\r\n\r\n11.2\r\n\r\n\r\n11.2\r\n\r\n\r\n0.8\r\n\r\n\r\n0.8\r\n\r\n\r\nLA\r\n\r\n\r\n12\r\n\r\n\r\n10.6\r\n\r\n\r\n10.3\r\n\r\n\r\n1.7\r\n\r\n\r\n1.4\r\n\r\n\r\nLAC\r\n\r\n\r\n9\r\n\r\n\r\n8.8\r\n\r\n\r\n9.3\r\n\r\n\r\n-0.3\r\n\r\n\r\n0.2\r\n\r\n\r\nLV\r\n\r\n\r\n10\r\n\r\n\r\n6.9\r\n\r\n\r\n8.2\r\n\r\n\r\n1.8\r\n\r\n\r\n3.1\r\n\r\n\r\nMIA\r\n\r\n\r\n9\r\n\r\n\r\n7.6\r\n\r\n\r\n9.0\r\n\r\n\r\n0.0\r\n\r\n\r\n1.4\r\n\r\n\r\nMIN\r\n\r\n\r\n8\r\n\r\n\r\n8.5\r\n\r\n\r\n9.8\r\n\r\n\r\n-1.8\r\n\r\n\r\n-0.5\r\n\r\n\r\nNE\r\n\r\n\r\n10\r\n\r\n\r\n12.4\r\n\r\n\r\n9.7\r\n\r\n\r\n0.3\r\n\r\n\r\n-2.4\r\n\r\n\r\nNO\r\n\r\n\r\n9\r\n\r\n\r\n9.3\r\n\r\n\r\n8.1\r\n\r\n\r\n0.9\r\n\r\n\r\n-0.3\r\n\r\n\r\nNYG\r\n\r\n\r\n4\r\n\r\n\r\n4.1\r\n\r\n\r\n5.2\r\n\r\n\r\n-1.2\r\n\r\n\r\n-0.1\r\n\r\n\r\nNYJ\r\n\r\n\r\n4\r\n\r\n\r\n4.1\r\n\r\n\r\n4.5\r\n\r\n\r\n-0.5\r\n\r\n\r\n-0.1\r\n\r\n\r\nPHI\r\n\r\n\r\n9\r\n\r\n\r\n9.9\r\n\r\n\r\n7.9\r\n\r\n\r\n1.1\r\n\r\n\r\n-0.9\r\n\r\n\r\nPIT\r\n\r\n\r\n9\r\n\r\n\r\n7.0\r\n\r\n\r\n6.4\r\n\r\n\r\n2.6\r\n\r\n\r\n2.0\r\n\r\n\r\nSEA\r\n\r\n\r\n7\r\n\r\n\r\n9.3\r\n\r\n\r\n9.8\r\n\r\n\r\n-2.8\r\n\r\n\r\n-2.3\r\n\r\n\r\nSF\r\n\r\n\r\n10\r\n\r\n\r\n10.1\r\n\r\n\r\n9.7\r\n\r\n\r\n0.3\r\n\r\n\r\n-0.1\r\n\r\n\r\nTB\r\n\r\n\r\n13\r\n\r\n\r\n12.0\r\n\r\n\r\n10.8\r\n\r\n\r\n2.2\r\n\r\n\r\n1.0\r\n\r\n\r\nTEN\r\n\r\n\r\n12\r\n\r\n\r\n10.2\r\n\r\n\r\n9.9\r\n\r\n\r\n2.1\r\n\r\n\r\n1.8\r\n\r\n\r\nWAS\r\n\r\n\r\n7\r\n\r\n\r\n6.0\r\n\r\n\r\n6.8\r\n\r\n\r\n0.2\r\n\r\n\r\n1.0\r\n\r\n\r\nMethod\r\nI put together a data set for the 2011 through 2020 regular seasons. Doing some EDA, the following histogram stood out.\r\n\r\nI fit an extremely simple logistic regression to this data set: for all home teams: win probability as a function of time average lead. The model summary looked acceptable.\r\nI binned the time average leads into 0.5 point bins, and I plotted both the average winning percentage for each bin. It looked visually like logistic regression would be appropriate, so I fit an extremely simple logistic regression to this data set: for all home teams: win probability as a function of time average lead. For the away perspective, I fit a separate model to the same data set for away games.\r\nThe “bins + model fit” plot for the away model is provided below.\r\n\r\nThe home model summary is provided below.\r\n\r\n\r\nCall:\r\nglm(formula = win ~ mean_point_diff, family = \"binomial\", data = results_df %>% \r\n    filter(home_away == \"home\"))\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-2.8364  -0.5750   0.1410   0.5969   2.6841  \r\n\r\nCoefficients:\r\n                Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)      0.08979    0.05768   1.557     0.12    \r\nmean_point_diff  0.30574    0.01248  24.495   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 3286.4  on 2399  degrees of freedom\r\nResidual deviance: 1875.6  on 2398  degrees of freedom\r\nAIC: 1879.6\r\n\r\nNumber of Fisher Scoring iterations: 6\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-28-2021-nfl-regular-season-win-luck/nfl_2021_win_luck.png",
    "last_modified": "2022-01-29T00:04:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-19-2021-nfl-regular-season-pass-defense-penalties/",
    "title": "2021 NFL Regular Season Defensive Pass Game Penalties",
    "description": "Defensive pass game penalties drawn and committed by team.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-01-19",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nIntro\r\nThe following is a collection of defensive pass game penalty statistics for the 2021 NFL regular season, derived from nflfastR play-by-play data.\r\nHere, “defensive pass” penalties include (1) defensive pass interference (DPI), (2) defensive holding, and (3) illegal contact penalties. These are all high leverage penalties that result in an automatic first down. DPIs often provide the offense with significant yardage.\r\n2021 Defensive Pass Penalty Stats\r\n\r\n\r\n\r\nSummary Graphics\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-19-2021-nfl-regular-season-pass-defense-penalties/nfl_2021_net_def_pass_penalties.png",
    "last_modified": "2022-01-20T17:02:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-15-2021-nfl-regular-season-offensive-holding/",
    "title": "2021 NFL Regular Season Offensive Holding",
    "description": "Offensive holding calls drawn and committed, by team and play type.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-01-15",
    "categories": [],
    "contents": "\r\nIntro\r\nThe following is a collection of offensive holding penalty statistics for the 2021 NFL regular season, derived from nflfastR play-by-play data.\r\nThe totals below include all called holding penalties, even if declined or offset by another penalty. Plays identified as QB scrambles are counted as “pass” play calls. Spikes and kneel downs are not counted. Two point conversion attempts are also not counted.\r\nFor the net offensive holds, positive numbers correspond to positive balance for the season: the team’s defense drew more offensive holds than they committed.\r\n\r\n\r\n\r\nLeague Wide Play Call Breakdown\r\nThe majority of play calls in the 2021 season were passes. For all offensive snaps across the NFL,\r\n58.5 percent of offensive snaps were pass play calls.\r\n41.5 percent were runs.\r\nOverall, more holds occurred on pass play calls.\r\n368 holds were committed on pass play calls\r\n315 holds were committed on run play calls.\r\nOn a per snap basis, however, run plays were more likely to result in an offensive holding. For all offensive snaps across the NFL,\r\n1.9 percent of pass play calls resulted in a hold.\r\n2.2 percent of run play calls resulted in a hold.\r\n2021 Offensive Holding Stats\r\n\r\n\r\n\r\nSummary Graphics\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-15-2021-nfl-regular-season-offensive-holding/nfl_2021_net_off_holds.png",
    "last_modified": "2022-01-20T16:36:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-14-2021-nfl-regular-season-time-average-metrics/",
    "title": "2021 NFL Regular Season Time Average Metrics",
    "description": "End of year time average metrics for the 2021 NFL regular season",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-01-14",
    "categories": [],
    "contents": "\r\nWin plot added and table updated 03/09/2022.\r\nIntro\r\nThe following is a collection of team summary statistics for the 2021 season.\r\nFor all of these statistics, the basic unit is the time average lead (regulation time only, in either points or possessions) for the 17 regular season games played in 2021.\r\nFor an introduction to time averages, check this out.\r\nGraphical Summaries\r\nPoint Differential Summaries\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nInteractive Table\r\n\r\n\r\n\r\nGlossary\r\nAbbreviations used in table:\r\nTALE: Time Average Lead Evaluation\r\nSeason time average lead with adjustment for opponent quality\r\nTALE = TAL + SoS\r\n\r\nUnits: Points\r\n\r\nTAL: Time Average Lead\r\nUnadjusted time average lead for regulation time of all regular season games.\r\nAt any give point in regulation of 2021, team __ led by an average of __ points.\r\n\r\nUnits: Points\r\n\r\nSoS: Strength of Schedule\r\nOpponent quality adjustment based on opponents’ average TAL of opponents time average lead\r\nDoes not consider games against team of interest\r\nFor example, SoS calculation for HOU does not include the games HOU’s opponents played against HOU\r\n\r\nUnits: Points\r\n\r\nMETAL: MEdian Time Average Lead\r\nThe median of individual game time average leads for season.\r\nUnits: Points\r\n\r\nMoV: Margin of Victory\r\nAggregate point differential for team, on per game basis\r\nMoV = (Season Points Scored - Season Points Allowed)/Games Played\r\n\r\nUnits: Points\r\n\r\n",
    "preview": "posts/2022-01-14-2021-nfl-regular-season-time-average-metrics/2021_tier_plot.png",
    "last_modified": "2022-03-17T22:17:44-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-02-net-offensive-holding/",
    "title": "NFL Net Offensive Holding Calls",
    "description": "A look at offensive holding calls by team and play type through Week 16.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2022-01-02",
    "categories": [],
    "contents": "\r\nMotivating questions\r\nThrough the black-and-gold colored glasses of a Steelers fan, it seemed like offensive holding calls basically were not called in 2020 (I can’t watch as much of other games as I’d like).\r\nI had been wondering how much a lack of holding calls may have negatively impacted the 2020 Steelers defense.\r\nHow crippling is a holding call to a drive?\r\nWhich teams have taken the most offensive holding penalties?\r\nWhich teams have drawn the most offensive holding calls?\r\nWhich teams draw the most holds while committing the fewest (i.e., drawn - taken = net holds drawn)?\r\nRecently, Matt Williamson wondered aloud whether more holds occur on run or pass plays on an episode of SNR: The Drive.\r\nMethod\r\nUsing play-by-play data via nflfastR:\r\nThrow out punt and kickoffs\r\nCount all holds called, even if declined or offset by another penalty\r\nIf not directly identified as a pass or run in the data set, infer whether the play call was a pass or run using keywords in the play description.\r\nSacks were counted as “pass” play calls.\r\nScrambles were also counted as play calls.\r\n\r\nThis is just amateur stuff for fun. I’m probably classifying some play calls incorrectly. I did a quick cross-check of total holding calls for 2021 against this website, and the results were tracking (it looks like the website includes special teams holds, whereas I threw them out).\r\nResults for 2021, Through Week 16\r\nAffect on series conversion rates\r\nBen Baldwin periodically post team series conversion rates, where a series is a success if it generates a new set of downs or scores a TD. Here’s a recent example.\r\nTypical series conversion rates for an offense are in the neighborhood of 70 percent.\r\nWhen an offense commits a holding penalty, the series conversion rate drops to 42.4 percent in 2021.\r\nNet offensive hold ranks\r\n\r\nOffensive holds committed ranks\r\n\r\nDefense offensive holding calls drawn ranks\r\n\r\nOffensive holds by play call\r\nThrough Week 16, 46.6 percent of all Offensive Holds have occurred on run plays.\r\nConclusion\r\nThis was a quick, interesting exercise.\r\nAn offensive holding call really does hurt a drive as much as it seems.\r\nIn the future, it would be interesting to look at the topic in more detail. For example, more holds were called on run plays (~54 percent) than pass plays in 2020, the opposite of 2021. Is there a trend in officiating, change in play call mix, or just random variation?\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-01-02T17:24:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-26-analytics-debate-gaps/",
    "title": "Gaps in the Analytics Debate",
    "description": "A few non-curmudgeonly issues with the \"pro analytics\" camp.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2021-12-27",
    "categories": [],
    "contents": "\r\nIssues with the “pro analytics” side of the analytics debate\r\nAs many have noted, Week 15 of the NFL helped bring the sport’s analytics debate to an unfortunate place.\r\nHere are a few non-curmudgeonly issues I have with some in the “pro-analytics” camp.\r\nCommunicating model estimates without uncertainty\r\nThe expected win probability advantage of decisions are communicated without any uncertainty. However, a ‘Kick FG’ vs. ‘Go’ 4th down analysis contains six estimated parameters.\r\n\r\nImplying expected value optimization is the only valid decision making paradigm\r\nFor environments where it is hard to anticipate every contingency, the expected value paradigm that dominates the NFL decision making discourse has legitimate alternatives. However, anyone who questions the “analytics experts” is frequently cast as being unsophisticated or unable to grasp “the math.”\r\nGary Klein’s Recognition-primed Decision model describes how experienced experts in many domains can reliably make high-quality, time-compressed decisions in a completely different manner.\r\nGerd Gigerenzer has offered a compelling defense of Gut Decisions in many domains, demonstrating where heuristics can be superior to optimization via complex algorithms/models.\r\n\r\n\r\nModel estimates without uncertainty\r\nLet’s look at an example. Just before the half in Week 15 against the KC Chiefs, Brandon Staley faced the following decision.\r\nLink: https://twitter.com/ben_bot_baldwin/status/1471673042371858433The “medium, go for it” recommendation is the result of an expected value calculation based on six estimated parameters.\r\nTwo “conversion rate” estimates\r\nFor example, P(Score TD | ‘Go for it’) = 60%.\r\n\r\nFour win probability estimates for the possible resulting game states\r\nFor example, P(Win | Failed ‘Kick FG’ attempt) = 53%.\r\n\r\nWhat happens if we propagate some uncertainty into the estimates that are inputs into the expected value calculation? (Like a less sophisticated cousin of this awesome Shiny app, only that also considers that the Win Probability model could also have small errors.)\r\nAssume FG estimate is very accurate (i.e., error N(0, 0.25%)).\r\nFor other parameters, assume estimates is within 5% margin of error (i.e., errors N(0, 2.5%)).\r\nAre errors of such magnitude sufficiently charitable to the model? For a toy exercise, I think so. That would seem like an excellent model to me! Sources of such errors could be attributable to injuries, weather, game plan, etc. that couldn’t be captured by a model.\r\n\r\n\r\n\r\nDistribution of Expected Win Probability Results with Random Error in Underlying ParametersWhat’s the point?\r\nSome use an expected win probability advantage of at least 1 WP as a threshold to judge when a decisions is “correct.” Assume we accept the expected value paradigm (i.e., ignore my second complaint at present).\r\nPropagating some modest uncertainty in the estimated parameters underlying the expected value calculation suggests that categorizing coaching quality based on agreement with a specific model may not be justified for decisions with relatively narrow margins.\r\nIn this simulation, 35.9 percent of the simulated expected value calculations fail to exceeded the 1 WP advantage threshold for the aggressive “Go for it” decision.\r\nI’m not suggesting I’ve done this optimally (or perhaps even acceptably) for this example, but I think it is important to express uncertainty with estimates. In the future, I hope the “pro analytics” side of the debate finds a way to communicate uncertainty with their estimates.\r\nExpected value optimization as only valid paradigm\r\nI highly recommend Gary Klein’s Sources of Power. Klein studied how individuals in complex domains (e.g., emergency responders, combatants) actually make decisions. The result was the (descriptive) Recognition-primed Decision (RPD) model.\r\nHere’s my rough summary of the findings relevant to NFL coaching decision discussion.\r\nContrary to prevailing wisdom, it is novice decision makers in many domains that make decisions by comparing the projected consequences of multiple alternative courses of action.\r\nExpert decision makers, in contrast, draw upon cues in the environment and recognize a high quality candidate solution using their prior experience.\r\nThis candidate solution is interrogated for potential problems via a (comparatively) simple mental simulation.\r\nField studies in these domains demonstrate this single-path analysis mode is an improvement over comparative analysis (similar to the expected win probability analysis).\r\n\r\nHere’s a recent example of “expected value optimization is the only valid decision making paradigm” in the analytics debate.\r\nLink: https://twitter.com/benbbaldwin/status/1473012593547751424I see a lot of Klein’s descriptive model of expert “naturalistic decision making” from Coach Belichick, including critical reflection on previous decisions to learn like an expert. If I were a Pats fan, I would not be discouraged.\r\nWhat’s the point?\r\nCoaches are trying to find the most probable single trajectory to victory; the expected value of a strategy will not be realized for a single decision. Additionally, it is very plausible to me that an expert coach could identify valid reasons – unique to the specific game situation – why an expected win probability model may not apply perfectly for the specific decision being made.\r\nThere is a lot of serious academic and other work that support the view that there are alternatives that can be superior to expected value optimization in some settings. Their discussion appears entirely absent from the slice of the analytics debate to which I am exposed.\r\nIf I were to anticipate criticisms, it would be something like the following: win probability models are now of sufficient quality that NFL games are now “smallish” enough words that no biased human expert can systematically be superior to the model. I’m skeptical.\r\nA few other book recommendations that explore alternatives to “rational” optimization in the applicable contexts:\r\nRadical Uncertainty, by John Kay and Mervyn King\r\nGut Feelings: The Intelligence of the Unconscious, by Gerd Gigerenzer\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-26-analytics-debate-gaps/lac_wk_15_sim_with_parameter_error_end_half.png",
    "last_modified": "2022-01-19T16:16:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-28-time-average-nfl-metrics/",
    "title": "Time Average Metrics for NFL Team Quality",
    "description": "An alternative to margin of victory that better reflects the path to the final result.",
    "author": [
      {
        "name": "Casey Caprini",
        "url": {}
      }
    ],
    "date": "2021-11-28",
    "categories": [],
    "contents": "\r\nRevised 12/22/2021 and 01/10/2022.\r\nWhat is a “time average lead”?\r\nThe time average point lead for a game is a coarse-grained metric that summarizes its overall competitiveness.\r\nLet’s consider a simple example. In Week 15 of the 2021 season, the New Orleans Saints went to Tampa Bay and upset the Bucs, winning 9-0.\r\nThis example is simple because there were only three scoring plays. Also, since the Bucs were shut out, the New Orleans score is equal to the New Orleans lead throughout the game. (If New Orleans had trailed at some point in the game, their “lead” would have been a negative number for that part of the game.)\r\nLet’s look at a plot of the Saints shutout.\r\n\r\n\r\n\r\n\r\nThe “time average lead” is just a weighted average of the team’s leads throughout the game, weighted by the fraction of the game they held each lead. Geometrically, it is the sum of the area under the “lead” curve, divided by 60 minutes played. The plot is annotated to show the areas of the three lead intervals and the time average lead for the game.\r\nWhy “time average” point differentials?\r\nA few notable features of time average lead as a point-estimate game summary:\r\nThe units of the metric is points, which is intuitive to football fans.\r\nFor better or worse, the value of the metric relies on the assumption that a time average is informative. Time averages are often informative for processes that are at least partially path-dependent. A time average is just an average and therefore has the advantage of not relying on other modeling assumptions.\r\nIf you’re curious, the 2021 NFL season time average results (and other derived metrics) can be found here.\r\nA few other concrete examples\r\nConsider two more complex games from Week 1 of 2021.\r\nPIT 23, BUF 16\r\nSF 41, DET 33\r\n\r\n\r\n\r\nThe margin of victory for the Steelers and the 49ers was very similar. However, the time average point differential is significantly different.\r\nTeam\r\nMargin of Victory\r\nTime Average Lead\r\nPIT\r\n7\r\n-2.1\r\nSF\r\n8\r\n13.3\r\nLet’s compare the Win Probability graph for the two games.\r\n\r\n\r\nDespite the similar margin of victory for both teams, the Win Probability graphs reflect very different paths to the result.\r\nThe Steelers surged to take control in the 4th quarter, fueled by a Bills turnover on downs near midfield and a Pittsburgh TD on a blocked punt.\r\nThe Lions never threatened the 49ers until it was too late. Detroit scored two TDs and converted two 2-point conversions after the 2-minute warning in the 4th quarter. On top of defensive lapses, the 49ers allowed a successful onside kick and fumbled while trying to kill clock.\r\nThe Steelers made big plays late to win a game they kept close, while the 49ers controlled the game until hilarity ensued inside of 2-minutes. The time average point differential better reflects the path these games took to the final outcome than the final margin of victory.\r\nRecent NFL seasons\r\nThe following shows the distribution of time average game results (from the home team’s perspective) for all regular season games in 2011 to 20202. \r\nAs expected,\r\nMost positive time average differentials result in wins, while most negative time average point differentials result in losses.\r\nThere is evidence of a home team advantage: on average, home teams lead by ~1.4 points at any given point in regulation time for the decade considered of regular season games considered.\r\nResources and sources of inspiration\r\nInspired by an Ole Peters talk on Ergodicity Economics, I decided to investigate time averages in NFL games. Like life, my prior is that a football game is path dependent. Game script matters. Play calling goals vary with game situation.\r\nAt the time, I was reading 2021 NFL season previews and retrospectives on the 2020 season. The hypothesis was this: because of path dependence, time averages might enable better retrospective assessment of regular season team quality than season point differential and other aggregate metrics (e.g., Pythagorean Win Expectation).\r\nI the most amateur of R users. This post contains adapted code and inspiration from:\r\nThe tidyverse.\r\nThe nflfastR of the nflverse.\r\nTom Mock’s excellent posts on plotting images and tables\r\nShannon Pileggi’s excellent blog post on creating your own R package.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-28-time-average-nfl-metrics/no_tb_tal.png",
    "last_modified": "2022-01-19T16:18:35-05:00",
    "input_file": {}
  }
]
